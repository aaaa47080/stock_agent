"""
å›ç­”è™•ç†å·¥å…·æ¨¡çµ„
åŒ…å«åƒè€ƒæ–‡ç»é©—è­‰ã€åˆä½µã€æ ¼å¼åŒ–ç­‰åŠŸèƒ½
"""

import os
import re
from typing import List, Tuple
from difflib import SequenceMatcher
from core.config import llm, get_reference_mapping
from retrieval.retrieval_utils import DATASET_FILES_CACHE


def convert_references_to_english(chinese_ref: str) -> str:
    """å°‡ä¸­æ–‡åƒè€ƒæ–‡ç»è½‰æ›ç‚ºè‹±æ–‡è·¯å¾‘"""
    reference_mapping = get_reference_mapping()
    return reference_mapping.get(chinese_ref, "medical_knowledge_base")


def extract_sources_from_knowledge(knowledge: str) -> list[str]:
    """å¾çŸ¥è­˜æ–‡æœ¬ä¸­æå–åƒè€ƒä¾†æº"""
    sources = set()
    pattern = r'ã€Š([^ã€‹]+)ã€‹'
    matches = re.findall(pattern, knowledge)
    for match in matches:
        sources.add(match.strip())
    return list(sources)


def verify_document_references(response_content: str, original_sources: list[str] = None) -> tuple[bool, list[str]]:
    """
    é©—è­‰å›ç­”ä¸­å¼•ç”¨çš„æ–‡ä»¶æ˜¯å¦æœ‰æ•ˆ
    å„ªå…ˆæª¢æŸ¥æ˜¯å¦åœ¨ original_sourcesï¼ˆæœ¬æ¬¡æª¢ç´¢å¯¦éš›è¿”å›çš„æ–‡ä»¶ï¼‰ä¸­
    æ”¯æ´æœ‰/ç„¡å‰¯æª”åçš„æ¯”å°
    """
    pattern = r'ã€Š([^ã€‹]+)ã€‹'
    referenced_docs = re.findall(pattern, response_content)
    if not referenced_docs:
        return True, []

    # å„ªå…ˆä½¿ç”¨æœ¬æ¬¡æª¢ç´¢å¯¦éš›è¿”å›çš„æ–‡ä»¶åˆ—è¡¨
    if original_sources:
        invalid_docs = []

        # æº–å‚™æ¨™æº–åŒ–çš„ä¾†æºåˆ—è¡¨ï¼ˆå»é™¤å‰¯æª”åå’Œè·¯å¾‘ï¼‰
        normalized_sources = set()
        for s in original_sources:
            # å–å¾—åŸºæœ¬æª”å
            basename = os.path.basename(s)
            # ç§»é™¤å‰¯æª”å
            name_without_ext = os.path.splitext(basename)[0]
            # åŠ å…¥å…©ç¨®å½¢å¼
            normalized_sources.add(basename)
            normalized_sources.add(name_without_ext)
            normalized_sources.add(s)  # ä¹ŸåŠ å…¥åŸå§‹å½¢å¼

        for doc in referenced_docs:
            # æ¨™æº–åŒ–å¼•ç”¨çš„æ–‡ä»¶å
            doc_basename = os.path.basename(doc)
            doc_without_ext = os.path.splitext(doc_basename)[0]

            # æª¢æŸ¥æ˜¯å¦åŒ¹é…ï¼ˆå®Œæ•´åç¨±ã€basenameã€æˆ–ç„¡å‰¯æª”åç‰ˆæœ¬ï¼‰
            if doc not in normalized_sources and \
               doc_basename not in normalized_sources and \
               doc_without_ext not in normalized_sources:
                invalid_docs.append(doc)

        return len(invalid_docs) == 0, invalid_docs

    # é€€å›ä½¿ç”¨å…¨å±€æ–‡ä»¶ç·©å­˜
    actual_files = DATASET_FILES_CACHE
    if not actual_files:
        return False, referenced_docs
    invalid_docs = []
    for doc in referenced_docs:
        doc_name = os.path.basename(doc)
        doc_without_ext = os.path.splitext(doc_name)[0]

        # æª¢æŸ¥å¸¶æœ‰æˆ–ä¸å¸¶å‰¯æª”åçš„ç‰ˆæœ¬
        if doc_name not in actual_files and doc_without_ext not in actual_files:
            invalid_docs.append(doc)
    return len(invalid_docs) == 0, invalid_docs


def correct_document_names(response_content: str, original_sources: list[str]) -> str:
    """
    åªç³¾æ­£æ˜é¡¯çš„æ‹¼å¯«éŒ¯èª¤ï¼ˆç›¸ä¼¼åº¦ â‰¥ 0.95ï¼‰
    """
    if not original_sources or not response_content:
        return response_content

    pattern = r'ã€Š([^ã€‹]+)ã€‹'
    referenced_docs = re.findall(pattern, response_content)
    if not referenced_docs:
        return response_content

    corrected_content = response_content
    corrections_made = []

    for ref_doc in referenced_docs:
        if ref_doc in original_sources:
            continue

        best_match = None
        best_ratio = 0.0
        for orig_source in original_sources:
            ratio = SequenceMatcher(None, ref_doc.lower(), orig_source.lower()).ratio()
            if ratio > best_ratio and ratio >= 0.95:
                best_ratio = ratio
                best_match = orig_source

        if best_match:
            corrected_content = corrected_content.replace(f'ã€Š{ref_doc}ã€‹', f'ã€Š{best_match}ã€‹')
            corrections_made.append(f"{ref_doc} â†’ {best_match} (ç›¸ä¼¼åº¦: {best_ratio:.2f})")

    if corrections_made:
        print(f"ğŸ“ è‡ªå‹•ç³¾æ­£äº† {len(corrections_made)} å€‹æ–‡ä»¶åç¨±æ‹¼å¯«éŒ¯èª¤:")
        for correction in corrections_made:
            print(f"   - {correction}")

    return corrected_content


def merge_duplicate_references(content: str) -> str:
    """å¿«é€Ÿåˆä½µé‡è¤‡çš„åƒè€ƒæ–‡ç»ï¼ˆåŸºæ–¼è¦å‰‡ï¼‰"""
    if '**åƒè€ƒä¾æ“š' not in content:
        return content

    ref_section_match = re.search(r'(\*\*åƒè€ƒä¾æ“š[ï¼š:]*\*\*.*?)$', content, re.DOTALL)
    if not ref_section_match:
        return content

    ref_section = ref_section_match.group(1)
    file_pattern = r'ã€Š([^ã€‹]+)ã€‹'
    content_lines = ref_section.split('\n')

    # å„²å­˜æ¯å€‹æ–‡ä»¶å°æ‡‰çš„å…§å®¹
    file_contents = {}
    current_file = None

    for line in content_lines:
        stripped = line.strip()

        # æª¢æ¸¬æ–‡ä»¶å
        file_match = re.search(file_pattern, stripped)
        if file_match:
            current_file = file_match.group(1)
            if current_file not in file_contents:
                file_contents[current_file] = []
            continue

        # å…§å®¹è¡Œ
        if current_file and stripped and stripped.startswith('-'):
            content_text = stripped.lstrip('- ').strip()
            if content_text and content_text not in file_contents[current_file]:
                file_contents[current_file] = []

                file_contents[current_file].append(content_text)

    # é‡å»ºåƒè€ƒä¾æ“šå€å¡Š
    merged_lines = ['**åƒè€ƒä¾æ“š**']
    for filename, contents in file_contents.items():
        if filename and filename.strip() and filename not in ['Unknown', '   ']:
            merged_lines.append(f'ã€Š{filename}ã€‹')
            for content in contents:
                if content:
                    merged_lines.append(f'   - {content}')
            merged_lines.append('')

    merged_ref_section = '\n'.join(merged_lines).strip()
    main_content = content[:ref_section_match.start()]

    return main_content.rstrip() + '\n\n' + merged_ref_section


async def merge_duplicate_references_with_llm(content: str) -> str:
    """ä½¿ç”¨ LLM æ™ºèƒ½åˆä½µåƒè€ƒæ–‡ç»"""
    if '**åƒè€ƒä¾æ“š' not in content:
        return content

    ref_section_match = re.search(r'(\*\*åƒè€ƒä¾æ“š[ï¼š:]*\*\*.*?)$', content, re.DOTALL)
    if not ref_section_match:
        return content

    ref_section = ref_section_match.group(1)
    file_pattern = r'ã€Š([^ã€‹]+)ã€‹'
    filenames = re.findall(file_pattern, ref_section)

    # å¦‚æœæ²’æœ‰é‡è¤‡ï¼Œç›´æ¥è¿”å›
    if len(filenames) == len(set(filenames)):
        return content

    # ä½¿ç”¨ prompt_config.py ä¸­çš„ REFERENCE_MERGE_PROMPT
    from core.prompt_config import REFERENCE_MERGE_PROMPT
    merge_prompt = REFERENCE_MERGE_PROMPT.format(ref_section=ref_section)

    try:
        response = await llm.ainvoke(merge_prompt)
        merged_ref_section = response.content.strip()

        if not merged_ref_section.startswith('**åƒè€ƒä¾æ“š'):
            merged_ref_section = '**åƒè€ƒä¾æ“š**\n' + merged_ref_section

        main_content = content[:ref_section_match.start()]
        return main_content.rstrip() + '\n\n' + merged_ref_section

    except Exception as e:
        print(f"âš ï¸ LLM åˆä½µå¤±æ•—: {e}ï¼Œä½¿ç”¨è¦å‰‡åˆä½µ")
        return merge_duplicate_references(content)


async def post_process_response(content: str, original_sources: list[str]) -> str:
    """å¾Œè™•ç†å›ç­”ï¼šæ¶ˆæ¯’ã€é©—è­‰ã€ç³¾æ­£ã€åˆä½µåƒè€ƒæ–‡ç»"""
    # print("\n" + "="*80)
    # print("ğŸ” é–‹å§‹å¾Œè™•ç†å›ç­”...")
    # print("="*80)

    # èª¿è©¦ï¼šæ‰“å° original_sources
    # print(f"ğŸ“‹ original_sources ({len(original_sources)} å€‹):")
    # for src in original_sources[:10]:  # åªé¡¯ç¤ºå‰10å€‹
    #     print(f"   - {src}")
    # if len(original_sources) > 10:
    #     print(f"   ... é‚„æœ‰ {len(original_sources) - 10} å€‹")

    # ğŸ›¡ï¸ 0. é¦–è¦æ­¥é©Ÿï¼šæ¶ˆæ¯’è¼¸å‡ºï¼Œç§»é™¤æ´©æ¼çš„ Prompt æŒ‡ä»¤
    sanitized_content,_ = sanitize_llm_output(content)
    # if was_sanitized:
    #     print("   âš ï¸ å·²ç§»é™¤æ½›åœ¨çš„ Prompt Leakage")

    # 1. ç³¾æ­£æ‹¼å¯«éŒ¯èª¤
    corrected_content = correct_document_names(sanitized_content, original_sources)

    # 2. é©—è­‰æ–‡ä»¶å¼•ç”¨
    is_valid, invalid_docs = verify_document_references(corrected_content, original_sources)
    if not is_valid:
        # print(f"\nâš ï¸ ç™¼ç¾ {len(invalid_docs)} å€‹ç„¡æ•ˆæ–‡ä»¶å¼•ç”¨:")
        for doc in invalid_docs:
            print(f"   - ã€Š{doc}ã€‹")

        # ç§»é™¤ç„¡æ•ˆå¼•ç”¨
        for doc in invalid_docs:
            pattern = rf'ã€Š{re.escape(doc)}ã€‹[^\n]*\n?'
            corrected_content = re.sub(pattern, '', corrected_content)
        # print("âœ… å·²ç§»é™¤ç„¡æ•ˆå¼•ç”¨")

    # 3. åˆä½µé‡è¤‡åƒè€ƒ
    final_content = await merge_duplicate_references_with_llm(corrected_content)

    # print("="*80)
    # print("âœ… å¾Œè™•ç†å®Œæˆ")
    # print("="*80 + "\n")

    return final_content

def parse_memory_results(result) -> list:
    """è§£æ mem0 è¨˜æ†¶çµæœ"""
    results = result.get("results", []) if isinstance(result, dict) else (
        result if isinstance(result, list) else []
    )
    memory_texts = []
    for item in results:
        if isinstance(item, dict):
            mem_content = item.get("memory", "").strip()
            if mem_content:
                memory_texts.append(mem_content)
        elif isinstance(item, str) and item.strip():
            memory_texts.append(item.strip())
    return memory_texts

def deduplicate_references(knowledge: str) -> str:
    """
    æå–æ‰€æœ‰ã€Šæ–‡ä»¶åã€‹ä¸‹çš„å…§å®¹ï¼Œæ ¹æ“šï¼ˆæ–‡ä»¶å, æ¨™æº–åŒ–å…§å®¹ï¼‰å»é‡ï¼Œ
    æœ€å¾Œçµ±ä¸€ç”¨å–®ä¸€ã€Œ**åƒè€ƒä¾æ“š**ã€æ¨™é¡Œå½™æ•´ã€‚
    """
    # åŒ¹é…æ¯ä¸€å€‹ã€Œ**åƒè€ƒä¾æ“š**ã€å€å¡Šå…§çš„æ‰€æœ‰ã€Š...ã€‹æ®µè½
    # æ³¨æ„ï¼šæœ‰äº›æ®µè½å¯èƒ½åŒ…å«å¤šå€‹ã€Šæ–‡ä»¶ã€‹ï¼Œä½†æ ¹æ“šä½ çš„è³‡æ–™ï¼Œé€šå¸¸ä¸€å€å¡Šä¸€æ–‡ä»¶
    ref_blocks = re.findall(r'\*\*åƒè€ƒä¾æ“š\*\*\s*\n((?:\s{0,4}ã€Š[^ã€‹]+ã€‹.*?)(?=\n\s*\*\*åƒè€ƒä¾æ“š\*\*|\Z))', knowledge, re.DOTALL)

    seen = set()
    unique_contents: List[str] = []

    for block in ref_blocks:
        # åœ¨æ¯å€‹ block ä¸­æ‰¾å‡ºæ‰€æœ‰ã€Šæ–‡ä»¶åã€‹åŠå…¶å°æ‡‰å…§å®¹
        # å‡è¨­æ¯å€‹ block åªæœ‰ä¸€å€‹ã€Š...ã€‹æ®µè½ï¼ˆç¬¦åˆä½ çš„è³‡æ–™çµæ§‹ï¼‰
        file_matches = re.findall(r'(ã€Š[^ã€‹]+ã€‹.*?)(?=ã€Š|\Z)', block, re.DOTALL)
        for fm in file_matches:
            fm = fm.strip()
            if not fm:
                continue
            filename_match = re.search(r'ã€Š([^ã€‹]+)ã€‹', fm)
            if not filename_match:
                continue
            filename = filename_match.group(1)
            normalized = re.sub(r'\s+', ' ', fm)
            key = (filename, normalized)
            if key not in seen:
                seen.add(key)
                unique_contents.append(fm)

    # æå–éã€Œåƒè€ƒä¾æ“šã€çš„å‰å°å…§å®¹ï¼ˆå¦‚ã€Œä»¥ä¸‹æ˜¯æª¢ç´¢åˆ°çš„ç›¸é—œåƒè€ƒè³‡æ–™ï¼šã€ï¼‰
    first_ref_pos = knowledge.find('**åƒè€ƒä¾æ“š**')
    if first_ref_pos == -1:
        prefix = knowledge
    else:
        prefix = knowledge[:first_ref_pos].rstrip()

    # çµ„åˆçµæœï¼šå‰å°å…§å®¹ + å–®ä¸€ã€Œ**åƒè€ƒä¾æ“š**ã€æ¨™é¡Œ + å»é‡å¾Œçš„æ–‡ä»¶å…§å®¹
    if unique_contents:
        ref_section = "**åƒè€ƒä¾æ“š**\n\n" + "\n\n".join(unique_contents)
        return prefix + "\n\n" + ref_section
    else:
        return prefix

def smart_truncate_knowledge(knowledge: str, max_chars: int = 7000) -> str:
    deduped = deduplicate_references(knowledge)

    if len(deduped) <= max_chars:
        return deduped

    # print(f"âš ï¸ çŸ¥è­˜éé•· ({len(deduped)} å­—å…ƒ)ï¼Œæˆªæ–·è‡³ {max_chars} å­—å…ƒ")

    ref_match = re.search(r'\*\*åƒè€ƒä¾æ“š\*\*.*$', deduped, re.DOTALL)
    if ref_match:
        ref_section = ref_match.group(0)
        main_content = deduped[:ref_match.start()].rstrip()

        available_chars = max_chars - len(ref_section)
        if available_chars > 50:
            truncated_main = main_content[:available_chars - 50] + "\n\n[... å…§å®¹éé•·ï¼Œå·²æˆªæ–· ...]"
            return truncated_main + "\n\n" + ref_section
        else:
            return ref_section[:max_chars] + "\n\n[... å…§å®¹éé•·ï¼Œå·²æˆªæ–· ...]"
    else:
        return deduped[:max_chars] + "\n\n[... å…§å®¹éé•·ï¼Œå·²æˆªæ–· ...]"


def sanitize_llm_output(content: str) -> Tuple[str, bool]:
    """
    ğŸ›¡ï¸ è¼¸å‡ºæ¶ˆæ¯’å‡½æ•¸ï¼šç§»é™¤å¯èƒ½æ´©æ¼çš„ Prompt æŒ‡ä»¤

    é˜²æ­¢ Prompt Leakage æ”»æ“Šï¼Œæ ¹æ“šé…ç½®æ–‡ä»¶ä¸­çš„è¦å‰‡ç§»é™¤ï¼š
    1. ã€æ ¼å¼æŒ‡ç¤º...ã€‘ã€ã€æŒ‡ç¤º...ã€‘ç­‰å…ƒæŒ‡ä»¤æ¨™è¨˜
    2. "ä¸è¦è¼¸å‡º"ã€"ç¦æ­¢"ã€"æ³¨æ„äº‹é …" ç­‰å…§éƒ¨è¦å‰‡
    3. ç³»çµ±æŒ‡ä»¤ç‰‡æ®µ

    è¦å‰‡é…ç½®ä½æ–¼ config.OUTPUT_SANITIZATION_CONFIG

    Args:
        content: LLM ç”Ÿæˆçš„åŸå§‹å…§å®¹

    Returns:
        (sanitized_content, was_sanitized): æ¶ˆæ¯’å¾Œçš„å…§å®¹å’Œæ˜¯å¦é€²è¡Œäº†ä¿®æ”¹
    """
    from core.config import OUTPUT_SANITIZATION_CONFIG

    # æª¢æŸ¥æ˜¯å¦å•Ÿç”¨
    if not OUTPUT_SANITIZATION_CONFIG.get('enabled', True):
        return content, False

    original_content = content

    # ğŸ”§ è¦å‰‡ 1: ç§»é™¤ç²¾ç¢ºåŒ¹é…çš„çŸ­èª
    exact_phrases = OUTPUT_SANITIZATION_CONFIG.get('exact_phrases_to_remove', [])
    for phrase in exact_phrases:
        content = content.replace(phrase, '')

    # ğŸ”§ è¦å‰‡ 2: ç§»é™¤æ¨™è¨˜æ¨¡å¼ï¼ˆæ­£å‰‡è¡¨é”å¼ï¼‰
    marker_patterns = OUTPUT_SANITIZATION_CONFIG.get('marker_patterns', [])
    for pattern in marker_patterns:
        content = re.sub(pattern, '', content, flags=re.IGNORECASE)

    # ğŸ”§ è¦å‰‡ 3: ç§»é™¤åŒ…å«æ•æ„Ÿé—œéµå­—çš„æ•´è¡Œ
    sensitive_patterns = OUTPUT_SANITIZATION_CONFIG.get('sensitive_line_patterns', [])
    if sensitive_patterns:
        lines = content.split('\n')
        filtered_lines = []
        for line in lines:
            is_sensitive = False
            for keyword_pattern in sensitive_patterns:
                if re.search(keyword_pattern, line, re.IGNORECASE):
                    is_sensitive = True
                    break
            if not is_sensitive:
                filtered_lines.append(line)
        content = '\n'.join(filtered_lines)

    # ğŸ”§ è¦å‰‡ 4: æ¸…ç†æ®˜ç•™ï¼ˆç§»é™¤é€£çºŒç©ºè¡Œï¼‰
    content = re.sub(r'\n{3,}', '\n\n', content)

    # ğŸ”§ è¦å‰‡ 5: ç§»é™¤é–‹é ­å’Œçµå°¾çš„ç©ºç™½
    content = content.strip()

    # æª¢æŸ¥æ˜¯å¦é€²è¡Œäº†ä¿®æ”¹
    was_modified = (content != original_content)

    if was_modified:
        print("ğŸ›¡ï¸ æª¢æ¸¬åˆ°æ½›åœ¨çš„ Prompt Leakageï¼Œå·²è‡ªå‹•ç§»é™¤")

    return content, was_modified

