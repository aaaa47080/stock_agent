"""
LLM å®¢æˆ¶ç«¯å·¥å»  - æ”¯æŒ OpenAI å’Œ OpenRouter
"""

import os
from dotenv import load_dotenv
import openai
from typing import Dict, Any
# å˜—è©¦å°å…¥ LangChain çš„ init_chat_modelï¼Œå¦‚æœæœªå®‰è£å‰‡è·³éæˆ–å ±éŒ¯
try:
    from langchain.chat_models import init_chat_model
    LANGCHAIN_AVAILABLE = True
except ImportError:
    LANGCHAIN_AVAILABLE = False
    print("Warning: langchain not installed. Local LLM support will be limited.")

# å˜—è©¦å°å…¥ Google Geminiï¼Œå¦‚æœæœªå®‰è£å‰‡è·³é
try:
    import google.generativeai as genai
    GOOGLE_GEMINI_AVAILABLE = True
except ImportError:
    genai = None
    GOOGLE_GEMINI_AVAILABLE = False

load_dotenv()


class GeminiWrapper:
    """Google Gemini API åŒ…è£å™¨ï¼Œæä¾› OpenAI å…¼å®¹æ¥å£"""
    # ... (GeminiWrapper implementation remains the same)
    def __init__(self, genai_module):
        self.genai = genai_module
        self.chat = self  # æ¨¡æ“¬ OpenAI çš„ client.chat çµæ§‹
        self.completions = self  # æ¨¡æ“¬ OpenAI çš„ client.chat.completions çµæ§‹

    def create(self, model: str, messages: list, response_format: dict = None, temperature: float = 0.5, **kwargs):
        """
        æ¨¡æ“¬ OpenAI çš„ chat.completions.create() æ–¹æ³•

        Args:
            model: Gemini æ¨¡å‹åç¨± (ä¾‹å¦‚ "gemini-1.5-pro")
            messages: OpenAI æ ¼å¼çš„æ¶ˆæ¯åˆ—è¡¨
            response_format: éŸ¿æ‡‰æ ¼å¼é…ç½® ({"type": "json_object"})
            temperature: ç”Ÿæˆæº«åº¦
            **kwargs: å…¶ä»–åƒæ•¸

        Returns:
            æ¨¡æ“¬ OpenAI éŸ¿æ‡‰æ ¼å¼çš„å°è±¡
        """
        # å°‡ OpenAI æ ¼å¼çš„æ¶ˆæ¯è½‰æ›ç‚º Gemini æ ¼å¼
        # Gemini åªéœ€è¦æœ€å¾Œä¸€æ¢ç”¨æˆ¶æ¶ˆæ¯çš„å…§å®¹
        prompt = ""
        for msg in messages:
            if msg["role"] == "user":
                prompt = msg["content"]

        # é…ç½®ç”Ÿæˆåƒæ•¸
        generation_config = {
            "temperature": temperature,
            "top_p": 0.95,
            "top_k": 40,
            "max_output_tokens": 8192,
        }

        # å¦‚æœéœ€è¦ JSON è¼¸å‡ºï¼Œè¨­ç½® response_mime_type å’Œå¼·åŒ–æç¤º
        if response_format and response_format.get("type") == "json_object":
            generation_config["response_mime_type"] = "application/json"
            # åœ¨æç¤ºè©æœ€å¾Œå¼·èª¿åªè¼¸å‡º JSON
            if "è«‹ä»¥ JSON æ ¼å¼å›è¦†" in prompt or "JSON æ ¼å¼" in prompt:
                # å·²ç¶“æœ‰ JSON æŒ‡ç¤ºï¼ŒåŠ å¼·èªªæ˜
                prompt = f"{prompt}\n\né‡è¦æé†’ï¼šåªè¼¸å‡ºç´” JSON å°è±¡ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡‹ã€æ€è€ƒéç¨‹æˆ–å…¶ä»–æ–‡å­—ã€‚"
            else:
                # æ²’æœ‰ JSON æŒ‡ç¤ºï¼Œæ·»åŠ å®Œæ•´èªªæ˜
                prompt = f"{prompt}\n\nè«‹ä»¥ç´” JSON æ ¼å¼å›è¦†ï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡å­—ã€è§£é‡‹æˆ–æ¨™è¨˜ã€‚"

        # å‰µå»º Gemini æ¨¡å‹å¯¦ä¾‹ï¼ˆå¸¶ç³»çµ±æŒ‡ä»¤ï¼‰
        system_instruction = None
        if response_format and response_format.get("type") == "json_object":
            system_instruction = "You are a JSON response generator. Always output valid JSON only, without any additional text, explanation, or markdown formatting."

        gemini_model = self.genai.GenerativeModel(
            model_name=model,
            generation_config=generation_config,
            system_instruction=system_instruction
        )

        # èª¿ç”¨ Gemini API
        response = gemini_model.generate_content(prompt)

        # ç²å–éŸ¿æ‡‰æ–‡æœ¬
        response_text = response.text

        # å¦‚æœéœ€è¦ JSON è¼¸å‡ºï¼Œæ¸…ç†å’Œé©—è­‰éŸ¿æ‡‰
        if response_format and response_format.get("type") == "json_object":
            import json
            import re
            import dirtyjson

            # å˜—è©¦æ¸…ç†éŸ¿æ‡‰ï¼ˆç§»é™¤å¯èƒ½çš„ markdown ä»£ç¢¼å¡Šæ¨™è¨˜ï¼‰
            cleaned_text = response_text.strip()

            # ç§»é™¤å¯èƒ½çš„ markdown JSON ä»£ç¢¼å¡Š
            if cleaned_text.startswith("```json"):
                cleaned_text = re.sub(r'^```json\s*\n', '', cleaned_text)
                cleaned_text = re.sub(r'\n```\s*$', '', cleaned_text)
            elif cleaned_text.startswith("```"):
                cleaned_text = re.sub(r'^```\s*\n', '', cleaned_text)
                cleaned_text = re.sub(r'\n```\s*$', '', cleaned_text)

            try:
                # å˜—è©¦è§£æ JSON ä»¥é©—è­‰æ ¼å¼
                parsed = dirtyjson.loads(cleaned_text)

                # æª¢æŸ¥æ˜¯å¦æœ‰ 'task' æˆ–å…¶ä»–åŒ…è£éµï¼ˆæŸäº› Gemini ç‰ˆæœ¬æœƒé€™æ¨£åšï¼‰
                if isinstance(parsed, dict) and len(parsed) == 1:
                    # å¦‚æœåªæœ‰ä¸€å€‹éµä¸”ä¸æ˜¯é æœŸçš„æ¥­å‹™éµï¼Œå¯èƒ½æ˜¯åŒ…è£
                    single_key = list(parsed.keys())[0]
                    if single_key in ['task', 'response', 'output', 'result', 'data']:
                        print(f"âš ï¸  æª¢æ¸¬åˆ° Gemini åŒ…è£éµ '{single_key}'ï¼Œå˜—è©¦è§£åŒ…...")
                        # å˜—è©¦è§£åŒ…
                        inner_value = parsed[single_key]
                        if isinstance(inner_value, dict):
                            parsed = inner_value
                            print(f"âœ… è§£åŒ…æˆåŠŸï¼Œæ–°çš„éµ: {list(parsed.keys())}")

                # è¨˜éŒ„èª¿è©¦ä¿¡æ¯
                print(f"ğŸ” Gemini JSON éŸ¿æ‡‰éµ: {list(parsed.keys()) if isinstance(parsed, dict) else type(parsed)}")

                # é‡æ–°åºåˆ—åŒ–ä»¥ç¢ºä¿æ ¼å¼æ­£ç¢º
                response_text = json.dumps(parsed, ensure_ascii=False)

            except json.JSONDecodeError as e:
                # è¨˜éŒ„åŸå§‹éŸ¿æ‡‰ä»¥ä¾¿èª¿è©¦
                print(f"âš ï¸  Gemini JSON è§£æå¤±æ•—: {e}")
                print(f"åŸå§‹éŸ¿æ‡‰å‰500å­—ç¬¦: {response_text[:500]}")
                # å˜—è©¦æå– JSONï¼ˆæŸ¥æ‰¾ç¬¬ä¸€å€‹ { åˆ°æœ€å¾Œä¸€å€‹ }ï¼‰
                first_brace = response_text.find('{')
                last_brace = response_text.rfind('}')
                if first_brace != -1 and last_brace != -1:
                    try:
                        extracted = response_text[first_brace:last_brace + 1]
                        parsed = dirtyjson.loads(extracted)
                        print(f"âœ… JSON æå–æˆåŠŸ")
                        response_text = json.dumps(parsed, ensure_ascii=False)
                    except:
                        pass  # ä¿æŒåŸå§‹éŸ¿æ‡‰æ–‡æœ¬

        # å°‡ Gemini éŸ¿æ‡‰è½‰æ›ç‚º OpenAI æ ¼å¼
        class Choice:
            def __init__(self, content):
                self.message = type('obj', (object,), {'content': content})()

        class Response:
            def __init__(self, content):
                self.choices = [Choice(content)]

        return Response(response_text)


class LangChainOpenAIAdapter:
    """
    é©é…å™¨ï¼šå°‡ LangChain ChatModel å°è£æˆé¡ä¼¼ OpenAI Client çš„æ¥å£
    ä¸»è¦ç”¨æ–¼è®“ç¾æœ‰ä»£ç¢¼ (client.chat.completions.create) èƒ½ç„¡ç¸«ä½¿ç”¨ LangChain ç‰©ä»¶
    """
    def __init__(self, langchain_model):
        self.model = langchain_model
        self.chat = self
        self.completions = self

    def create(self, model: str = None, messages: list = None, response_format: dict = None, temperature: float = None, **kwargs):
        """
        æ¨¡æ“¬ OpenAI çš„ create æ–¹æ³•
        """
        # 1. è½‰æ› messages (OpenAI dict -> LangChain Message objects)
        # Using langchain_core.messages as langchain.schema is deprecated/moved
        from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
        lc_messages = []
        for msg in messages:
            role = msg.get("role")
            content = msg.get("content")
            if role == "system":
                lc_messages.append(SystemMessage(content=content))
            elif role == "user":
                lc_messages.append(HumanMessage(content=content))
            elif role == "assistant":
                lc_messages.append(AIMessage(content=content))
            else:
                lc_messages.append(HumanMessage(content=content)) # Fallback

        # 2. åŸ·è¡Œ Invoke
        # æ³¨æ„ï¼štemperature ç­‰åƒæ•¸åœ¨ init_chat_model æ™‚å·²ç¶“è¨­å®šï¼Œé€™è£¡å¦‚æœè¦è¦†è“‹éœ€è¦ bind
        # ä½†ç‚ºäº†ç°¡å–®èµ·è¦‹ï¼Œé€™è£¡ç›´æ¥ invokeï¼Œå¿½ç•¥é‹è¡Œæ™‚çš„è¶…åƒæ•¸è¦†è“‹ (é™¤éä½¿ç”¨ bind)
        try:
            response = self.model.invoke(lc_messages)
            content = response.content
        except Exception as e:
            print(f"LangChain invoke error: {e}")
            raise e

        # 3. å°è£å› OpenAI æ ¼å¼çš„å›å‚³
        class Choice:
            def __init__(self, content):
                self.message = type('obj', (object,), {'content': content})()

        class Response:
            def __init__(self, content):
                self.choices = [Choice(content)]
        
        return Response(content)


class LLMClientFactory:
    """LLM å®¢æˆ¶ç«¯å·¥å» ï¼Œæ”¯æŒå¤šç¨® LLM æä¾›å•†"""

    @staticmethod
    def _get_api_key(provider: str) -> str:
        """
        ç²å– API Key çš„æ ¸å¿ƒæ–¹æ³•ã€‚
        å„ªå…ˆç´šï¼šå‹•æ…‹ Settings > os.environ > .env
        """
        from utils.settings import Settings

        if provider == "openai":
            return Settings.OPENAI_API_KEY or os.getenv("OPENAI_API_KEY", "")
        elif provider == "openai_server":
            # ä¼ºæœå™¨ç«¯å°ˆç”¨ keyï¼ˆä¸æœƒè¢«ç”¨æˆ¶è¦†è“‹ï¼‰
            # ç”¨æ–¼ Market Pulseã€æ–°èå¯©æŸ¥ç­‰å¹³å°ç´šåŠŸèƒ½
            return Settings.SERVER_OPENAI_API_KEY or os.getenv("SERVER_OPENAI_API_KEY", "") or os.getenv("OPENAI_API_KEY", "")
        elif provider == "google_gemini":
            # å…¼å®¹ Google å®˜æ–¹ SDK çš„è®Šæ•¸åç¨±
            return os.getenv("GOOGLE_API_KEY") or getattr(Settings, "GOOGLE_API_KEY", "") or os.getenv("GEMINI_API_KEY", "")
        elif provider == "openrouter":
            return os.getenv("OPENROUTER_API_KEY") or getattr(Settings, "OPENROUTER_API_KEY", "")
        return ""

    @staticmethod
    def create_client(provider: str, model: str = None) -> Any:
        """
        å‰µå»º LLM å®¢æˆ¶ç«¯

        Args:
            provider: æä¾›å•† ("openai", "openrouter", "google_gemini", "local")
            model: æ¨¡å‹åç¨±ï¼ˆç”¨æ–¼è¨˜éŒ„ï¼‰

        Returns:
            é…ç½®å¥½çš„ OpenAI å®¢æˆ¶ç«¯ (æˆ– LangChain é©é…å™¨)
        """
        if provider.lower() == "openai":
            return LLMClientFactory._create_openai_client()
        elif provider.lower() == "openrouter":
            return LLMClientFactory._create_openrouter_client()
        elif provider.lower() == "google_gemini":
            return LLMClientFactory._create_google_gemini_client()
        elif provider.lower() == "local":
            return LLMClientFactory._create_local_langchain_client(model)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ LLM æä¾›å•†: {provider}")

    @staticmethod
    def _create_openai_client():
        """å‰µå»º OpenAI å®¢æˆ¶ç«¯"""
        api_key = LLMClientFactory._get_api_key("openai")
        if not api_key:
            raise ValueError("æœªæ‰¾åˆ°æœ‰æ•ˆ OpenAI API Keyã€‚è«‹å…ˆåœ¨è¨­ç½®ä¸­è¼¸å…¥ä¸¦é©—è­‰ã€‚")

        return openai.OpenAI(api_key=api_key)

    @staticmethod
    def _create_openrouter_client():
        """å‰µå»º OpenRouter å®¢æˆ¶ç«¯"""
        api_key = LLMClientFactory._get_api_key("openrouter")
        if not api_key:
            raise ValueError("æœªæ‰¾åˆ°æœ‰æ•ˆ OpenRouter API Keyã€‚è«‹å…ˆåœ¨è¨­ç½®ä¸­è¼¸å…¥ä¸¦é©—è­‰ã€‚")

        # OpenRouter ä½¿ç”¨ OpenAI å…¼å®¹çš„ API
        return openai.OpenAI(
            api_key=api_key,
            base_url="https://openrouter.ai/api/v1"
        )

    @staticmethod
    def _create_google_gemini_client():
        """å‰µå»º Google Gemini å®¢æˆ¶ç«¯"""
        if not GOOGLE_GEMINI_AVAILABLE:
            raise ValueError("Google Gemini SDK æœªå®‰è£ï¼Œè«‹é‹è¡Œ: pip install google-generativeai")

        api_key = LLMClientFactory._get_api_key("google_gemini")
        if not api_key:
            raise ValueError("æœªæ‰¾åˆ°æœ‰æ•ˆ Google API Keyã€‚è«‹å…ˆåœ¨è¨­ç½®ä¸­è¼¸å…¥ä¸¦é©—è­‰ã€‚")

        genai.configure(api_key=api_key)
        return GeminiWrapper(genai)

    @staticmethod
    def _create_local_langchain_client(model_name: str):
        """å‰µå»ºæœ¬åœ° LangChain å®¢æˆ¶ç«¯ï¼Œä¸¦å°è£ç‚º OpenAI å…¼å®¹æ¥å£"""
        if not LANGCHAIN_AVAILABLE:
            raise ImportError("LangChain is required for local model support. Please install it.")

        try:
            from core.config import LOCAL_LLM_CONFIG
        except ImportError:
            # Fallback default if config not found
            LOCAL_LLM_CONFIG = {
                "base_url": "http://localhost:8000/v1",
                "api_key": "not-needed",
                "temperature": 0.1,
                "seed": 42
            }

        base_url = LOCAL_LLM_CONFIG.get("base_url", "http://localhost:8000/v1")
        api_key = LOCAL_LLM_CONFIG.get("api_key", "not-needed")
        temperature = LOCAL_LLM_CONFIG.get("temperature", 0.1)
        seed = LOCAL_LLM_CONFIG.get("seed", 42)

        # é€™è£¡ model_provider è¨­ç‚º "openai" æ˜¯å› ç‚º vLLM/Ollama é€šå¸¸æä¾› OpenAI å…¼å®¹çš„ API
        llm = init_chat_model(
            model=model_name or "local-model",
            model_provider="openai",
            base_url=base_url,
            api_key=api_key,
            temperature=temperature,
            max_retries=2,
            model_kwargs={"seed": seed}
        )
        
        # è¿”å›é©é…å™¨ï¼Œè®“ç¾æœ‰ä»£ç¢¼ (client.chat.completions.create) å¯ä»¥ç¹¼çºŒå·¥ä½œ
        return LangChainOpenAIAdapter(llm)

    @staticmethod
    def get_model_info(config: Dict[str, str]) -> str:
        """
        ç²å–æ¨¡å‹ä¿¡æ¯å­—ç¬¦ä¸²

        Args:
            config: æ¨¡å‹é…ç½®å­—å…¸ {"provider": "...", "model": "..."}

        Returns:
            æ¨¡å‹ä¿¡æ¯å­—ç¬¦ä¸²
        """
        provider = config.get("provider", "openai")
        model = config.get("model", "unknown")

        if provider == "openrouter":
            return f"{model} (via OpenRouter)"
        elif provider == "google_gemini":
            return f"{model} (Google Gemini Official)"
        elif provider == "local":
            return f"{model} (Local LangChain)"
        else:
            return f"{model} (OpenAI)"



def supports_json_mode(model: str) -> bool:
    """
    æª¢æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒ JSON æ¨¡å¼

    Args:
        model: æ¨¡å‹åç¨±

    Returns:
        æ˜¯å¦æ”¯æŒ JSON æ¨¡å¼
    """
    # å·²çŸ¥ä¸æ”¯æŒ JSON æ¨¡å¼çš„æ¨¡å‹
    unsupported_models = [
        "gemma",  # Google Gemma ç³»åˆ—
        "llama",  # Meta Llama éƒ¨åˆ†ç‰ˆæœ¬
    ]

    # æª¢æŸ¥æ¨¡å‹åç¨±æ˜¯å¦åŒ…å«ä¸æ”¯æŒçš„æ¨¡å‹é—œéµå­—
    model_lower = model.lower()
    for unsupported in unsupported_models:
        if unsupported in model_lower:
            return False

    # é»˜èªå‡è¨­æ”¯æŒï¼ˆGPTã€Claudeã€Gemini Pro ç­‰ï¼‰
    return True


def extract_json_from_response(response_text: str) -> dict:
    """
    å¾æ¨¡å‹éŸ¿æ‡‰ä¸­æå– JSON å…§å®¹

    è™•ç†ä»¥ä¸‹æƒ…æ³ï¼š
    1. ç´” JSON éŸ¿æ‡‰
    2. JSON åŒ…è£¹åœ¨ä»£ç¢¼å¡Šä¸­ï¼ˆ```json ... ```ï¼‰
    3. JSON å‰å¾Œæœ‰å…¶ä»–æ–‡å­—
    4. ç©ºéŸ¿æ‡‰æˆ–ç„¡æ•ˆéŸ¿æ‡‰

    Args:
        response_text: æ¨¡å‹çš„åŸå§‹éŸ¿æ‡‰æ–‡æœ¬

    Returns:
        è§£æå¾Œçš„ JSON å­—å…¸

    Raises:
        ValueError: å¦‚æœç„¡æ³•æå–æœ‰æ•ˆçš„ JSON
    """
    import re
    import json

    if not response_text or not response_text.strip():
        raise ValueError("éŸ¿æ‡‰ç‚ºç©º")

    # å˜—è©¦ 1: ç›´æ¥è§£ææ•´å€‹éŸ¿æ‡‰
    try:
        return json.loads(response_text)
    except json.JSONDecodeError:
        pass

    # å˜—è©¦ 2: æå–ä»£ç¢¼å¡Šä¸­çš„ JSONï¼ˆ```json ... ``` æˆ– ``` ... ```ï¼‰
    code_block_patterns = [
        r'```json\s*\n(.*?)\n```',  # ```json ... ```
        r'```\s*\n(.*?)\n```',       # ``` ... ```
    ]

    for pattern in code_block_patterns:
        matches = re.findall(pattern, response_text, re.DOTALL)
        if matches:
            try:
                return json.loads(matches[0])
            except json.JSONDecodeError:
                continue

    # å˜—è©¦ 3: æŸ¥æ‰¾ç¬¬ä¸€å€‹ { åˆ°æœ€å¾Œä¸€å€‹ } ä¹‹é–“çš„å…§å®¹
    first_brace = response_text.find('{')
    last_brace = response_text.rfind('}')

    if first_brace != -1 and last_brace != -1 and first_brace < last_brace:
        try:
            json_str = response_text[first_brace:last_brace + 1]
            return json.loads(json_str)
        except json.JSONDecodeError:
            pass

    # å¦‚æœæ‰€æœ‰å˜—è©¦éƒ½å¤±æ•—ï¼Œæ‹‹å‡ºéŒ¯èª¤
    raise ValueError(f"ç„¡æ³•å¾éŸ¿æ‡‰ä¸­æå–æœ‰æ•ˆçš„ JSONã€‚éŸ¿æ‡‰å‰100å€‹å­—ç¬¦: {response_text[:100]}")


def create_llm_client_from_config(config: Dict[str, str], user_client: Any = None) -> tuple:
    """
    å¾é…ç½®å‰µå»º LLM å®¢æˆ¶ç«¯

    Args:
        config: æ¨¡å‹é…ç½® {"provider": "...", "model": "..."}
        user_client: å¯é¸çš„ç”¨æˆ¶æä¾›çš„ LLM å®¢æˆ¶ç«¯ (ç”¨æ–¼ provider="user_provided" æ™‚)

    Returns:
        (client, model_name) å…ƒçµ„
    """
    provider = config.get("provider", "openai")
    model = config.get("model", "gpt-4o")

    if provider == "user_provided":
        if user_client:
            return user_client, model
        else:
            # å¦‚æœæ²’æœ‰æä¾›ç”¨æˆ¶å®¢æˆ¶ç«¯ï¼Œå›é€€åˆ° OpenAI (å‡è¨­ç³»çµ±æœ‰é»˜èª Key) 
            # æˆ–è€…æ‹‹å‡ºæ›´æ˜ç¢ºçš„éŒ¯èª¤
            print("Warning: Config specifies 'user_provided' but no user_client passed. Fallback to OpenAI.")
            provider = "openai"

    client = LLMClientFactory.create_client(provider, model)
    return client, model


# ä¾¿æ·å‡½æ•¸
def get_bull_researcher_client():
    """ç²å–å¤šé ­ç ”ç©¶å“¡çš„ LLM å®¢æˆ¶ç«¯"""
    from core.config import BULL_RESEARCHER_MODEL
    return create_llm_client_from_config(BULL_RESEARCHER_MODEL)


def get_bear_researcher_client():
    """ç²å–ç©ºé ­ç ”ç©¶å“¡çš„ LLM å®¢æˆ¶ç«¯"""
    from core.config import BEAR_RESEARCHER_MODEL
    return create_llm_client_from_config(BEAR_RESEARCHER_MODEL)


def get_trader_client():
    """ç²å–äº¤æ˜“å“¡çš„ LLM å®¢æˆ¶ç«¯"""
    from core.config import TRADER_MODEL
    return create_llm_client_from_config(TRADER_MODEL)


if __name__ == "__main__":
    # æ¸¬è©¦
    print("æ¸¬è©¦ LLM å®¢æˆ¶ç«¯å·¥å» \n")

    # æ¸¬è©¦ OpenAI
    try:
        client = LLMClientFactory.create_client("openai")
        print("âœ… OpenAI å®¢æˆ¶ç«¯å‰µå»ºæˆåŠŸ")
    except Exception as e:
        print(f"âŒ OpenAI å®¢æˆ¶ç«¯å‰µå»ºå¤±æ•—: {e}")

    # æ¸¬è©¦ OpenRouter
    try:
        client = LLMClientFactory.create_client("openrouter")
        print("âœ… OpenRouter å®¢æˆ¶ç«¯å‰µå»ºæˆåŠŸ")
    except Exception as e:
        print(f"âš ï¸  OpenRouter å®¢æˆ¶ç«¯å‰µå»ºå¤±æ•—: {e}")
        print("   æç¤º: éœ€è¦è¨­ç½® OPENROUTER_API_KEY ç’°å¢ƒè®Šé‡")

    # æ¸¬è©¦é…ç½®
    from core.config import BULL_RESEARCHER_MODEL, BEAR_RESEARCHER_MODEL

    print(f"\nå¤šé ­ç ”ç©¶å“¡: {LLMClientFactory.get_model_info(BULL_RESEARCHER_MODEL)}")
    print(f"ç©ºé ­ç ”ç©¶å“¡: {LLMClientFactory.get_model_info(BEAR_RESEARCHER_MODEL)}")

    # æ¸¬è©¦ Google Gemini
    try:
        # é€™è£¡ä¸å¯¦éš›èª¿ç”¨ generate_contentï¼Œåƒ…æ¸¬è©¦å®¢æˆ¶ç«¯æ˜¯å¦èƒ½æˆåŠŸåˆå§‹åŒ–
        # å¯¦éš›çš„æ¨¡å‹èª¿ç”¨æ‡‰åœ¨ Agent æˆ–å…¶ä»–æ¥­å‹™é‚è¼¯ä¸­è™•ç†
        client = LLMClientFactory.create_client("google_gemini")
        print("âœ… Google Gemini å®¢æˆ¶ç«¯å‰µå»ºæˆåŠŸ")
    except Exception as e:
        print(f"âŒ Google Gemini å®¢æˆ¶ç«¯å‰µå»ºå¤±æ•—: {e}")
        print("   æç¤º: éœ€è¦è¨­ç½® GOOGLE_API_KEY ç’°å¢ƒè®Šé‡")
