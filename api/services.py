import asyncio
import numpy as np
from datetime import datetime, timedelta
from typing import List, Dict, Any

from core.database import get_cache, set_cache
from core.config import (
    SUPPORTED_EXCHANGES, MARKET_PULSE_TARGETS, 
    MARKET_PULSE_UPDATE_INTERVAL, FUNDING_RATE_UPDATE_INTERVAL,
    SCREENER_UPDATE_INTERVAL_MINUTES
)
from analysis.crypto_screener import screen_top_cryptos
from analysis.market_pulse import get_market_pulse
from trading.okx_api_connector import OKXAPIConnector
from data.data_fetcher import get_data_fetcher

from api.globals import (
    logger, 
    cached_screener_result, 
    MARKET_PULSE_CACHE, 
    FUNDING_RATE_CACHE, 
    screener_lock, 
    funding_rate_lock,
    ANALYSIS_STATUS
)

# --- Market Pulse Cache Functions ---
def save_market_pulse_cache(silent=True):
    """Save Market Pulse data to DB."""
    try:
        set_cache("MARKET_PULSE", MARKET_PULSE_CACHE)
        if not silent:
            logger.info(f"Market Pulse cache saved to DB")
    except Exception as e:
        logger.error(f"Failed to save Market Pulse cache: {e}")

def load_market_pulse_cache():
    """Load Market Pulse data from DB."""
    try:
        data = get_cache("MARKET_PULSE")
        if data:
            # We modify the global dictionary in place
            MARKET_PULSE_CACHE.clear()
            MARKET_PULSE_CACHE.update(data)
            logger.info(f"Loaded Market Pulse cache from DB ({len(MARKET_PULSE_CACHE)} symbols)")
    except Exception as e:
        logger.error(f"Failed to load Market Pulse cache: {e}")

# --- Funding Rate Cache Persistence ---
def save_funding_rate_cache(silent=True):
    """Save Funding Rate data to DB (Persistence)."""
    try:
        set_cache("FUNDING_RATES", FUNDING_RATE_CACHE.get("data", {}))
        if not silent:
            logger.info(f"Funding Rate cache saved to DB")
    except Exception as e:
        logger.error(f"Failed to save Funding Rate cache: {e}")

def load_funding_rate_cache():
    """Load Funding Rate data from DB (Persistence)."""
    try:
        data = get_cache("FUNDING_RATES")
        if data:
            FUNDING_RATE_CACHE["data"] = data
            FUNDING_RATE_CACHE["timestamp"] = datetime.now().isoformat() # Mark as loaded now, even if old
            logger.info(f"Loaded Funding Rate cache from DB ({len(data)} symbols)")
            return True
        return False
    except Exception as e:
        logger.error(f"Failed to load Funding Rate cache: {e}")
        return False

# --- Background Tasks ---

async def update_funding_rates():
    """Update all funding rates from OKX."""
    async with funding_rate_lock:
        try:
            logger.info("Updating funding rates...")
            loop = asyncio.get_running_loop()
            okx = OKXAPIConnector()

            # åœ¨åŸ·è¡Œç·’æ± ä¸­åŸ·è¡Œï¼ˆå› ç‚ºæ˜¯åŒæ­¥ API èª¿ç”¨ï¼‰
            funding_rates = await loop.run_in_executor(None, okx.get_all_funding_rates)

            if "error" not in funding_rates:
                FUNDING_RATE_CACHE["timestamp"] = datetime.now().isoformat()
                FUNDING_RATE_CACHE["data"] = funding_rates
                
                # [Optimization] Persist to DB so it survives restart
                save_funding_rate_cache()
                
                logger.info(f"Funding rates updated & saved: {len(funding_rates)} symbols")
            else:
                logger.error(f"Failed to update funding rates: {funding_rates.get('error')}")
        except Exception as e:
            logger.error(f"Funding rate update error: {e}")

async def funding_rate_update_task():
    """Background task to update funding rates periodically."""
    # [Optimization] Try to load from DB immediately on startup
    if load_funding_rate_cache():
        initial_delay = 5 # If loaded, wait a bit before refreshing
    else:
        initial_delay = 1 # If empty, start refresh almost immediately

    await asyncio.sleep(initial_delay)

    # åˆå§‹æ›´æ–°
    await update_funding_rates()

    # å®šæœŸæ›´æ–°
    while True:
        await asyncio.sleep(FUNDING_RATE_UPDATE_INTERVAL)
        await update_funding_rates()

async def update_single_market_pulse(symbol: str, fixed_sources: List[str], semaphore: asyncio.Semaphore = None):
    """Helper to update a single symbol for Market Pulse with Timeout protection."""
    loop = asyncio.get_running_loop()
    
    async def _do_update():
        try:
            logger.info(f"[Background] Updating Market Pulse for {symbol}...")
            # ç‚ºåŒæ­¥å‡½æ•¸åŸ·è¡ŒåŠ ä¸Šè¶…æ™‚ä¿è­· (180ç§’)
            # æ³¨æ„: run_in_executor æœ¬èº«ä¸èƒ½ç›´æ¥ cancelï¼Œä½†åœ¨é€™è£¡ wrap ä¸€å±¤ wait_for å¯ä»¥è®“ asyncio ç¹¼çºŒå¾€ä¸‹èµ°
            result = await asyncio.wait_for(
                loop.run_in_executor(
                    None, 
                    lambda: get_market_pulse(symbol, enabled_sources=fixed_sources)
                ),
                timeout=180.0
            )
            
            if result and "error" not in result:
                MARKET_PULSE_CACHE[symbol] = result
                logger.info(f"[Background] Successfully updated {symbol}")
            else:
                logger.warning(f"[Background] Failed to update {symbol}: {result.get('error', 'Unknown error')}")
        except asyncio.TimeoutError:
            logger.error(f"[Background] â±ï¸ Timeout updating {symbol} - skipping after 180s")
        except asyncio.CancelledError:
            logger.warning(f"[Background] ğŸ›‘ Task for {symbol} was cancelled.")
            raise  # Re-raise to let the gathered task know it was cancelled
        except Exception as e:
            logger.error(f"[Background] Error updating {symbol}: {e}")

    if semaphore:
        async with semaphore:
            await _do_update()
    else:
        await _do_update()

async def refresh_all_market_pulse_data(target_symbols: List[str] = None):
    """
    Refreshes Market Pulse data with Smart Resume and Fixed Window logic.
    - Fixed Windows: 00:00, 04:00, 08:00, 12:00, 16:00, 20:00
    - Resume: Skips symbols already analyzed in the current window.
    - Consistency: All data in a window shares the same timestamp.
    """
    # 1. è¨ˆç®—ç•¶å‰ 4 å°æ™‚çª—å£çš„èµ·å§‹æ™‚é–“
    now = datetime.now()
    window_hour = (now.hour // 4) * 4
    window_start = now.replace(hour=window_hour, minute=0, second=0, microsecond=0)
    window_start_iso = window_start.isoformat()
    
    logger.info(f"ğŸ¯ Current Update Window: {window_start_iso}")

    # 2. æ±ºå®šè¦åˆ†æçš„å¹£ç¨®æ¸…å–®
    # 2. æ±ºå®šè¦åˆ†æçš„å¹£ç¨®æ¸…å–®
    final_symbols = set()
    
    # [Target Priority 1] Always analyze config targets (BTC, ETH, etc.)
    for s in MARKET_PULSE_TARGETS:
         final_symbols.add(s.upper())
         
    if target_symbols:
        # [Target Priority 0] Explicit request (e.g. from user click or API)
        for s in target_symbols:
            final_symbols.add(s.upper())
        logger.info(f"ğŸ¯ Targeted analysis request for {len(target_symbols)} symbols: {target_symbols}")
    else:
        # [Target Priority 2] Analyze what's currently popular/visible in Screener
        # Instead of fetching fresh top 50, we look at what the user is likely seeing
        screener_data = cached_screener_result.get("data", {})
        
        added_count = 0
        if screener_data:
            # Add Top Volume
            for item in screener_data.get("top_volume", []):
                if item.get("Symbol"): 
                    final_symbols.add(item["Symbol"].upper())
                    added_count += 1
            
            # Add Top Gainers
            for item in screener_data.get("top_gainers", []):
                if item.get("Symbol"):
                    final_symbols.add(item["Symbol"].upper())
                    added_count += 1
            
            # Add Top Losers
            for item in screener_data.get("top_losers", []):
                if item.get("Symbol"):
                    final_symbols.add(item["Symbol"].upper())
                    added_count += 1
                    
        if added_count > 0:
            logger.info(f"ğŸ¯ Added {added_count} symbols from current Screener results for analysis.")
        else:
            # Fallback if screener is empty: Use Config Targets only, or minimal fetch
            logger.info("âš ï¸ No screener data found. Analyzing minimal config targets only.")

    # æ¨™æº–åŒ–ä¸¦éæ¿¾å·²å®Œæˆçš„å¹£ç¨®
    symbols_to_process = []
    skipped_count = 0
    
    # è¨ºæ–·ï¼šç¢ºèªç•¶å‰è¨˜æ†¶é«”ä¸­çš„å¿«å–ç‹€æ…‹
    current_cache_size = len(MARKET_PULSE_CACHE)
    logger.info(f"ğŸ“¦ [æ•¸æ“šè¨ºæ–·] å¿«å–ä¸­ç¾æœ‰ {current_cache_size} å€‹å¹£ç¨®æ•¸æ“š")
    
    for s in final_symbols:
        # çµ±ä¸€æ¨™æº–åŒ–ï¼šåªå–ç¬¬ä¸€æ®µä¸¦å¤§å¯« (ä¾‹å¦‚ BTC-USDT -> BTC)
        norm = s.split("-")[0].split("/")[0].upper()
        if not norm: continue
        
        existing = MARKET_PULSE_CACHE.get(norm)
        if existing and "timestamp" in existing:
            try:
                cache_time = datetime.fromisoformat(existing["timestamp"])
                if cache_time >= window_start:
                    skipped_count += 1
                    continue
            except:
                pass
        
        symbols_to_process.append(norm)
    
    if skipped_count > 0:
        logger.info(f"â­ï¸ [Resume] åµæ¸¬åˆ°ç•¶å‰çª—å£ ({window_hour}:00) å·²æœ‰ {skipped_count} å€‹å¹£ç¨®ï¼Œå°‡è‡ªå‹•è·³éã€‚")
    else:
        logger.info(f"â„¹ï¸ [New Run] ç•¶å‰çª—å£ ({window_hour}:00) å°šç„¡å®Œæˆè¨˜éŒ„ï¼Œé–‹å§‹å…¨é‡åˆ†æã€‚")

    if not symbols_to_process:
        return window_start_iso

    # Init Progress
    ANALYSIS_STATUS["is_running"] = True
    ANALYSIS_STATUS["total"] = len(symbols_to_process) + skipped_count 
    ANALYSIS_STATUS["completed"] = skipped_count
    ANALYSIS_STATUS["start_time"] = now.isoformat()
    
    FIXED_SOURCES = ['google', 'cryptopanic', 'newsapi', 'cryptocompare']
    sem = asyncio.Semaphore(2)  # æ¸›å°‘ä¸¦ç™¼æ•¸ï¼Œé¿å… OKX API SSL é€£æ¥å•é¡Œ
    
    # [Optimization] Batch saving variables
    unsaved_changes = 0
    BATCH_SIZE = 10  # Reduced DB write frequency by 90%
    save_lock = asyncio.Lock()

    async def _safe_save_cache():
        """Helper to save cache securely using lock"""
        async with save_lock:
            loop = asyncio.get_running_loop()
            await loop.run_in_executor(None, set_cache, "MARKET_PULSE", MARKET_PULSE_CACHE)
            logger.info(f"ğŸ’¾ [Batch Save] Saved Market Pulse cache to DB")

    async def _tracked_update(sym):
        nonlocal unsaved_changes
        try:
            await update_single_market_pulse(sym, FIXED_SOURCES, semaphore=sem)
            
            # Update success behavior: Mark timestamp but DON'T save immediately
            if sym in MARKET_PULSE_CACHE:
                MARKET_PULSE_CACHE[sym]["timestamp"] = window_start_iso
                
                # Check if we should trigger a batch save
                should_save = False
                async with save_lock: # Reuse lock for counter safety if needed, or just be loose about it
                     unsaved_changes += 1
                     if unsaved_changes >= BATCH_SIZE:
                         should_save = True
                         unsaved_changes = 0 # Reset counter
                
                if should_save:
                    await _safe_save_cache()

        finally:
            ANALYSIS_STATUS["completed"] += 1

    # Execute Tasks
    tasks = []
    for sym in symbols_to_process:
        tasks.append(_tracked_update(sym))
        
    await asyncio.gather(*tasks)
    
    ANALYSIS_STATUS["is_running"] = False
    logger.info(f"âœ… Market Pulse Analysis completed for {len(tasks)} symbols.")
    
    # Final Save
    save_market_pulse_cache(silent=False)
    return window_start_iso

async def trigger_on_demand_analysis(symbols: List[str]):
    """
    Trigger immediate background analysis for specific symbols if they are stale.
    Designed for "Analyze what user is interested in" feature.
    """
    if not symbols: return
    
    # 1. Check validity (skip if recently updated)
    now = datetime.now()
    # Align to current 4-hour window
    window_hour = (now.hour // 4) * 4
    window_start = now.replace(hour=window_hour, minute=0, second=0, microsecond=0)
    
    to_update = []
    for s in symbols:
        norm = s.upper().split('-')[0]
        # logic: if cache exists AND timestamp is >= window_start, it's fresh enough
        existing = MARKET_PULSE_CACHE.get(norm)
        is_fresh = False
        if existing and "timestamp" in existing:
            try:
                ts = datetime.fromisoformat(existing["timestamp"])
                if ts >= window_start:
                    is_fresh = True
            except: pass
            
        if not is_fresh:
            to_update.append(norm)
            
    if not to_update:
        # logger.info("âœ¨ Requested symbols are already fresh.")
        return

    logger.info(f"ğŸš€ Triggering On-Demand Analysis for: {to_update}")
    
    # 2. Launch background updates
    # We use a semaphore to prevent overwhelming the API, but separate from the main loop
    sem = asyncio.Semaphore(4) 
    FIXED_SOURCES = ['google', 'cryptopanic', 'newsapi', 'cryptocompare']
    
    async def _runner(sym):
        try:
            # Re-use update_single_market_pulse
            await update_single_market_pulse(sym, FIXED_SOURCES, semaphore=sem)
            
            # Simple in-memory update mark (save is handled by background loop occasionally, or next startup)
            # But for good UX, let's update cache timestamp
            if sym in MARKET_PULSE_CACHE:
                MARKET_PULSE_CACHE[sym]["timestamp"] = window_start.isoformat()
        except Exception as e:
            logger.error(f"On-demand analysis failed for {sym}: {e}")

    # Fire and forget (create task)
    for sym in to_update:
        asyncio.create_task(_runner(sym))

async def update_market_pulse_task():
    """
    èƒŒæ™¯ä»»å‹™ï¼šå®šæœŸæ›´æ–°å¸‚å ´è„ˆå‹•åˆ†æã€‚

    âœ… æ™ºèƒ½æ’ç¨‹ç­–ç•¥ï¼š
    1. å•Ÿå‹•æ™‚å„ªå…ˆä¿¡ä»»ç¾æœ‰å¿«å–
    2. åªæœ‰åœ¨å¿«å–ç¢ºå¯¦éæœŸï¼ˆè¶…é 4 å°æ™‚ï¼‰æ™‚æ‰è§¸ç™¼æ›´æ–°
    3. å®šæœŸé€±æœŸæ€§æ›´æ–°
    """
    logger.info("ğŸš€ [Background] Initializing Market Pulse task...")

    loop = asyncio.get_running_loop()
    initial_sleep_time = 0

    try:
        # --- æ­¥é©Ÿ 1: æª¢æŸ¥ç¾æœ‰å¿«å–ç‹€æ…‹ï¼ˆå„ªå…ˆä¿¡ä»»å¿«å–ï¼‰---
        now = datetime.now()
        cache_count = len(MARKET_PULSE_CACHE)
        logger.info(f"ğŸ“¦ Current cache has {cache_count} symbols")

        # å¦‚æœå¿«å–æœ‰æ•¸æ“šï¼Œæª¢æŸ¥æœ€æ–°çš„æ™‚é–“æˆ³
        if cache_count > 0:
            timestamps = []
            for sym, data in MARKET_PULSE_CACHE.items():
                if data and "timestamp" in data:
                    try:
                        ts = datetime.fromisoformat(data["timestamp"])
                        timestamps.append(ts)
                    except ValueError:
                        pass

            if timestamps:
                newest_ts = max(timestamps)
                oldest_ts = min(timestamps)
                newest_age = (now - newest_ts).total_seconds()
                oldest_age = (now - oldest_ts).total_seconds()

                logger.info(f"ğŸ“Š Cache age: newest={newest_age/60:.1f}min, oldest={oldest_age/60:.1f}min (threshold={MARKET_PULSE_UPDATE_INTERVAL/60:.0f}min)")

                # å¦‚æœæœ€è€çš„æ•¸æ“šéƒ½é‚„æ²’éæœŸï¼Œå°±ä¸éœ€è¦æ›´æ–°
                if oldest_age < MARKET_PULSE_UPDATE_INTERVAL:
                    remaining_time = MARKET_PULSE_UPDATE_INTERVAL - oldest_age
                    logger.info(f"âœ… All cache is fresh! Sleeping for {remaining_time/60:.1f} minutes until next update.")
                    initial_sleep_time = remaining_time
                elif oldest_age < MARKET_PULSE_UPDATE_INTERVAL * 3:
                    # [å„ªåŒ–] å¦‚æœæ•¸æ“šåªæ˜¯ã€Œç¨å¾®éæœŸã€(ä¾‹å¦‚éæœŸå¹¾å°æ™‚)ï¼Œä¸è¦åœ¨å•Ÿå‹•æ™‚é˜»å¡åšå…¨é‡æ›´æ–°
                    # å…ˆè®“ API å¯ä»¥è®€å–é€™äº›èˆŠæ•¸æ“šï¼Œç„¶å¾Œåœ¨èƒŒæ™¯ç¨å¾Œ(1åˆ†é˜å¾Œ)å†é–‹å§‹æ›´æ–°
                    logger.info(f"âš ï¸ Cache is stale ({oldest_age/3600:.1f}h) but usable for startup. Skipping immediate blocking refresh.")
                    initial_sleep_time = 60 # 1åˆ†é˜å¾Œå†é–‹å§‹èƒŒæ™¯æ›´æ–°
                else:
                    # æœ‰åš´é‡éæœŸæ•¸æ“š (>12å°æ™‚)ï¼Œè§¸ç™¼æ›´æ–°
                    logger.info(f"â° Cache is too old ({oldest_age/3600:.1f}h). Triggering immediate refresh...")
                    await refresh_all_market_pulse_data()
                    initial_sleep_time = MARKET_PULSE_UPDATE_INTERVAL
            else:
                # æ²’æœ‰æœ‰æ•ˆæ™‚é–“æˆ³ï¼Œè§¸ç™¼æ›´æ–°
                logger.info("âš ï¸ No valid timestamps in cache. Triggering refresh...")
                await refresh_all_market_pulse_data()
                initial_sleep_time = MARKET_PULSE_UPDATE_INTERVAL
        else:
            # å¿«å–ç‚ºç©ºï¼Œè§¸ç™¼å…¨é‡æ›´æ–°
            logger.info("ğŸ“­ Cache is empty. Triggering initial full refresh...")
            await refresh_all_market_pulse_data()
            initial_sleep_time = MARKET_PULSE_UPDATE_INTERVAL

    except Exception as e:
        logger.error(f"âš ï¸ Error during startup check: {e}. Falling back to immediate full update.")
        try:
            await refresh_all_market_pulse_data()
            initial_sleep_time = MARKET_PULSE_UPDATE_INTERVAL
        except Exception as ex:
            logger.error(f"âŒ Fallback update failed: {ex}")
            initial_sleep_time = 300

    # 2. é€²å…¥é€±æœŸæ€§æ›´æ–°è¿´åœˆ
    while True:
        # ç­‰å¾…ä¸‹ä¸€æ¬¡æ›´æ–°æ™‚é–“
        if initial_sleep_time > 0:
            logger.info(f"ğŸ’¤ Sleeping for {initial_sleep_time}s...")
            await asyncio.sleep(initial_sleep_time)
        
        try:
            logger.info("ğŸ”„ [Background] Starting scheduled Market Pulse update cycle...")
            # é€±æœŸæ€§ä»»å‹™åŸ·è¡Œå…¨é‡æ›´æ–° (å‚³å…¥ None æœƒè‡ªå‹•æŠ“æ‰€æœ‰)
            await refresh_all_market_pulse_data() 
            logger.info("âœ… [Background] Scheduled update cycle complete.")
        except Exception as e:
            logger.error(f"âŒ [Background] Market Pulse update cycle error: {e}")
        
        # [Optimization] Align next update to the next 4-hour wall clock mark
        # Instead of sleeping fixed 4 hours, we sleep until 00:00, 04:00, 08:00...
        now = datetime.now()
        current_block = now.hour // 4
        next_block_hour = (current_block + 1) * 4
        
        # Handle day rollover (e.g. 24 -> 00 of next day)
        if next_block_hour >= 24:
            next_target = now.replace(hour=0, minute=0, second=0, microsecond=0) + timedelta(days=1)
        else:
            next_target = now.replace(hour=next_block_hour, minute=0, second=0, microsecond=0)
            
        # Add a small buffer (e.g., 60 seconds) to ensure data providers have closed their candles
        next_target += timedelta(seconds=60)
        
        seconds_until_next = (next_target - datetime.now()).total_seconds()
        
        # Safety check: if calculation is weird, fallback to 5 minutes
        if seconds_until_next < 0:
             seconds_until_next = 300
             
        initial_sleep_time = seconds_until_next
        logger.info(f"ğŸ“… Next scheduled update aligned to: {next_target} (in {initial_sleep_time/60:.1f} minutes)")

async def update_screener_prices_fast():
    """
    å¿«é€Ÿæ›´æ–°ä»»å‹™ï¼šåªæ›´æ–° cached_screener_result ä¸­çš„åƒ¹æ ¼è³‡è¨Šã€‚
    ä¸é‡æ–°è¨ˆç®—æŒ‡æ¨™æˆ–æ’åï¼Œåƒ…æŠ“å–ç•¶å‰æœ€æ–°åƒ¹æ ¼ (Ticker)ã€‚
    """
    if cached_screener_result["data"] is None:
        return

    try:
        # ä½¿ç”¨ OKX ç²å–æœ€æ–°åƒ¹æ ¼ (Ticker)
        fetcher = get_data_fetcher("okx") 
        
        # æ”¶é›†æ‰€æœ‰éœ€è¦æ›´æ–°çš„ Symbol
        symbols = set()
        data = cached_screener_result["data"]
        
        for list_name in ["top_performers", "oversold", "overbought"]:
            if data.get(list_name):
                for item in data[list_name]:
                    symbols.add(item["Symbol"])
        
        if not symbols:
            return

        loop = asyncio.get_running_loop()
        # OKX get_tickers returns a different structure, but Binance format is used in price_map
        # We'll adapt it to use OKX's ticker structure
        tickers_result = await loop.run_in_executor(None, lambda: fetcher._make_request(fetcher.market_base_url, "/market/tickers", params={"instType": "SPOT"}))
        
        if not tickers_result or tickers_result.get("code") != "0":
            return
            
        all_tickers = tickers_result.get("data", [])

        # å»ºç«‹åƒ¹æ ¼æŸ¥æ‰¾è¡¨ {symbol: {price, change_percent}}
        price_map = {}
        for t in all_tickers:
            instId = t['instId'] # e.g. BTC-USDT
            symbol_key = instId.replace("-", "") # BTCUSDT
            price_map[symbol_key] = {
                'price': float(t['last']),
                'change': ((float(t['last']) - float(t['open24h'])) / float(t['open24h']) * 100) if float(t['open24h']) != 0 else 0
            }

        # æ›´æ–°å¿«å–ä¸­çš„æ•¸æ“š
        updated = False
        for list_name in ["top_performers", "oversold", "overbought"]:
            if data.get(list_name):
                for item in data[list_name]:
                    s = item["Symbol"].replace("/", "").replace("-", "")
                    if s in price_map:
                        item["Close"] = price_map[s]['price']
                        item["price_change_24h"] = price_map[s]['change']
                        updated = True
        
        if updated:
            if data.get("top_performers"):
                data["top_performers"].sort(key=lambda x: float(x.get("price_change_24h", 0)), reverse=True)

            timestamp_str = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            cached_screener_result["data"]["last_updated"] = timestamp_str
            cached_screener_result["timestamp"] = timestamp_str
            
    except Exception as e:
        pass

async def run_screener_analysis():
    """åŸ·è¡Œå¯¦éš›çš„åˆ†æå·¥ä½œä¸¦æ›´æ–°å¿«å– (é‡å‹ä»»å‹™)"""
    # å¦‚æœå·²ç¶“é–å®šï¼Œè¡¨ç¤ºæ­£åœ¨é‹è¡Œï¼Œç„¡éœ€é‡è¤‡
    if screener_lock.locked():
        logger.info("Screener analysis already in progress, skipping...")
        return
        
    async with screener_lock:
        logger.info("Starting heavy screener analysis...")
        try:
            exchange = SUPPORTED_EXCHANGES[0]
            loop = asyncio.get_running_loop()
            
            # Use lightweight screener for background task
            from analysis.crypto_screener_light import screen_top_cryptos_light
            
            df_volume, df_gainers, df_losers, _ = await loop.run_in_executor(
                None,
                lambda: screen_top_cryptos_light(
                    exchange=exchange,
                    limit=10,
                    interval="1d",
                    target_symbols=None
                )
            )

            # Handle potential None/NaN
            top_volume = df_volume.replace({np.nan: None}) if not df_volume.empty else df_volume
            top_gainers = df_gainers.replace({np.nan: None}) if not df_gainers.empty else df_gainers
            top_losers = df_losers.replace({np.nan: None}) if not df_losers.empty else df_losers
            
            # åªæœ‰ç•¶æœ‰æ•¸æ“šæ™‚æ‰æ›´æ–°å¿«å–ï¼Œé¿å…å› ç¶²è·¯éŒ¯èª¤å°è‡´å¿«å–è¢«æ¸…ç©º
            if not top_volume.empty or not top_gainers.empty:
                timestamp_str = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                cached_screener_result["timestamp"] = timestamp_str
                cached_screener_result["data"] = {
                    "top_volume": top_volume.to_dict(orient="records"),
                    "top_gainers": top_gainers.to_dict(orient="records"),
                    "top_losers": top_losers.to_dict(orient="records"),
                    "last_updated": timestamp_str
                }
                
                # [Optimization] RAM Only - No DB write
                logger.info(f"Background screener analysis complete (RAM updated). (Volume: {len(top_volume)}, Gainers: {len(top_gainers)}, Losers: {len(top_losers)})")
            else:
                logger.warning("Background screener analysis returned empty results. Skipping cache update.")
                
            # è§£é™¤é–å®šç”± async with è™•ç†
            
        except Exception as e:
            logger.error(f"âŒ [åˆ†æä»»å‹™] åŸ·è¡Œå¤±æ•—: {e}")

async def update_screener_task():
    """
    èƒŒæ™¯ä»»å‹™ï¼š
    1. æ¯ç§’åŸ·è¡Œå¿«é€Ÿåƒ¹æ ¼æ›´æ–° (Fast Update)
    2. æ¯ N åˆ†é˜åŸ·è¡Œå®Œæ•´åˆ†æ (Heavy Analysis) - é™ä½é »ç‡ä»¥é¿å… API å°ç¦
    """
    logger.info("ğŸš€ Starting initial Screener analysis (In-Memory)...")
    # Immediately run analysis on startup
    await run_screener_analysis()
    
    # [Optimization] Use config variable (minutes -> seconds)
    update_interval_sec = SCREENER_UPDATE_INTERVAL_MINUTES * 60
    logger.info(f"Screener background task interval set to {SCREENER_UPDATE_INTERVAL_MINUTES} min ({update_interval_sec}s)")

    counter = 1
    while True:
        # æ¯ç§’éƒ½å˜—è©¦æ›´æ–°åƒ¹æ ¼
        asyncio.create_task(update_screener_prices_fast())
        
        # å®šæœŸåŸ·è¡Œå®Œæ•´åˆ†æ
        if counter % update_interval_sec == 0:
            asyncio.create_task(run_screener_analysis())
            counter = 0 # Reset counter to prevent overflow
        
        counter += 1
        await asyncio.sleep(1)
