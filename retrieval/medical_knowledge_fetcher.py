import asyncio
from core import config
import json
import time
from typing import List, Optional

import opencc
from langchain_core.messages import SystemMessage, HumanMessage
from langchain_text_splitters import RecursiveCharacterTextSplitter

from rag_system.rag_system_core import convert_references_to_english
from rag_system.rag_jsonl import (
    search_vectordb_with_scores,
    load_existing_vectordb,
)
from core.config import DB_CONNECTION_STRING, _VECTORSTORE_CACHE, RETRIEVAL_CONFIG
from utils.prompt_function import get_criteria_matching_prompt, get_system_prompt_for_multi_source  # æ–°å¢ï¼šå¾ prompts å°å…¥
from retrieval.clue_extraction import process_retrieval_with_clue_extraction  # æ–°å¢ï¼šä½¿ç”¨ç·šç´¢æå–

# ========================================
# å…¨å±€é…ç½®
# ========================================

# ç¹ç°¡è½‰æ›å™¨
convert = opencc.OpenCC('s2tw.json')

# ========================================
# å·¥å…·å‡½æ•¸
# ========================================

def count_tokens(text: str) -> int:
    """ä¼°ç®— token æ•¸é‡"""
    if not isinstance(text, str):
        text = str(text)
    return max(1, int(len(text) / 1.3))


def create_text_splitter(chunk_size: int = 3000, chunk_overlap: int = 256) -> RecursiveCharacterTextSplitter:
    """å‰µå»ºæ–‡æœ¬åˆ†å‰²å™¨"""
    return RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separators=["\n\n", "\n", "ã€‚", "!", "?", ";", ",", " ", ""],
        length_function=count_tokens
    )

# ========================================
# VectorStore ç®¡ç†
# ========================================

async def get_or_create_vectordb(connection_string: str, collection_name: str):
    """ç²å–æˆ–å‰µå»º vectorstoreï¼ˆä½¿ç”¨å¿«å–é¿å…é‡è¤‡è¼‰å…¥ï¼‰"""
    key = (connection_string, collection_name)
    
    if key not in _VECTORSTORE_CACHE:
        _VECTORSTORE_CACHE[key] = load_existing_vectordb(connection_string, collection_name)
    
    return _VECTORSTORE_CACHE[key]


# ========================================
# LLM èª¿ç”¨
# ========================================

async def safe_llm_invoke(messages: List):
    """å®‰å…¨çš„ LLM èª¿ç”¨ï¼ˆç„¡ä¸¦ç™¼é™åˆ¶ï¼‰"""
    try:
        response = await config.llm.ainvoke(messages)
        return response
    except Exception as e:
        print(f"âŒ LLM èª¿ç”¨å¤±æ•—: {e}")
        raise


# ========================================
# æ ¸å¿ƒç•°æ­¥å‡½æ•¸
# ========================================

async def find_best_matching_criteria(query: str) -> str:
    """åŒ¹é…æœ€ä½³é†«ç™‚æº–å‰‡"""
    system_prompt = get_criteria_matching_prompt()  # ä¿®æ”¹ï¼šå¾ prompts.py å°å…¥
    
    try:
        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=f"å•é¡Œ: {query}")
        ]
        response = await safe_llm_invoke(messages)
        result = response.content.strip()
        return result if result != "No Answer" else "No Answer"
    except Exception as e:
        print(f"âŒ æº–å‰‡åŒ¹é…å¤±æ•—: {e}")
        return "No Answer"


# ========================================
# æ‘˜è¦è™•ç†ï¼ˆä¿®æ”¹ç‚ºæ”¯æ´å¤šä¾†æºï¼‰
# ========================================
# åˆªé™¤åŸæœ‰çš„ get_system_prompt_for_multi_source å‡½æ•¸
# ç¾åœ¨å¾ prompts.py å°å…¥

async def _summarize_chunk_multi_source(
    chunk_text: str,
    focus_query: Optional[str] = None,
    data_type: Optional[str] = None,
    system_prompt: Optional[str] = None,
    is_final: bool = False
) -> str:
    """æ‘˜è¦å–®å€‹æ–‡æœ¬å¡Šï¼ˆå¤šä¾†æºç‰ˆæœ¬ï¼‰"""
    if is_final:
        system_prompt += """\n\nã€é‡è¦ã€‘é€™æ˜¯æœ€çµ‚æ•´ç†ï¼Œè«‹ç¢ºä¿æ¯å€‹è³‡æ–™ä¾†æºçš„ç­”æ¡ˆéƒ½ç¨ç«‹å‘ˆç¾ï¼Œ
        ä¸¦ä¿ç•™æ‰€æœ‰åƒè€ƒè³‡æ–™ä¾†æºå®Œæ•´æª”æ¡ˆåç¨±æ¨™ï¼Œè¼¸å‡ºæ ¼å¼è«‹ä¾ç…§ä¸‹æ–¹ï¼š
        è«‹åš´æ ¼éµå®ˆä½¿ç”¨ä»¥ä¸‹æ ¼å¼ï¼Œä¸¦è«‹æ³¨æ„æ–‡ä»¶åç¨±ä¸è¦é‡è¤‡ï¼Œå¦å‰‡æ‡‰è©²åˆä½µå…§å®¹å¾Œè¡¨è¿°
        ã€è¼¸å‡ºæ ¼å¼ã€‘
        **åƒè€ƒä¾æ“š**
        ã€Šæ–‡ä»¶åç¨±1ã€‹
            -å…§æ–‡å…§å®¹1
            -å…§æ–‡å…§å®¹2
            -å…§æ–‡å…§å®¹3
        å¦‚æœé‚„æœ‰æ›´å¤šå…§æ–‡å…§å®¹ï¼Œä¾æ­¤é¡æ¨...
        ã€Šæ–‡ä»¶åç¨±2ã€‹-å…§æ–‡å…§å®¹2
            -å…§æ–‡å…§å®¹1
            -å…§æ–‡å…§å®¹2
            -å…§æ–‡å…§å®¹3
        å¦‚æœé‚„æœ‰æ›´å¤šå…§æ–‡å…§å®¹ï¼Œä¾æ­¤é¡æ¨...
        ã€Šæ–‡ä»¶åç¨±3ã€‹-å…§æ–‡å…§å®¹3
            -å…§æ–‡å…§å®¹1
            -å…§æ–‡å…§å®¹2
            -å…§æ–‡å…§å®¹3
        å¦‚æœé‚„æœ‰æ›´å¤šå…§æ–‡å…§å®¹ï¼Œä¾æ­¤é¡æ¨...
        ã€Šæ–‡ä»¶åç¨±4ã€‹-å…§æ–‡å…§å®¹4
            -å…§æ–‡å…§å®¹1
            -å…§æ–‡å…§å®¹2
            -å…§æ–‡å…§å®¹3
        å¦‚æœé‚„æœ‰æ›´å¤šå…§æ–‡å…§å®¹ï¼Œä¾æ­¤é¡æ¨...

        ...ä¾æ­¤é¡æ¨ï¼ˆæœ‰åƒè€ƒåˆ°çš„ç—‡ç‹€éƒ½å¿…é ˆçµ¦å‡ºä»–çš„åƒè€ƒæ–‡ç»åç¨±ä»¥åŠå…§å®¹ã€‚
        """
    
    user_content = f"åŸå§‹è³‡æ–™ï¼š\n{chunk_text}"
    if focus_query:
        user_content = f"ç”¨æˆ¶æŸ¥è©¢ï¼šã€Œ{focus_query}ã€\n\n" + user_content
    
    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=user_content)
    ]
    
    try:
        response = await safe_llm_invoke(messages)
        cleaned_response = response.content.strip()
        
        if not cleaned_response or len(cleaned_response.replace("(", "").replace(")", "")) < 3:
            return "ï¼ˆç„¡æ³•æå–æœ‰æ•ˆæ‘˜è¦ï¼‰"
        
        return cleaned_response
    except Exception as e:
        print(f"âŒ æ‘˜è¦å¤±æ•—: {e}")
        return "ï¼ˆæ•´ç†æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼‰"


async def summarize_with_llm_multi_source(
    raw_content,
    focus_query: Optional[str] = None,
    data_type: Optional[str] = None,
    verbose: bool = False
) -> str:
    """ä½¿ç”¨ LLM é€²è¡Œå¤šä¾†æºæ‘˜è¦ï¼ˆæ”¯æ´è¶…é•·å…§å®¹åˆ†å¡Šï¼‰"""
    if isinstance(raw_content, (list, dict)):
        try:
            content_str = json.dumps(raw_content, ensure_ascii=False, indent=2)
        except:
            content_str = str(raw_content)
    else:
        content_str = str(raw_content) if raw_content is not None else ""
    
    if not content_str.strip():
        return "ï¼ˆç„¡æœ‰æ•ˆå…§å®¹ï¼‰"
    
    system_prompt = get_system_prompt_for_multi_source(data_type)
    total_tokens = count_tokens(content_str + system_prompt + (focus_query or ""))
    
    if verbose:
        print(f"ğŸ“Š åˆå§‹å…§å®¹é•·åº¦: {total_tokens} tokens")

    # ====== æ–°å¢ï¼šå¦‚æœè¶…éå®‰å…¨é–¾å€¼ï¼ˆä¾‹å¦‚ 7000ï¼‰ï¼Œå°±åˆ†å¡Šè™•ç† ======
    MAX_SAFE_TOKENS = 8192  # ç•™ç©ºé–“çµ¦ prompt å’Œè¼¸å‡º
    if total_tokens <= MAX_SAFE_TOKENS:
        # å®‰å…¨ï¼Œç›´æ¥æ‘˜è¦
        return await _summarize_chunk_multi_source(
            chunk_text=content_str,
            focus_query=focus_query,
            data_type=data_type,
            system_prompt=system_prompt,
            is_final=True
        )
    
    # ====== è¶…é•·ï¼šåˆ†å¡Šè™•ç† ======
    if verbose:
        print("âš ï¸ å…§å®¹éé•·ï¼Œå•Ÿç”¨åˆ†å¡Šæ‘˜è¦...")
    
    # å»ºç«‹ text splitterï¼ˆä½¿ç”¨ä½ å·²å®šç¾©çš„ï¼‰
    splitter = create_text_splitter(chunk_size=8192, chunk_overlap=200)
    chunks = splitter.split_text(content_str)
    
    if verbose:
        print(f"âœ‚ï¸ åˆ‡åˆ†ç‚º {len(chunks)} å€‹æ–‡æœ¬å¡Š")

    # ä¸¦è¡Œæ‘˜è¦æ¯å¡Š
    async def summarize_single_chunk(i, chunk):
        summary = await _summarize_chunk_multi_source(
            chunk_text=chunk,
            focus_query=focus_query,
            data_type=data_type,
            system_prompt=system_prompt,
            is_final=False  # ä¸æ˜¯æœ€çµ‚
        )
        return f"ã€ç‰‡æ®µ {i+1}ã€‘\n{summary}"

    chunk_summaries = await asyncio.gather(
        *[summarize_single_chunk(i, chunk) for i, chunk in enumerate(chunks)]
    )

    # åˆä½µæ‰€æœ‰ç‰‡æ®µæ‘˜è¦ï¼Œå†åšä¸€æ¬¡æœ€çµ‚æ•´ç†ï¼ˆå¯é¸ï¼‰
    combined_summary = "\n\n".join(chunk_summaries)
    
    # å¯é¸ï¼šå†é€ä¸€æ¬¡çµ¦ LLM åšæœ€çµ‚æ•´åˆï¼ˆä½†è¦å°å¿ƒé•·åº¦ï¼ï¼‰
    # é€™è£¡ä¿å®ˆèµ·è¦‹ï¼Œç›´æ¥è¿”å›åˆä½µçµæœ
    return combined_summary

# ========================================
# æœ€çµ‚ç­”æ¡ˆåˆä½µï¼ˆä¿®æ”¹ç‚ºä¸æ•´åˆï¼Œåªæ˜¯æ’ç‰ˆï¼‰
# ========================================

async def combine_multi_source_answers(
    pdf_summary: str,
    rag_summary: str,
    public_rag_summary: str,
    original_query: str
) -> str:
    """åˆä½µå¤šä¾†æºç­”æ¡ˆï¼ˆæ¥µç°¡ç‰ˆæœ¬ï¼Œç´”ç‚º LLM å„ªåŒ–ï¼‰"""

    # æª¢æŸ¥æ‘˜è¦æ˜¯å¦æœ‰æ•ˆ
    invalid_markers = [
        "ï¼ˆå…§å®¹èˆ‡å•é¡Œç„¡é—œï¼‰",
        "ï¼ˆç„¡æœ‰æ•ˆå…§å®¹ï¼‰",
        "ï¼ˆä»»å‹™åŸ·è¡Œå¤±æ•—ï¼‰",
        "ï¼ˆæ‘˜è¦å¤±æ•—ï¼‰",
        "ï¼ˆæ•´ç†æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼‰"
    ]

    def is_valid(summary: str) -> bool:
        has_invalid_marker = any(marker in summary for marker in invalid_markers)
        is_long_enough = len(summary.strip()) >= 10
        result = not has_invalid_marker and is_long_enough

        # ğŸ”§ èª¿è©¦ï¼šé¡¯ç¤ºé©—è­‰éç¨‹
        if summary:
            print(f"   é©—è­‰ '{summary[:30]}...': æœ‰æ•ˆæ¨™è¨˜={not has_invalid_marker}, é•·åº¦è¶³å¤ ={is_long_enough}, çµæœ={result}")

        return result

    # æ”¶é›†æœ‰æ•ˆç­”æ¡ˆ
    valid_answers = []

    if is_valid(pdf_summary):
        #print(f"   âœ… pdf_summary æœ‰æ•ˆï¼ŒåŠ å…¥çµæœ")
        valid_answers.append(f"ä¾†æºï¼šPDFæº–å‰‡\n{pdf_summary}")
    else:
        print(f"   âŒ pdf_summary ç„¡æ•ˆ")

    if is_valid(rag_summary):
        #print(f"   âœ… rag_summary æœ‰æ•ˆï¼ŒåŠ å…¥çµæœ")
        valid_answers.append(f"ä¾†æºï¼šé†«ç™‚çŸ¥è­˜åº«\n{rag_summary}")
    else:
        print(f"   âŒ rag_summary ç„¡æ•ˆ")

    if is_valid(public_rag_summary):
        #print(f"   âœ… public_rag_summary æœ‰æ•ˆï¼ŒåŠ å…¥çµæœ")
        valid_answers.append(f"ä¾†æºï¼šè¡›æ•™åœ’åœ°\n{public_rag_summary}")
    else:
        print(f"   âŒ public_rag_summary ç„¡æ•ˆ")

    # å¦‚æœæ²’æœ‰æœ‰æ•ˆç­”æ¡ˆ
    if not valid_answers:
        print(f"âš ï¸ combine_multi_source_answers: æ²’æœ‰æœ‰æ•ˆç­”æ¡ˆï¼Œè¿”å› 'No Answer'")
        return "No Answer"

    # ç°¡å–®åˆ†éš”
    result = "\n---\n".join(valid_answers)
    #print(f"âœ… combine_multi_source_answers è¿”å›: {len(result)} å­—å…ƒ, {len(valid_answers)} å€‹ä¾†æº")
    return result


# ========================================
# æŸ¥è©¢ä»»å‹™å®šç¾©ï¼ˆä½¿ç”¨å¤šä¾†æºæ‘˜è¦ï¼‰
# ========================================

async def task_pdf_rag(user_query: str, matched_criteria: str) -> str:
    """PDF æº–å‰‡æª¢ç´¢ï¼ˆä½¿ç”¨ç·šç´¢æå–ï¼‰"""
    cname = convert_references_to_english(matched_criteria)
    if cname == "No Answer":
        return "ï¼ˆç„¡æœ‰æ•ˆå…§å®¹ï¼‰"

    vectorstore = await get_or_create_vectordb(DB_CONNECTION_STRING, cname)
    results = search_vectordb_with_scores(vectorstore, user_query, k=RETRIEVAL_CONFIG['pdf_rag_k'])
    # ä½¿ç”¨ç·šç´¢æå–ä»£æ›¿åŸæœ¬çš„æ‘˜è¦ï¼ˆç¾åœ¨è¿”å›ä¸‰å€‹å€¼ï¼‰
    formatted_knowledge, sources, _ = await process_retrieval_with_clue_extraction(
        sub_question=user_query,
        documents_with_scores=results,
        main_question=user_query,
        store=None,  # å¯ä»¥é¸æ“‡æ€§åŠ å…¥ store é€²è¡Œå¿«å–
        user_id="default"
    )

    if not formatted_knowledge:
        return "ï¼ˆç„¡æœ‰æ•ˆå…§å®¹ï¼‰"

    return formatted_knowledge


async def task_medical_rag(user_query: str, matched_criteria: str) -> str:
    """é†«ç™‚çŸ¥è­˜åº«æª¢ç´¢ï¼ˆä½¿ç”¨ç·šç´¢æå–ï¼‰"""
    vectorstore = await get_or_create_vectordb(DB_CONNECTION_STRING, "medical_knowledge_base")
    enhance_question = user_query if matched_criteria == "No Answer" else f"{matched_criteria}:{user_query}"
    results = search_vectordb_with_scores(vectorstore, enhance_question, k=RETRIEVAL_CONFIG['medical_rag_k'])
    # ä½¿ç”¨ç·šç´¢æå–ä»£æ›¿åŸæœ¬çš„æ‘˜è¦ï¼ˆç¾åœ¨è¿”å›ä¸‰å€‹å€¼ï¼‰
    formatted_knowledge, sources, _ = await process_retrieval_with_clue_extraction(
        sub_question=enhance_question,
        documents_with_scores=results,
        main_question=user_query,
        store=None,
        user_id="default"
    )

    if not formatted_knowledge:
        return "ï¼ˆç„¡æœ‰æ•ˆå…§å®¹ï¼‰"

    return formatted_knowledge


async def task_public_rag(user_query: str) -> str:
    """è¡›æ•™åœ’åœ°è³‡æ–™åº«æª¢ç´¢ï¼ˆä½¿ç”¨ç·šç´¢æå–ï¼‰"""
    vectorstore = await get_or_create_vectordb(
        DB_CONNECTION_STRING,
        "public_health_information_of_education_sites"
    )
    results = search_vectordb_with_scores(vectorstore, user_query, k=RETRIEVAL_CONFIG.get('public_rag_k', 5))
    # ä½¿ç”¨ç·šç´¢æå–ä»£æ›¿åŸæœ¬çš„æ‘˜è¦ï¼ˆç¾åœ¨è¿”å›ä¸‰å€‹å€¼ï¼‰
    formatted_knowledge, sources, _ = await process_retrieval_with_clue_extraction(
        sub_question=user_query,
        documents_with_scores=results,
        main_question=user_query,
        store=None,
        user_id="default"
    )

    if not formatted_knowledge:
        return "ï¼ˆç„¡æœ‰æ•ˆå…§å®¹ï¼‰"

    return formatted_knowledge


# ========================================
# æ ¸å¿ƒï¼šä¸€éµç²å–æœ€çµ‚ç­”æ¡ˆ
# ========================================

async def get_final_answer_from_query_async(
    user_query: str,
    verbose: bool = False
) -> str:
    """å¾ç”¨æˆ¶æŸ¥è©¢ç²å–æœ€çµ‚ç­”æ¡ˆï¼ˆä½¿ç”¨ medical_kb_jsonl è³‡æ–™æºï¼‰

    Args:
        user_query: ç”¨æˆ¶å•é¡Œ
        verbose: æ˜¯å¦é¡¯ç¤ºè©³ç´°è³‡è¨Š

    Returns:
        str: æœ€çµ‚ç­”æ¡ˆï¼ˆåŒ…å«æ‰€æœ‰ä¾†æºçš„ç¨ç«‹ç­”æ¡ˆï¼‰æˆ– "No Answer"
    """
    start_time = time.time()
    matched_criteria = await find_best_matching_criteria(user_query)

    async def safe_task(coro, task_name: str) -> str:
        """å®‰å…¨åŸ·è¡Œä»»å‹™ï¼Œæ•ç²ç•°å¸¸"""
        try:
            task_start = time.time()
            result = await coro
            task_elapsed = time.time() - task_start
            # if verbose:
            #     print(f"âœ… {task_name} å®Œæˆ ({task_elapsed:.2f}ç§’)")
            return result
        except Exception as e:
            print(f"âŒ {task_name} å¤±æ•—: {e}")
            return "ï¼ˆä»»å‹™åŸ·è¡Œå¤±æ•—ï¼‰"

    # ä¸¦è¡ŒåŸ·è¡Œæª¢ç´¢ä»»å‹™ï¼ˆPDF + JSONL é†«ç™‚çŸ¥è­˜åº«ï¼‰
    pdf_summary, rag_summary = await asyncio.gather(
        safe_task(
            task_pdf_rag(user_query, matched_criteria),
            "PDF æº–å‰‡"
        ),
        safe_task(
            task_medical_rag(user_query, matched_criteria),
            "é†«ç™‚çŸ¥è­˜åº«"
        )
    )
    public_rag_summary = ""

    final_answer = await combine_multi_source_answers(
        pdf_summary=pdf_summary,
        rag_summary=rag_summary,
        public_rag_summary=public_rag_summary,
        original_query=user_query
    )
    
    elapsed_time = time.time() - start_time
    print("\n" + "=" * 80)
    print(f"âœ… å®Œæˆï¼ç¸½è€—æ™‚: {elapsed_time:.2f} ç§’")
    print("=" * 80)
    
    return final_answer


# ========================================
# ä½¿ç”¨ç¯„ä¾‹
# ========================================

if __name__ == "__main__":
    # æ¸¬è©¦å•é¡Œ
    user_query = "æˆ‘æœ‰ä¸€é»é«˜è¡€å£“"
    
    async def main():
        print("\n" + "ğŸ¥ " * 40)
        print(f"ç”¨æˆ¶å•é¡Œ: {user_query}")
        print("ğŸ¥ " * 40 + "\n")
        
        answer = await get_final_answer_from_query_async(
            user_query,
            verbose=True
        )
        
        if answer != "No Answer":
            answer = convert.convert(answer)
        
        print("\n" + "=" * 80)
        print("ğŸ¯ æœ€çµ‚ç­”æ¡ˆ:")
        print("=" * 80)
        print(answer)
        print("=" * 80 + "\n")
    
    asyncio.run(main())