"""
æª¢ç´¢å·¥å…·æ¨¡çµ„
åŒ…å«çŸ¥è­˜æª¢ç´¢ã€æŸ¥è©¢æ“´å±•ã€è¤‡é›œåº¦æª¢æŸ¥ç­‰åŠŸèƒ½
ä½¿ç”¨ LangGraph çš„ç¯€é»å’Œæ¢ä»¶é‚Šä¾†çµ„ç¹”æª¢ç´¢æµç¨‹
"""
import json
import os
import re
import asyncio
# ğŸ”§ ä¿®å¾©å¾ªç’°å°å…¥ï¼šç§»é™¤é ‚å±¤å°å…¥ï¼Œæ”¹ç‚ºåœ¨å‡½æ•¸å…§éƒ¨å°å…¥ get_cache_manager
from typing import List, Tuple, Dict, Optional, TypedDict, Literal, Set
from langchain_core.messages import SystemMessage, HumanMessage
from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph, END
from retrieval.clue_extraction import process_retrieval_with_clue_extraction
from core.config import llm, remove_think_tags, DB_CONNECTION_STRING, RETRIEVAL_CONFIG
from core.prompt_config import (
    ENTITY_QUERY_EXPANSION_PROMPT,
    QUERY_COMPLEXITY_CHECK_PROMPT_V2,
    SUB_QUESTIONS_GENERATION_PROMPT,
    DISEASE_EXTRACTION_PROMPT,
    MEDICAL_PROCEDURE_CHECK_PROMPT
)
from rag_system.rag_jsonl import (
    search_vectordb_with_scores,
    get_structured_search_results,
    load_existing_vectordb
)
from rag_system.rag_files import get_structured_pdf_results
from core.config import get_reference_mapping
from core.dataclass import QueryComplexityResult
from core.datasource_config import get_registry, DataSource

# ğŸ†• Langfuse 3.0 åŒ¯å…¥ (å¯é¸ä¾è³´)
try:
    from langfuse import observe, get_client as get_langfuse_client
    LANGFUSE_AVAILABLE = True
except ImportError:
    LANGFUSE_AVAILABLE = False
    get_langfuse_client = None
    # å‰µå»ºç©ºçš„è£é£¾å™¨ä»¥é¿å…éŒ¯èª¤
    def observe(*args, **_):  # noqa: F841
        def decorator(func):
            return func
        return decorator if not args else decorator(args[0])


# ğŸš¨ ç–¾ç—…åç¨±äº’æ–¥çµ„å®šç¾©ï¼ˆç”¨æ–¼åš´æ ¼éæ¿¾ï¼‰
DISEASE_EXCLUSION_GROUPS = [
    # ç¬¬ä¸€çµ„ï¼šæµæ„Ÿç›¸é—œç–¾ç—…ï¼ˆäº’æ–¥ï¼‰
    ["æµæ„Ÿ", "æ–°å‹Aå‹æµæ„Ÿ"],
    # ç¬¬äºŒçµ„ï¼šMç—˜ç›¸é—œç–¾ç—…ï¼ˆäº’æ–¥ï¼‰
    # ["Mç—˜", "é¦¬å ¡ç—…æ¯’å‡ºè¡€ç†±"],
    # å¯ä»¥ç¹¼çºŒæ·»åŠ å…¶ä»–äº’æ–¥çµ„...
]


def filter_documents_by_disease_name(
    documents_with_scores: List[Tuple],
    user_disease: str
) -> List[Tuple]:
    """
    æ ¹æ“šç”¨æˆ¶æŸ¥è©¢çš„ç–¾ç—…åç¨±ï¼Œåš´æ ¼éæ¿¾æª¢ç´¢çµæœ

    ç›®çš„ï¼šé˜²æ­¢èªç¾©ç›¸ä¼¼ä½†å¯¦éš›ä¸åŒçš„ç–¾ç—…æ–‡ä»¶è¢«æª¢ç´¢å‡ºä¾†
    ä¾‹å¦‚ï¼šæŸ¥è©¢ã€Œæµæ„Ÿã€æ™‚ï¼Œä¸æ‡‰è¿”å›ã€Œæ–°å‹Aå‹æµæ„Ÿã€çš„æ–‡ä»¶

    Args:
        documents_with_scores: æª¢ç´¢çµæœ [(Document, score), ...]
        user_disease: ç”¨æˆ¶æŸ¥è©¢ä¸­çš„ç–¾ç—…åç¨±ï¼ˆä¾‹å¦‚ï¼š"æµæ„Ÿ"ã€"æ–°å‹Aå‹æµæ„Ÿ"ï¼‰

    Returns:
        éæ¿¾å¾Œçš„æª¢ç´¢çµæœ
    """
    if not user_disease or not documents_with_scores:
        return documents_with_scores

    # æ‰¾åˆ°ç”¨æˆ¶ç–¾ç—…æ‰€å±¬çš„äº’æ–¥çµ„
    exclusion_group = None
    for group in DISEASE_EXCLUSION_GROUPS:
        if user_disease in group:
            exclusion_group = group
            break

    # å¦‚æœç”¨æˆ¶ç–¾ç—…ä¸åœ¨ä»»ä½•äº’æ–¥çµ„ä¸­ï¼Œä¸é€²è¡Œéæ¿¾
    if not exclusion_group:
        return documents_with_scores

    # éæ¿¾é‚è¼¯
    filtered_results = []
    excluded_diseases = [d for d in exclusion_group if d != user_disease]

    for doc, score in documents_with_scores:
        # æå–æ–‡ä»¶å…§å®¹å’Œ metadata
        content = doc.page_content if hasattr(doc, 'page_content') else str(doc)
        metadata = doc.metadata if hasattr(doc, 'metadata') else {}

        # æª¢æŸ¥ä¾†æºæ–‡ä»¶åæˆ–å…§å®¹æ˜¯å¦åŒ…å«å…¶ä»–äº’æ–¥ç–¾ç—…
        source = metadata.get('source', '')

        # åˆ¤æ–·æ˜¯å¦æ‡‰è©²éæ¿¾æ‰
        should_exclude = False
        for excluded_disease in excluded_diseases:
            # æª¢æŸ¥ä¾†æºæ–‡ä»¶å
            if excluded_disease in source:
                should_exclude = True
                print(f"   ğŸš« éæ¿¾æ–‡ä»¶ï¼ˆä¾†æºåŒ…å«äº’æ–¥ç–¾ç—…ï¼‰: {source} (åŒ…å«ã€Œ{excluded_disease}ã€ï¼Œä½†ç”¨æˆ¶æŸ¥è©¢ã€Œ{user_disease}ã€)")
                break

            # æª¢æŸ¥å…§å®¹æ¨™é¡Œï¼ˆå‰200å­—å…ƒï¼Œé€šå¸¸åŒ…å«æ¨™é¡Œï¼‰
            if excluded_disease in content[:200]:
                should_exclude = True
                print(f"   ğŸš« éæ¿¾æ–‡ä»¶ï¼ˆå…§å®¹åŒ…å«äº’æ–¥ç–¾ç—…ï¼‰: {source[:50]}... (åŒ…å«ã€Œ{excluded_disease}ã€ï¼Œä½†ç”¨æˆ¶æŸ¥è©¢ã€Œ{user_disease}ã€)")
                break

        if not should_exclude:
            filtered_results.append((doc, score))

    if len(filtered_results) < len(documents_with_scores):
        filtered_count = len(documents_with_scores) - len(filtered_results)
        print(f"   âœ… ç–¾ç—…åç¨±åš´æ ¼éæ¿¾ï¼šå·²éæ¿¾æ‰ {filtered_count} å€‹ä¸åŒ¹é…çš„æ–‡ä»¶ï¼ˆäº’æ–¥ç–¾ç—…ï¼‰")

    return filtered_results


# ğŸ”§ å®šç¾©æ™ºèƒ½åˆä½µå‡½æ•¸ï¼ˆå¾ graph_nodes.py ç§»æ¤ï¼‰
def merge_document_content_smart(existing_content: str, new_content: str) -> str:
    """
    æ™ºèƒ½åˆä½µæ–‡æª”å…§å®¹ï¼Œé¿å…é‡è¤‡çš„QAå°

    Args:
        existing_content: å·²å­˜åœ¨çš„æ–‡æª”å…§å®¹
        new_content: æ–°çš„æ–‡æª”å…§å®¹

    Returns:
        åˆä½µå¾Œçš„å…§å®¹ï¼ˆå»é™¤é‡è¤‡QAå°ï¼‰
    """
    import re

    # å¦‚æœå®Œå…¨ç›¸åŒï¼Œç›´æ¥è¿”å›
    if new_content in existing_content:
        return existing_content

    # ğŸ”§ ä¿®å¾©ï¼šæå–å•é¡Œ+ç­”æ¡ˆçµ„åˆä½œç‚ºå»é‡æ¨™è­˜ï¼Œé¿å…èª¤åˆªä¸åŒç­”æ¡ˆ
    existing_qa_pairs = set()
    # åŒ¹é…å®Œæ•´çš„ "å•é¡Œ:...ç­”æ¡ˆ:..." æ ¼å¼ï¼ˆä¸åŒ…å«é—œéµå­—å’Œåƒè€ƒï¼‰
    for match in re.finditer(r'å•é¡Œ:(.*?)ç­”æ¡ˆ:(.*?)(?=é—œéµå­—:|åƒè€ƒ:|å•é¡Œ:|$)', existing_content, re.DOTALL):
        question_text = match.group(1).strip()
        answer_text = match.group(2).strip()
        existing_qa_pairs.add((question_text, answer_text))

    # å¦‚æœæ²’æœ‰QAå°æ ¼å¼ï¼Œç›´æ¥è¿½åŠ 
    if not existing_qa_pairs:
        return existing_content + "\n\n" + new_content

    # éæ¿¾æ–°å…§å®¹ä¸­çš„é‡è¤‡QAå°
    new_paragraphs = []
    for para in new_content.split('\n'):
        para = para.strip()
        if not para:
            continue

        # æª¢æŸ¥æ˜¯å¦ç‚ºQAå°ï¼ˆä¸åŒ…å«é—œéµå­—å’Œåƒè€ƒï¼‰
        qa_match = re.search(r'å•é¡Œ:(.*?)ç­”æ¡ˆ:(.*?)(?=é—œéµå­—:|åƒè€ƒ:|å•é¡Œ:|$)', para, re.DOTALL)
        if qa_match:
            question_text = qa_match.group(1).strip()
            answer_text = qa_match.group(2).strip()
            qa_key = (question_text, answer_text)
            # ğŸ”§ å¦‚æœå•é¡Œ+ç­”æ¡ˆçµ„åˆå·²å­˜åœ¨ï¼Œè·³é
            if qa_key in existing_qa_pairs:
                continue
            else:
                # æ–°çš„QAå°ï¼Œæ·»åŠ åˆ°é›†åˆä¸¦ä¿ç•™
                existing_qa_pairs.add(qa_key)
                new_paragraphs.append(para)
        else:
            # éQAå°æ ¼å¼ï¼Œç›´æ¥ä¿ç•™
            new_paragraphs.append(para)

    # å¦‚æœæœ‰æ–°çš„éé‡è¤‡å…§å®¹ï¼Œè¿½åŠ 
    if new_paragraphs:
        return existing_content + "\n\n" + "\n".join(new_paragraphs)
    else:
        return existing_content


# ==================== æª¢ç´¢ç‹€æ…‹å®šç¾© ====================

class RetrievalState(TypedDict):
    """æª¢ç´¢æµç¨‹çš„ç‹€æ…‹

    æ³¨æ„: store å°è±¡ä¸èƒ½æ”¾åœ¨ç‹€æ…‹ä¸­,å› ç‚ºç„¡æ³•åºåˆ—åŒ–
    éœ€è¦é€šé config['configurable']['store'] å‚³é
    """
    # è¼¸å…¥åƒæ•¸
    query: str
    main_question: Optional[str]
    use_clue_extraction: Optional[bool]
    user_id: str

    # è³‡æ–™æºé¸æ“‡
    datasource_ids: Optional[List[str]]  # è¦ä½¿ç”¨çš„è³‡æ–™æº ID åˆ—è¡¨ï¼ŒNone=è‡ªå‹•é¸æ“‡

    # ä¸­é–“ç‹€æ…‹
    actual_query: str
    matched_disease: str
    is_medical: bool
    is_procedure: bool
    retrieval_strategy: Literal["multi_source", "procedure", "none"]  # æ”¹ç‚º multi_source

    # æª¢ç´¢çµæœ
    knowledge: str
    original_sources: List[str]
    original_docs_dict: Dict[str, str]
    matched_table_images: Optional[List[Dict]]  # ğŸ†• è¡¨æ ¼åœ–ç‰‡åŒ¹é…çµæœ

    # ä¸­é–“çµæœ(ç”¨æ–¼çµ„åˆ)
    retrieval_summaries: Dict[str, str]  # {datasource_id: summary}
    json_results: Dict
    pdf_results: Dict


# æ–‡ä»¶ç·©å­˜
def _load_dataset_files() -> set[str]:
    """é è¼‰ DataSet æ–‡ä»¶åˆ—è¡¨"""
    dataset_path = os.getenv("DATASET_PATH", "/home/danny/AI-agent/DataSet") + "/"
    actual_files = set()
    try:
        for _, _, files in os.walk(dataset_path):
            for file in files:
                if file.endswith(('.pdf', '.xlsx', '.xls')):
                    actual_files.add(file)
        print(f"âœ… å·²é è¼‰ {len(actual_files)} å€‹æ–‡ä»¶åˆ°è¨˜æ†¶é«”")
    except Exception as e:
        print(f"âš ï¸ ç„¡æ³•é è¼‰ DataSet æ–‡ä»¶åˆ—è¡¨: {e}")
    return actual_files

DATASET_FILES_CACHE = _load_dataset_files()


# ==================== è¼”åŠ©å‡½æ•¸ ====================

def convert_references_to_english(chinese_ref):
    """å°‡ä¸­æ–‡åƒè€ƒæ–‡ç»åç¨±è½‰æ›ç‚ºè‹±æ–‡ collection name

    æ”¯æŒæ¨¡ç³ŠåŒ¹é…ï¼Œè™•ç†ä»¥ä¸‹æƒ…æ³:
    - å¿½ç•¥ç·¨è™Ÿå‰ç¶´ï¼ˆå¦‚ "1_", "2_" ç­‰ï¼‰
    - å¿½ç•¥æ–‡ä»¶æ“´å±•åï¼ˆå¦‚ ".pdf"ï¼‰
    - è™•ç†å‰å¾Œç©ºæ ¼
    """
    reference_mapping = get_reference_mapping()

    # æ¸…ç†è¼¸å…¥ï¼šç§»é™¤ .pdf æ“´å±•åå’Œå‰å¾Œç©ºæ ¼
    cleaned_ref = chinese_ref.strip()
    if cleaned_ref.endswith('.pdf'):
        cleaned_ref = cleaned_ref[:-4]

    # 1. ç›´æ¥åŒ¹é…å®Œæ•´åç¨±
    if cleaned_ref in reference_mapping:
        return reference_mapping[cleaned_ref]

    # 2. ç§»é™¤è¼¸å…¥çš„ç·¨è™Ÿå‰ç¶´ï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
    cleaned_ref_without_prefix = cleaned_ref.split('_', 1)[-1] if '_' in cleaned_ref else cleaned_ref

    # 3. æ¨¡ç³ŠåŒ¹é…ï¼šç§»é™¤ mapping ä¸­éµçš„ç·¨è™Ÿå‰ç¶´å¾Œæ¯”å°
    for key, value in reference_mapping.items():
        # ç§»é™¤ mapping éµçš„ "æ•¸å­—_" å‰ç¶´
        key_without_prefix = key.split('_', 1)[-1] if '_' in key else key

        # æ¯”å°æ¸…ç†å¾Œçš„åƒè€ƒåç¨±
        if cleaned_ref_without_prefix == key_without_prefix:
            return value

    # 4. éƒ¨åˆ†åŒ¹é…ï¼šæª¢æŸ¥æ˜¯å¦åŒ…å«é—œéµè©ï¼ˆå®¹éŒ¯æ©Ÿåˆ¶ï¼‰
    for key, value in reference_mapping.items():
        key_without_prefix = key.split('_', 1)[-1] if '_' in key else key
        if cleaned_ref_without_prefix in key_without_prefix or key_without_prefix in cleaned_ref_without_prefix:
            print(f"âš ï¸ ä½¿ç”¨éƒ¨åˆ†åŒ¹é…: '{chinese_ref}' â†’ '{key}' â†’ '{value}'")
            return value

    # 5. ğŸ†• é™„ä»¶æ™ºèƒ½åŒ¹é…ï¼šå¦‚æœæ˜¯é™„ä»¶æ–‡æª”ï¼Œæå–ç–¾ç—…åç¨±ä¸¦åŒ¹é…ä¸»æ–‡æª”çš„ collection
    if cleaned_ref.startswith('é™„ä»¶'):
        # æå–é™„ä»¶ä¸­çš„ç–¾ç—…é—œéµå­—
        import re
        # ç­–ç•¥1ï¼šæå–ã€Œå› æ‡‰XXXã€ä¸­çš„ç–¾ç—…åç¨±ï¼ˆè™•ç†ã€Œå› æ‡‰ç™¼ç†±ä¼´è¡€å°æ¿æ¸›å°‘ç¶œåˆç—‡ï¼Œ...ã€ï¼‰
        disease_match = re.search(r'å› æ‡‰([^ï¼Œ,ã€]+)', cleaned_ref)
        if disease_match:
            disease_keyword = disease_match.group(1).strip()
            # åœ¨ mapping ä¸­å°‹æ‰¾åŒ…å«æ­¤ç–¾ç—…åç¨±çš„ä¸»æ–‡æª”
            for key, value in reference_mapping.items():
                if disease_keyword in key:
                    print(f"âœ… é™„ä»¶æ™ºèƒ½åŒ¹é…: '{chinese_ref}' â†’ ç–¾ç—…é—œéµå­— '{disease_keyword}' â†’ '{key}' â†’ '{value}'")
                    return value

        # ç­–ç•¥2ï¼šæå–ã€Œé™„ä»¶X_ã€ä¹‹å¾Œçš„å…§å®¹ï¼Œç›´åˆ°é‡åˆ°ã€Œï¼Œã€æˆ–ã€Œå€‹äººé˜²è­·ã€
        disease_match = re.search(r'é™„ä»¶[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[_ã€]([^ï¼Œ,]+?)(?:ï¼Œ|å€‹äººé˜²è­·|é†«ç™‚ç…§è­·)', cleaned_ref)
        if disease_match:
            disease_keyword = disease_match.group(1).strip()
            # åœ¨ mapping ä¸­å°‹æ‰¾åŒ…å«æ­¤ç–¾ç—…åç¨±çš„ä¸»æ–‡æª”
            for key, value in reference_mapping.items():
                if disease_keyword in key:
                    print(f"âœ… é™„ä»¶æ™ºèƒ½åŒ¹é…: '{chinese_ref}' â†’ ç–¾ç—…é—œéµå­— '{disease_keyword}' â†’ '{key}' â†’ '{value}'")
                    return value

    # æ‰¾ä¸åˆ°å‰‡è¿”å›é è¨­å€¼
    print(f"âš ï¸ ç„¡æ³•åŒ¹é…åƒè€ƒæ–‡ç»: '{chinese_ref}' â†’ ä½¿ç”¨é è¨­å€¼ 'medical_knowledge_base'")
    return "medical_knowledge_base"


def extract_sources_from_knowledge(knowledge: str) -> list[str]:
    """å¾çŸ¥è­˜æ–‡æœ¬ä¸­æå–ã€Šæ–‡ä»¶åã€‹æ ¼å¼çš„ä¾†æº"""
    sources = set()
    pattern = r'ã€Š([^ã€‹]+)ã€‹'
    matches = re.findall(pattern, knowledge)
    for match in matches:
        sources.add(match.strip())
    return list(sources)


def extract_disease_name(user_message: str) -> str:
    """æå–ç–¾ç—…åç¨±"""
    messages = [
        SystemMessage(content=DISEASE_EXTRACTION_PROMPT),
        HumanMessage(content=user_message)
    ]
    try:
        disease = llm.invoke(messages).content.strip()
        disease = remove_think_tags(disease)
        return disease
    except Exception:
        return "No_Answer"


# ==================== LangGraph ç¯€é»å‡½æ•¸ ====================

async def initialize_retrieval_node(state: RetrievalState) -> RetrievalState:
    """åˆå§‹åŒ–ç¯€é»ï¼šè™•ç†é…ç½®å’Œæ¸…ç†æŸ¥è©¢"""
    from core.config import RETRIEVAL_CONFIG

    # è¨­ç½®é è¨­å€¼
    if state.get("main_question") is None:
        state["main_question"] = state["query"]

    if state.get("use_clue_extraction") is None:
        state["use_clue_extraction"] = RETRIEVAL_CONFIG.get('enable_clue_extraction', False)

    if state.get("user_id") is None:
        state["user_id"] = "default"

    # æ¸…ç†æŸ¥è©¢æ–‡æœ¬
    query = state["query"]
    actual_query = query.split("\n[ç³»çµ±è¨»è¨˜ï¼š")[0] if "[ç³»çµ±è¨»è¨˜ï¼š" in query else query
    state["actual_query"] = actual_query

    # åˆå§‹åŒ–çµæœå­—æ®µ
    state["knowledge"] = ""
    state["original_sources"] = []
    state["original_docs_dict"] = {}
    state["rag_summary"] = ""
    state["pdf_summary"] = ""
    state["json_results"] = {}
    state["pdf_results"] = {}
    state["matched_disease"] = ""
    state["is_medical"] = True
    state["is_procedure"] = False

    #print(f"ğŸ”§ [INIT] æŸ¥è©¢: {actual_query}")
    #print(f"ğŸ”§ [INIT] é…ç½®: clue={state['use_clue_extraction']}, datasource_ids={state.get('datasource_ids')}")

    return state


def route_retrieval_strategy(state: RetrievalState) -> Literal["multi_source", "check_procedure"]:
    """
    è·¯ç”±ç¯€é»ï¼šæ±ºå®šä½¿ç”¨å“ªç¨®æª¢ç´¢ç­–ç•¥

    æ‰€æœ‰å•é¡Œéƒ½ä½¿ç”¨å¤šè³‡æ–™æºæ¶æ§‹é€²è¡Œæª¢ç´¢
    """
    datasource_ids = state.get("datasource_ids")

    # å¦‚æœä½¿ç”¨è€…æŒ‡å®šäº†è³‡æ–™æºï¼Œä½¿ç”¨æŒ‡å®šçš„è³‡æ–™æº
    if datasource_ids:
        print(f"ğŸ”€ [ROUTE] ä½¿ç”¨è€…æŒ‡å®šè³‡æ–™æº â†’ å¤šè³‡æ–™æºæª¢ç´¢")
        print(f"   æŒ‡å®šè³‡æ–™æº: {', '.join(datasource_ids)}")
    else:
        print("ğŸ”€ [ROUTE] ä½¿ç”¨å¤šè³‡æ–™æºæª¢ç´¢ï¼ˆè‡ªå‹•é¸æ“‡è³‡æ–™æºï¼‰")

    return "multi_source"

def extract_qa_pairs_from_text(text: str) -> Set[Tuple[str, str]]:
    """
    å¾æ–‡æœ¬ä¸­æå–æ‰€æœ‰ QA å°
    è¿”å›: {(å•é¡Œ, ç­”æ¡ˆ), ...}
    """
    qa_set = set()
    
    # åŒ¹é… "å•é¡Œ: xxx ç­”æ¡ˆ: yyy" æ ¼å¼
    pattern = r'å•é¡Œ:\s*(.*?)\s*ç­”æ¡ˆ:\s*(.*?)(?=\nå•é¡Œ:|\né—œéµå­—:|\nåƒè€ƒ:|\nã€|$)'
    
    for match in re.finditer(pattern, text, re.DOTALL):
        question = match.group(1).strip()
        answer = match.group(2).strip()
        
        # æ¸…ç†ç©ºæ ¼
        question = re.sub(r'\s+', ' ', question)
        answer = re.sub(r'\s+', ' ', answer)
        
        if question and answer and len(question) > 3 and len(answer) > 5:
            qa_set.add((question, answer))
    
    return qa_set

def clean_incomplete_sentences(text: str) -> str:
    """
    ç§»é™¤æ–‡æœ¬æœ«å°¾ä¸å®Œæ•´çš„å¥å­

    ç­–ç•¥ï¼šå¾æ–‡æœ¬æœ«å°¾é–‹å§‹ï¼Œç§»é™¤çœ‹èµ·ä¾†ä¸å®Œæ•´çš„å¥å­ç‰‡æ®µ
    ï¼ˆä¾‹å¦‚å–®å€‹å­—æ¯å¾Œè·Ÿå¥è™Ÿï¼Œå¦‚ "G." æˆ– " K."ï¼‰

    Args:
        text: åŸå§‹æ–‡æœ¬

    Returns:
        æˆªå–åˆ°æœ€å¾Œä¸€å€‹å®Œæ•´å¥å­çš„æ–‡æœ¬
    """
    if not text or len(text.strip()) < 10:
        return text

    text = text.strip()

    # å®šç¾©å¥å­çµæŸæ¨™é»
    sentence_endings = ['ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?', 'ã€', 'ã€', 'ã€‹', 'ï¼‰', ')']

    # æŒçºŒå¾æœ«å°¾ç§»é™¤ä¸å®Œæ•´çš„å¥å­ï¼Œç›´åˆ°æ‰¾åˆ°å®Œæ•´çš„ç‚ºæ­¢
    while True:
        # æ‰¾åˆ°æœ€å¾Œä¸€å€‹å¥å­çµæŸæ¨™é»
        last_pos = -1
        last_char = None

        for ending in sentence_endings:
            pos = text.rfind(ending)
            if pos > last_pos:
                last_pos = pos
                last_char = ending

        # å¦‚æœæ²’æœ‰æ‰¾åˆ°å¥å­çµæŸæ¨™é»ï¼Œè¿”å›åŸæ–‡
        if last_pos == -1:
            return text

        # è·³éæ•¸å­—ä¸­çš„å°æ•¸é»ï¼ˆå¦‚38.5ï¼‰
        if last_char == '.' and last_pos < len(text) - 1 and text[last_pos + 1].isdigit():
            # å¾é€™å€‹å°æ•¸é»ä¹‹å‰ç¹¼çºŒæ‰¾
            text = text[:last_pos]
            if len(text.strip()) < 10:
                return text  # å¤ªçŸ­äº†ï¼Œè¿”å›åŸæ–‡
            continue

        # æª¢æŸ¥é€™å€‹å¥è™Ÿå¾Œé¢æ˜¯å¦é‚„æœ‰å…§å®¹
        after_punctuation = text[last_pos + 1:].strip()

        if after_punctuation:
            # æå–å¾Œé¢çš„ç´”æ–‡æœ¬ï¼ˆå»é™¤æ¨™é»å’Œç©ºæ ¼ï¼‰
            pure_after = ''.join(c for c in after_punctuation if c.isalnum() or '\u4e00' <= c <= '\u9fff')

            # å¦‚æœå¾Œé¢çš„å…§å®¹å¾ˆçŸ­ï¼ˆ<=3å€‹å­—ç¬¦ï¼‰ï¼Œèªç‚ºæ˜¯ä¸å®Œæ•´çš„å¥å­
            if len(pure_after) <= 3:
                # ç§»é™¤é€™å€‹ä¸å®Œæ•´çš„éƒ¨åˆ†ï¼Œç¹¼çºŒæª¢æŸ¥
                text = text[:last_pos + 1]
                continue

        # åˆ°é€™è£¡ï¼Œèªªæ˜å¥è™Ÿå¾Œé¢æ²’æœ‰å…§å®¹ï¼Œæˆ–è€…å¾Œé¢çš„å…§å®¹è¶³å¤ é•·
        # æª¢æŸ¥é€™å€‹å¥å­æœ¬èº«çš„å…§å®¹æ˜¯å¦è¶³å¤ é•·
        # æ‰¾åˆ°ä¸Šä¸€å€‹å¥è™Ÿ
        prev_pos = -1
        for ending in sentence_endings:
            pos = text[:last_pos].rfind(ending)
            if pos > prev_pos:
                prev_pos = pos

        # æå–ç•¶å‰å¥å­å…§å®¹ï¼ˆå…©å€‹å¥è™Ÿä¹‹é–“ï¼‰
        if prev_pos >= 0:
            sentence_content = text[prev_pos + 1:last_pos].strip()
        else:
            sentence_content = text[:last_pos].strip()

        # æå–ç´”æ–‡æœ¬
        pure_content = ''.join(c for c in sentence_content if c.isalnum() or '\u4e00' <= c <= '\u9fff')

        # å¦‚æœå¥å­å…§å®¹å¤ªçŸ­ï¼ˆ<=3å€‹å­—ç¬¦ï¼‰ï¼Œèªç‚ºæ˜¯ä¸å®Œæ•´çš„
        if len(pure_content) <= 3:
            # ç§»é™¤åˆ°ä¸Šä¸€å€‹å¥è™Ÿï¼Œç¹¼çºŒæª¢æŸ¥
            if prev_pos >= 0:
                text = text[:prev_pos + 1]
                if len(text.strip()) < 10:
                    return text  # å¤ªçŸ­äº†ï¼Œè¿”å›åŸæ–‡
                continue
            else:
                # æ²’æœ‰ä¸Šä¸€å€‹å¥è™Ÿäº†ï¼Œè¿”å›åŸæ–‡
                return text

        # æ‰¾åˆ°äº†å®Œæ•´çš„å¥å­ï¼Œè¿”å›çµæœ
        return text


def extract_pdf_paragraphs_from_text(text: str) -> Set[str]:
    """
    å¾æ–‡æœ¬ä¸­æå– PDF æ®µè½ï¼ˆæ’é™¤ QA å°ï¼‰
    è¿”å›: {æ®µè½1, æ®µè½2, ...}
    """
    # æ­¥é©Ÿ1: å…ˆç§»é™¤æ‰€æœ‰ QA æ ¼å¼çš„å…§å®¹ï¼ˆæ›´åš´æ ¼çš„åŒ¹é…ï¼‰
    # åŒ¹é…å®Œæ•´çš„ "å•é¡Œ: ... ç­”æ¡ˆ: ..." æ ¼å¼
    text_clean = re.sub(
        r'å•é¡Œ:\s*.*?\s*ç­”æ¡ˆ:\s*.*?(?=\n\s*å•é¡Œ:|\n\s*é—œéµå­—:|\n\s*åƒè€ƒ:|\n\s*ã€|\n\s*$|$)',
        '',
        text,
        flags=re.DOTALL
    )

    # æ­¥é©Ÿ2: é¡å¤–ç§»é™¤å–®ç¨å‡ºç¾çš„ "å•é¡Œ:" æˆ– "ç­”æ¡ˆ:" è¡Œ
    text_clean = re.sub(r'\n\s*å•é¡Œ:.*', '', text_clean)
    text_clean = re.sub(r'\n\s*ç­”æ¡ˆ:.*', '', text_clean)

    # æ­¥é©Ÿ3: ç§»é™¤æ¨™è¨˜å’Œåˆ†éš”ç·š
    text_clean = re.sub(r'ã€åŸå§‹PDFæ®µè½ã€‘', '', text_clean)
    text_clean = re.sub(r'\[ä¾†æºæ–‡ä»¶:.*?\]', '', text_clean)
    text_clean = re.sub(r'[â”€=]{10,}', '', text_clean)

    # æ­¥é©Ÿ4: æŒ‰æ®µè½åˆ†å‰²
    paragraphs = re.split(r'\n\s*\n+', text_clean)

    pdf_set = set()
    for para in paragraphs:
        para = para.strip()

        # éæ¿¾å¤ªçŸ­æˆ–ç©ºçš„æ®µè½
        if len(para) < 20:
            continue

        # ğŸ”§ é¡å¤–æª¢æŸ¥ï¼šå¦‚æœæ®µè½ä¸­é‚„åŒ…å« "å•é¡Œ:" æˆ– "ç­”æ¡ˆ:"ï¼Œè·³é
        if 'å•é¡Œ:' in para or 'ç­”æ¡ˆ:' in para:
            continue

        # æ¸…ç†ç©ºæ ¼
        para = re.sub(r'\s+', ' ', para)
        para = re.sub(r'([\u4e00-\u9fff])\s+([\u4e00-\u9fff])', r'\1\2', para)

        # ğŸ†• ç§»é™¤ä¸å®Œæ•´çš„å¥å­
        para = clean_incomplete_sentences(para)

        pdf_set.add(para)

    return pdf_set


# ==================== æ®µè½è™•ç†è¼”åŠ©å‡½æ•¸ ====================

def get_paragraph_signature(text: str) -> str:
    """ç²å–æ®µè½ç°½åï¼Œç”¨æ–¼å¿«é€Ÿå»é‡ï¼ˆå–å‰ 50 å€‹å­—å…ƒï¼‰"""
    clean = re.sub(r'\s+', '', text)
    return clean[:50] if len(clean) >= 50 else clean


def is_reference_citation(text: str) -> bool:
    """åˆ¤æ–·æ˜¯å¦ç‚ºå¼•ç”¨æ–‡ç»æ ¼å¼"""
    patterns = [
        r'^\d+\.\s*[A-Z][a-z]+\s+[A-Z]{1,2}',  # 38. Fisher DA...
        r'^[A-Z][a-z]+\s+[A-Z]{1,2},',  # Fisher DA,
        r'^\d+\.\s*[\u4e00-\u9fff]',  # 38. è¡Œæ”¿é™¢...
    ]
    return any(re.match(p, text.strip()) for p in patterns)


def is_figure_description(text: str) -> bool:
    """åˆ¤æ–·æ˜¯å¦ç‚ºåœ–ç‰‡æè¿°"""
    return bool(re.match(r'^(åœ–|åœ–ç‰‡|Figure|Fig\.?)\s*\d+', text.strip()))


def extract_qa_and_pdf(content: str) -> Tuple[List[Tuple[str, str]], List[str]]:
    """æå– QA å°å’Œ PDF æ®µè½"""
    qa_pairs = []
    pdf_paragraphs = []

    if 'ã€åŸå§‹PDFæ®µè½ã€‘' in content:
        parts = content.split('ã€åŸå§‹PDFæ®µè½ã€‘', 1)
        qa_part, pdf_part = parts[0].strip(), parts[1].strip() if len(parts) > 1 else ""
    else:
        qa_part, pdf_part = content, ""

    # æå– QA å°
    qa_pattern = r'å•é¡Œ:\s*(.*?)\s*ç­”æ¡ˆ:\s*(.*?)(?=\nå•é¡Œ:|\né—œéµå­—:|\nåƒè€ƒ:|\nã€|$)'
    for match in re.finditer(qa_pattern, qa_part, re.DOTALL):
        q, a = match.group(1).strip(), match.group(2).strip()
        if q and a:
            qa_pairs.append((q, a))

    # å¦‚æœæ²’æœ‰ QA å°ï¼Œè™•ç†ç‚º PDF æ®µè½
    if not qa_pairs:
        if 'ã€ç–¾ç—…åç¨±ã€‘' in qa_part or 'åƒè€ƒè³‡æ–™ä¾†æº' in qa_part:
            if qa_part.strip():
                pdf_paragraphs.append(qa_part.strip())
        else:
            for p in re.split(r'\n\s*\n', qa_part):
                p = p.strip()
                if p and len(p) > 10:
                    pdf_paragraphs.append(p)

    # è™•ç† PDF æ®µè½
    if pdf_part:
        pdf_part = re.sub(r'\[ä¾†æºæ–‡ä»¶:.*?\]', '', pdf_part)
        pdf_part = re.sub(r'â”€+', '', pdf_part)
        for p in re.split(r'\n\s*\n', pdf_part):
            p = p.strip()
            if p and len(p) > 10:
                pdf_paragraphs.append(p)

    return qa_pairs, pdf_paragraphs


def merge_documents_intelligently(_doc_name: str, existing_content: str, new_content: str) -> str:
    """
    æ™ºèƒ½åˆä½µåŒä¸€æ–‡æª”çš„ QA å’Œ PDF å…§å®¹ï¼ŒåŠ å¼·å»é‡

    Args:
        _doc_name: æ–‡æª”åç¨±ï¼ˆä¿ç•™ä¾›æœªä¾†æ“´å±•ï¼Œç›®å‰æœªä½¿ç”¨ï¼‰
        existing_content: å·²å­˜åœ¨çš„å…§å®¹
        new_content: æ–°å…§å®¹

    ç­–ç•¥ï¼š
    1. åˆ†åˆ¥æå– QA å°å’Œ PDF æ®µè½
    2. QA ä½¿ç”¨ dict å»é‡
    3. PDF ä½¿ç”¨ç°½åå»é‡ï¼Œä¸¦è·³éå¼•ç”¨æ–‡ç»å’Œåœ–ç‰‡æè¿°
    4. é‡æ–°çµ„è£ï¼šQA åœ¨å‰ï¼ŒPDF åœ¨å¾Œ
    """
    existing_qa, existing_pdf = extract_qa_and_pdf(existing_content)
    new_qa, new_pdf = extract_qa_and_pdf(new_content)

    # åˆä½µä¸¦å»é‡ QA
    all_qa = {(q, a): True for q, a in existing_qa + new_qa}

    # æ™ºèƒ½å»é‡ PDF æ®µè½
    seen_signatures = set()
    unique_pdf = []

    for p in existing_pdf + new_pdf:
        p = p.strip()
        if len(p) < 10:
            continue

        # è·³éå¼•ç”¨æ–‡ç»å’Œåœ–ç‰‡æè¿°ï¼ˆé€™äº›å®¹æ˜“é‡è¤‡ä¸”è³‡è¨Šåƒ¹å€¼ä½ï¼‰
        if is_reference_citation(p) or is_figure_description(p):
            continue

        # ä½¿ç”¨ç°½åå»é‡
        sig = get_paragraph_signature(p)
        if sig and sig not in seen_signatures:
            seen_signatures.add(sig)
            unique_pdf.append(p)

    # é‡å»ºå…§å®¹
    result = []

    if all_qa:
        for q, a in all_qa.keys():
            result.extend([f"å•é¡Œ: {q}", f"ç­”æ¡ˆ: {a}", ""])

    if unique_pdf:
        if all_qa:
            result.extend(["â”€" * 80, "ã€åŸå§‹PDFæ®µè½ã€‘", ""])
        for p in unique_pdf:
            result.extend([p, ""])

    return "\n".join(result).strip()


async def retrieve_multi_source_node(state: RetrievalState, config: Optional[RunnableConfig] = None) -> RetrievalState:
    """
    ğŸ†• å¤šè³‡æ–™æºæª¢ç´¢ç¯€é»
    æ ¹æ“šä½¿ç”¨è€…æŒ‡å®šæˆ–è‡ªå‹•é¸æ“‡çš„è³‡æ–™æºé€²è¡Œæª¢ç´¢
    
    ä¿®æ”¹é»ï¼š
    - ä¿ç•™ metadata ä¸­çš„ has_table å’Œ table_images
    - è¡¨æ ¼åŒ¹é…å„ªå…ˆå¾ metadata å–å¾—ï¼Œfallback åˆ°ç›¸ä¼¼åº¦åŒ¹é…
    """
    query = state["query"]
    main_question = state["main_question"]
    use_clue_extraction = state["use_clue_extraction"]
    user_id = state["user_id"]
    datasource_ids = state.get("datasource_ids")

    # å¾ config ä¸­ç²å– store
    store = None
    if config and "configurable" in config:
        store = config["configurable"].get("store")

    # ğŸ”¹ æ­¥é©Ÿ 1: é¸æ“‡è³‡æ–™æº
    registry = get_registry()
    selected_sources: List[DataSource] = []

    if datasource_ids:
        for ds_id in datasource_ids:
            ds = registry.get(ds_id)
            if ds and ds.enabled:
                selected_sources.append(ds)
            elif ds and not ds.enabled:
                print(f"   âš ï¸ è³‡æ–™æº {ds_id} å·²åœç”¨ï¼Œè·³é")
            else:
                print(f"   âš ï¸ æ‰¾ä¸åˆ°è³‡æ–™æº {ds_id}")
    else:
        selected_sources = registry.get_by_scenario(is_medical=True)

    if not selected_sources:
        print(f"   âŒ æ²’æœ‰å¯ç”¨çš„è³‡æ–™æº")
        return state

    loop = asyncio.get_running_loop()
    retrieval_results = {}  # {datasource_id: (summary, docs_dict, results)}
    raw_results = {}  # ğŸ†• ä¿å­˜åŸå§‹æª¢ç´¢çµæœï¼Œç”¨æ–¼æå– reference

    # ğŸ”¹ æ­¥é©Ÿ 2: å°æ¯å€‹è³‡æ–™æºé€²è¡Œæª¢ç´¢
    for datasource in selected_sources:
        try:
            # è¼‰å…¥å‘é‡è³‡æ–™åº«
            vectorstore = await loop.run_in_executor(
                None, load_existing_vectordb, DB_CONNECTION_STRING, datasource.collection_name
            )
            results_with_scores = search_vectordb_with_scores(
                vectorstore, query, k=datasource.default_k
            )

            # ğŸš¨ ç–¾ç—…åç¨±åš´æ ¼éæ¿¾ï¼ˆé˜²æ­¢èªç¾©ç›¸ä¼¼ä½†ä¸åŒçš„ç–¾ç—…æ–‡ä»¶è¢«æª¢ç´¢ï¼‰
            disease_name = extract_disease_name(main_question)
            if disease_name and disease_name not in ["No_Answer", "æœªçŸ¥ç–¾ç—…", "ç„¡", "none", "null", ""]:
                print(f"   ğŸ” æª¢æ¸¬åˆ°ç–¾ç—…åç¨±ï¼š{disease_name}ï¼Œå•Ÿç”¨åš´æ ¼éæ¿¾")
                results_with_scores = filter_documents_by_disease_name(results_with_scores, disease_name)

            # ğŸ†• ä¿å­˜åŸå§‹æª¢ç´¢çµæœï¼ˆç”¨æ–¼å¾ŒçºŒ reference æå–ï¼‰
            raw_results[datasource.id] = results_with_scores

            # è™•ç†çµæœ
            summary = ""
            docs_dict = {}
            results = None

            if use_clue_extraction and store:
                try:
                    summary, _, docs_dict = await process_retrieval_with_clue_extraction(
                        sub_question=query,
                        documents_with_scores=results_with_scores,
                        main_question=main_question,
                        store=store,
                        user_id=user_id
                    )
                    # æ ¼å¼è½‰æ›ï¼š{doc: {'content': str, 'score': float}} â†’ ä¿ç•™å®Œæ•´è³‡è¨Š
                    new_docs_dict = {}
                    for k, v in docs_dict.items():
                        if isinstance(v, dict):
                            content = v.get('content', '')
                            # ğŸ†• åªå°PDFå…§å®¹æ¸…ç†ï¼ˆä¸åŒ…å«QAå°æ ¼å¼çš„å…§å®¹ï¼‰
                            if not ('å•é¡Œ:' in content and 'ç­”æ¡ˆ:' in content):
                                content = clean_incomplete_sentences(content)
                            new_docs_dict[k] = {
                                'content': content,
                                'score': v.get('score', 999.0),
                                'has_table': v.get('has_table', False),
                                'table_images': v.get('table_images', [])
                            }
                        else:
                            content = str(v)
                            # ğŸ†• åªå°PDFå…§å®¹æ¸…ç†ï¼ˆä¸åŒ…å«QAå°æ ¼å¼çš„å…§å®¹ï¼‰
                            if not ('å•é¡Œ:' in content and 'ç­”æ¡ˆ:' in content):
                                content = clean_incomplete_sentences(content)
                            new_docs_dict[k] = {'content': content, 'score': 999.0, 'has_table': False, 'table_images': []}
                    docs_dict = new_docs_dict
                    
                except Exception as e:
                    print(f"   âš ï¸ ç·šç´¢æå–å¤±æ•—: {e}ï¼Œä½¿ç”¨åŸå§‹æ ¼å¼")
                    if datasource.source_type == "jsonl":
                        results, summary = get_structured_search_results(
                            results_with_scores, query, top_n=datasource.default_k,
                            show_scores=False, format_type="markdown"
                        )
                    else:
                        results, summary = get_structured_pdf_results(
                            results_with_scores, query,
                            show_scores=False, format_type="markdown"
                        )
            else:
                if datasource.source_type == "jsonl":
                    results, summary = get_structured_search_results(
                        results_with_scores, query, top_n=datasource.default_k,
                        show_scores=False, format_type="markdown"
                    )
                else:
                    results, summary = get_structured_pdf_results(
                        results_with_scores, query,
                        show_scores=False, format_type="markdown"
                    )
                
                # ğŸ†• ä¿®æ”¹ï¼šè™•ç†çµæœæ™‚ä¿ç•™ metadata ä¸­çš„è¡¨æ ¼è³‡è¨Š
                if results and 'documents' in results:
                    for doc_data in results['documents']:
                        source_info = doc_data.get('source', {})
                        source_file = source_info.get('file', '')

                        if source_file:
                            content = doc_data.get('content', '')
                            
                            # ğŸ†• å¾ metadata æå–è¡¨æ ¼è³‡è¨Š
                            metadata = doc_data.get('metadata', {})
                            has_table = metadata.get('has_table', False)
                            table_images = metadata.get('table_images', [])
                            
                            # å¦‚æœ metadata æ²’æœ‰ï¼Œæª¢æŸ¥ source_info
                            if not has_table:
                                has_table = source_info.get('has_table', False)
                            if not table_images:
                                table_images = source_info.get('table_images', [])

                            if datasource.source_type == "jsonl":
                                # JSONL æ•¸æ“šè™•ç†ï¼ˆä¸æ¸…ç†ï¼Œä¿æŒåŸæ¨£ï¼‰
                                if content and 'å•é¡Œ:' in content and 'ç­”æ¡ˆ:' in content:
                                    docs_dict[source_file] = {
                                        'content': content,
                                        'has_table': has_table,
                                        'table_images': table_images
                                    }
                                else:
                                    title = doc_data.get('title', '')
                                    keywords = doc_data.get('keywords', '')
                                    reference = source_info.get('reference', '')

                                    qa_text = ""
                                    if title:
                                        qa_text += f"å•é¡Œ: {title}\n"
                                    if content:
                                        qa_text += f"ç­”æ¡ˆ: {content}\n"
                                    if keywords:
                                        qa_text += f"é—œéµå­—: {keywords}\n"
                                    if reference:
                                        qa_text += f"åƒè€ƒ: {reference}\n"

                                    if qa_text:
                                        docs_dict[source_file] = {
                                            'content': qa_text.strip(),
                                            'has_table': has_table,
                                            'table_images': table_images
                                        }
                            else:
                                # PDF æ•¸æ“šè™•ç† - ğŸ†• åªæœ‰ PDF éœ€è¦æ¸…ç†ä¸å®Œæ•´çš„å¥å­
                                if content:
                                    # ğŸ†• æ¸…ç†ä¸å®Œæ•´çš„å¥å­
                                    content = clean_incomplete_sentences(content)
                                    docs_dict[source_file] = {
                                        'content': content,
                                        'has_table': has_table,
                                        'table_images': table_images
                                    }

            retrieval_results[datasource.id] = (summary, docs_dict, results)
            
        except Exception as e:
            import traceback
            traceback.print_exc()

    # ğŸ”¹ æ­¥é©Ÿ 2.5: å‹•æ…‹ PDF æª¢ç´¢ï¼ˆå¦‚æœ medical_kb_jsonl æœ‰ reference æ¬„ä½ï¼‰
    # ğŸ†• æ”¹é€²ï¼šå¾åŸå§‹æª¢ç´¢çµæœæå– referenceï¼Œä¸ä¾è³´ç·šç´¢æå–çµæœ
    if "medical_kb_jsonl" in raw_results:
        try:
            unique_references = set()

            # ğŸ†• å¾åŸå§‹æª¢ç´¢çµæœï¼ˆresults_with_scoresï¼‰æå– reference
            raw_jsonl_results = raw_results["medical_kb_jsonl"]
            if raw_jsonl_results:
                for doc, _ in raw_jsonl_results:
                    metadata = doc.metadata if hasattr(doc, 'metadata') else {}
                    ref = metadata.get('reference', '')
                    if ref:
                        unique_references.add(ref)
                        print(f"   ğŸ“Œ å¾åŸå§‹ JSONL æª¢ç´¢çµæœæå– reference: {ref}")

            # ğŸ†• å›é€€ï¼šå¦‚æœåŸå§‹çµæœæ²’æœ‰ referenceï¼Œå˜—è©¦å¾ç·šç´¢æå–å¾Œçš„çµæœä¸­æå–
            if not unique_references and "medical_kb_jsonl" in retrieval_results:
                jsonl_summary, _, jsonl_results = retrieval_results["medical_kb_jsonl"]
                if jsonl_results and 'documents' in jsonl_results:
                    for doc in jsonl_results['documents']:
                        source = doc.get('source', {})
                        ref = source.get('reference', '')
                        if ref:
                            unique_references.add(ref)

                # å†å¾æ‘˜è¦ä¸­æå–
                if not unique_references and jsonl_summary:
                    ref_matches = re.findall(r'ã€Š([^ã€‹]+)ã€‹', jsonl_summary)
                    unique_references.update(ref_matches)

            print(f"   ğŸ“‹ å¾ JSONL çµæœæå–åˆ° {len(unique_references)} å€‹ä¸åŒçš„ reference")
            
            for selected_reference in unique_references:
                try:
                    pdf_collection_name = convert_references_to_english(selected_reference)
                    pdf_k = RETRIEVAL_CONFIG.get("pdf_k", 3)
                    
                    # print(f"   ğŸ”„ å‹•æ…‹æª¢ç´¢ PDF: {selected_reference} â†’ {pdf_collection_name} (k={pdf_k})")
                    
                    pdf_vectorstore = await loop.run_in_executor(
                        None, load_existing_vectordb, DB_CONNECTION_STRING, pdf_collection_name
                    )
                    
                    if pdf_vectorstore:
                        pdf_results_with_scores = search_vectordb_with_scores(
                            pdf_vectorstore, query, k=pdf_k
                        )
                        
                        if pdf_results_with_scores:
                            pdf_summary = ""
                            pdf_docs_dict = {}
                            
                            if use_clue_extraction and store:
                                try:
                                    print(f"      ğŸ” å° PDF é€²è¡Œç·šç´¢æå–åˆ¤æ–·ç›¸é—œæ€§...")
                                    pdf_summary, _, pdf_docs_dict = await process_retrieval_with_clue_extraction(
                                        sub_question=query,
                                        documents_with_scores=pdf_results_with_scores,
                                        main_question=main_question,
                                        store=store,
                                        user_id=user_id
                                    )

                                    # ğŸ†• æª¢æŸ¥æ˜¯å¦æœ‰ç›¸é—œçµæœ
                                    if pdf_docs_dict:
                                        print(f"      âœ… PDF ç·šç´¢æå–æˆåŠŸï¼Œæ‰¾åˆ° {len(pdf_docs_dict)} å€‹ç›¸é—œæ–‡æª”")
                                    else:
                                        print(f"      âš ï¸ PDF ç·šç´¢æå–åˆ¤æ–·ï¼šç„¡ç›¸é—œæ–‡æª”")

                                    # ğŸ†• æ ¼å¼è½‰æ›ä¸¦ä¿ç•™è¡¨æ ¼è³‡è¨Š
                                    new_pdf_docs = {}
                                    for k, v in pdf_docs_dict.items():
                                        if isinstance(v, dict):
                                            content = v.get('content', '')
                                            # ğŸ†• æ¸…ç†PDFå…§å®¹ä¸­çš„ä¸å®Œæ•´å¥å­
                                            content = clean_incomplete_sentences(content)
                                            new_pdf_docs[k] = {
                                                'content': content,
                                                'has_table': v.get('has_table', False),
                                                'table_images': v.get('table_images', [])
                                            }
                                        else:
                                            # ğŸ†• æ¸…ç†PDFå…§å®¹ä¸­çš„ä¸å®Œæ•´å¥å­
                                            content = clean_incomplete_sentences(str(v))
                                            new_pdf_docs[k] = {'content': content, 'has_table': False, 'table_images': []}
                                    pdf_docs_dict = new_pdf_docs
                                    
                                except Exception as e:
                                    print(f"      âš ï¸ PDF ç·šç´¢æå–å¤±æ•—: {e}ï¼Œä½¿ç”¨åŸå§‹æ ¼å¼")
                                    pdf_results, pdf_summary = get_structured_pdf_results(
                                        pdf_results_with_scores, query,
                                        show_scores=False, format_type="markdown"
                                    )
                                    if pdf_results and 'documents' in pdf_results:
                                        for doc_data in pdf_results['documents']:
                                            source_info = doc_data.get('source', {})
                                            source_file = source_info.get('file', '')
                                            content = doc_data.get('content', '')
                                            metadata = doc_data.get('metadata', {})

                                            if source_file and content:
                                                # ğŸ†• æ¸…ç†ä¸å®Œæ•´çš„å¥å­
                                                content = clean_incomplete_sentences(content)
                                                pdf_docs_dict[source_file] = {
                                                    'content': content,
                                                    'has_table': metadata.get('has_table', False),
                                                    'table_images': metadata.get('table_images', [])
                                                }
                            else:
                                pdf_results, pdf_summary = get_structured_pdf_results(
                                    pdf_results_with_scores, query,
                                    show_scores=False, format_type="markdown"
                                )
                                if pdf_results and 'documents' in pdf_results:
                                    for doc_data in pdf_results['documents']:
                                        source_info = doc_data.get('source', {})
                                        source_file = source_info.get('file', '')
                                        content = doc_data.get('content', '')
                                        metadata = doc_data.get('metadata', {})

                                        # ğŸ” DEBUG: é¡¯ç¤ºæª¢ç´¢åˆ°çš„ metadata
                                        print(f"\nğŸ” [DEBUG] æª¢ç´¢åˆ°æ–‡æª”: {source_file}")
                                        print(f"   metadata keys: {list(metadata.keys())}")
                                        print(f"   has_table: {metadata.get('has_table', 'NOT_FOUND')}")
                                        print(f"   table_images: {metadata.get('table_images', 'NOT_FOUND')}")

                                        if source_file and content:
                                            # ğŸ†• æ¸…ç†ä¸å®Œæ•´çš„å¥å­
                                            content = clean_incomplete_sentences(content)
                                            pdf_docs_dict[source_file] = {
                                                'content': content,
                                                'has_table': metadata.get('has_table', False),
                                                'table_images': metadata.get('table_images', [])
                                            }

                                            print(f"   âœ… å„²å­˜åˆ° pdf_docs_dict:")
                                            print(f"      has_table: {pdf_docs_dict[source_file]['has_table']}")
                                            print(f"      table_images: {pdf_docs_dict[source_file]['table_images']}")
                            
                            retrieval_results[f"dynamic_pdf_{pdf_collection_name}"] = (pdf_summary, pdf_docs_dict, None)
                            print(f"      âœ… å‹•æ…‹ PDF æª¢ç´¢å®Œæˆ: {len(pdf_results_with_scores)} ç­†çµæœ")
                    else:
                        print(f"      âš ï¸ æ‰¾ä¸åˆ° PDF å‘é‡è³‡æ–™åº«: {pdf_collection_name}")
                        
                except Exception as e:
                    print(f"   âš ï¸ å‹•æ…‹ PDF æª¢ç´¢å¤±æ•— ({selected_reference}): {e}")
                    
        except Exception as e:
            print(f"   âš ï¸ å‹•æ…‹ PDF æª¢ç´¢æ•´é«”å¤±æ•—: {e}")

    # ğŸ”¹ æ­¥é©Ÿ 3: åˆä½µçµæœ
    if not retrieval_results:
        return state

    all_summaries = [summary for summary, _, _ in retrieval_results.values() if summary]
    all_docs = {}

    for ds_id, (summary, docs, _) in retrieval_results.items():
        
        for doc_name, doc_info in docs.items():
            # ğŸ†• è™•ç†æ–°çš„å­—å…¸æ ¼å¼
            if isinstance(doc_info, dict):
                doc_content = doc_info.get('content', '')
                has_table = doc_info.get('has_table', False)
                table_images = doc_info.get('table_images', [])
            else:
                doc_content = str(doc_info)
                has_table = False
                table_images = []
            
            if doc_name in all_docs:
                # åˆä½µå…§å®¹
                existing = all_docs[doc_name]
                existing_content = existing.get('content', '') if isinstance(existing, dict) else str(existing)
                merged_content = merge_documents_intelligently(doc_name, existing_content, doc_content)
                
                # åˆä½µè¡¨æ ¼è³‡è¨Šï¼ˆå–è¯é›†ï¼‰
                existing_tables = existing.get('table_images', []) if isinstance(existing, dict) else []
                merged_tables = list(set(existing_tables + table_images))
                
                all_docs[doc_name] = {
                    'content': merged_content,
                    'has_table': has_table or existing.get('has_table', False),
                    'table_images': merged_tables
                }
            else:
                normalized_content = merge_documents_intelligently(doc_name, "", doc_content)
                all_docs[doc_name] = {
                    'content': normalized_content,
                    'has_table': has_table,
                    'table_images': table_images
                }

    combined_knowledge = "\n\n---\n\n".join(all_summaries) if all_summaries else ""
    original_sources = extract_sources_from_knowledge(combined_knowledge)

    # æ›´æ–°ç‹€æ…‹
    state["knowledge"] = combined_knowledge
    state["original_sources"] = original_sources
    state["original_docs_dict"].update(all_docs)

    # åˆä½µå®Œæˆ

    # ğŸ†• æ­¥é©Ÿ 4: è¡¨æ ¼åœ–ç‰‡åŒ¹é…ï¼ˆå„ªå…ˆå¾ metadataï¼Œfallback åˆ°ç›¸ä¼¼åº¦åŒ¹é…ï¼‰
    try:
        import sys
        import os
        ocr_model_path = os.path.join(os.path.dirname(__file__), 'ocr_model')
        if ocr_model_path not in sys.path:
            sys.path.insert(0, ocr_model_path)

        from retrieval.table_matcher import has_table_format, process_search_result

        matched_tables = []
        metadata_match_count = 0
        similarity_match_count = 0

        # æª¢æŸ¥åˆä½µå¾Œçš„æ–‡æª”æ˜¯å¦åŒ…å«è¡¨æ ¼
        for doc_name, doc_info in all_docs.items():
            if isinstance(doc_info, dict):
                doc_content = doc_info.get('content', '')
                doc_metadata = {
                    'has_table': doc_info.get('has_table', False),
                    'table_images': doc_info.get('table_images', [])
                }
            else:
                doc_content = str(doc_info)
                doc_metadata = None

            # ğŸ†• å„ªå…ˆæª¢æŸ¥ metadata ä¸­æ˜¯å¦æœ‰è¡¨æ ¼
            if doc_metadata and doc_metadata.get('has_table') and doc_metadata.get('table_images'):
                print(f"      ğŸ“„ æ–‡æª” {doc_name}: å¾ metadata å–å¾—è¡¨æ ¼")

                from core.config import EXTRACTED_TABLES_DIR

                for img_filename in doc_metadata['table_images']:
                    # å…¼å®¹æ–°èˆŠæ ¼å¼ï¼šå¦‚æœæ˜¯å®Œæ•´è·¯å¾‘ï¼Œå‰‡æå–æª”æ¡ˆåç¨±
                    if os.path.sep in img_filename or '/' in img_filename:
                        # èˆŠæ ¼å¼ï¼šå®Œæ•´è·¯å¾‘ï¼Œæå–æª”æ¡ˆåç¨±
                        img_filename = os.path.basename(img_filename)

                    # çµ„åˆå®Œæ•´è·¯å¾‘ï¼šEXTRACTED_TABLES_DIR / images / æª”æ¡ˆåç¨±
                    full_path = os.path.join(EXTRACTED_TABLES_DIR, "images", img_filename)

                    if os.path.exists(full_path):
                        matched_tables.append({
                            'matched': True,
                            'table_content': None,
                            'image_path': full_path,
                            'similarity': 1.0,
                            'source': 'metadata'
                        })
                        metadata_match_count += 1
                        print(f"         âœ… metadata åŒ¹é…: {img_filename}")
                    else:
                        print(f"         âš ï¸ åœ–ç‰‡ä¸å­˜åœ¨: {full_path}")
                        
            # ğŸ†• Fallback: å¦‚æœ metadata æ²’æœ‰ä½†å…§å®¹åŒ…å«è¡¨æ ¼ï¼Œç”¨ç›¸ä¼¼åº¦åŒ¹é…
            elif has_table_format(doc_content):
                print(f"      ğŸ“„ æ–‡æª” {doc_name}: metadata ç„¡è¡¨æ ¼è³‡è¨Šï¼Œä½¿ç”¨ç›¸ä¼¼åº¦åŒ¹é…")

                page_num = None
                page_match = re.search(r'_p(\d+)_', doc_name)
                if page_match:
                    page_num = int(page_match.group(1))

                match_result = process_search_result(
                    content=doc_content,
                    source_file=doc_name,
                    page_num=page_num,
                    metadata=None  # æ˜ç¢ºå‚³å…¥ Noneï¼Œè§¸ç™¼ç›¸ä¼¼åº¦åŒ¹é…
                )

                if match_result.get('matched_tables'):
                    for table in match_result['matched_tables']:
                        img_path = table.get('image_path', '')
                        table_filename = os.path.basename(img_path) if img_path else 'æœªçŸ¥æª”æ¡ˆ'
                        print(f"         âœ… ç›¸ä¼¼åº¦åŒ¹é…: {table_filename} ({table['similarity']:.2%})")
                        matched_tables.append(table)
                        similarity_match_count += 1

        if matched_tables:
            # å»é‡ï¼ˆæ ¹æ“š image_pathï¼‰
            seen_paths = set()
            unique_tables = []
            for table in matched_tables:
                img_path = table.get('image_path', '')
                if img_path and img_path not in seen_paths:
                    seen_paths.add(img_path)
                    unique_tables.append(table)

            state["matched_table_images"] = unique_tables
            print(f"   ğŸ‰ ç¸½å…±åŒ¹é…åˆ° {len(unique_tables)} å€‹è¡¨æ ¼åœ–ç‰‡")
            print(f"      - metadata åŒ¹é…: {metadata_match_count} å€‹")
            print(f"      - ç›¸ä¼¼åº¦åŒ¹é…: {similarity_match_count} å€‹")
        else:
            print(f"   â„¹ï¸  æª¢ç´¢çµæœä¸­ç„¡è¡¨æ ¼å…§å®¹æˆ–æœªåŒ¹é…åˆ°è¡¨æ ¼")

    except Exception as e:
        print(f"   âš ï¸  è¡¨æ ¼åŒ¹é…å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()

    return state

async def retrieve_public_rag_node(state: RetrievalState, config: Optional[RunnableConfig] = None) -> RetrievalState:
    """è¡›æ•™åœ’åœ°æª¢ç´¢ç¯€é»"""
    query = state["query"]
    main_question = state["main_question"]
    use_clue_extraction = state["use_clue_extraction"]
    user_id = state["user_id"]

    # å¾ config ä¸­ç²å– store (éåºåˆ—åŒ–å°è±¡)
    store = None
    if config and "configurable" in config:
        store = config["configurable"].get("store")

    loop = asyncio.get_running_loop()

    try:
        vectorstore = await loop.run_in_executor(
            None, load_existing_vectordb, DB_CONNECTION_STRING, "public_health_information_of_education_sites"
        )

        if vectorstore:
            # ä½¿ç”¨é…ç½®ä¸­çš„æª¢ç´¢æ•¸é‡
            from core.config import RETRIEVAL_CONFIG
            k_value = RETRIEVAL_CONFIG.get("public_rag_k", 3)
            results_with_scores = search_vectordb_with_scores(vectorstore, query, k=k_value)

            if results_with_scores:
                # ç·šç´¢æå–
                if use_clue_extraction and store:
                    try:
                        knowledge, _, original_docs_dict = await process_retrieval_with_clue_extraction(
                            sub_question=query,
                            documents_with_scores=results_with_scores,
                            main_question=main_question,
                            store=store,
                            user_id=user_id
                        )
                        state["knowledge"] = knowledge
                        state["original_docs_dict"] = original_docs_dict
                        print("âœ… è¡›æ•™åœ’åœ°ç·šç´¢æå–æˆåŠŸ")
                    except Exception as e:
                        print(f"âš ï¸ è¡›æ•™åœ’åœ°ç·šç´¢æå–å¤±æ•—: {e}ï¼Œä½¿ç”¨åŸå§‹æ ¼å¼")
                        results, _ = get_structured_pdf_results(
                            results_with_scores,
                            query,
                            show_scores=False,
                            format_type="llm_friendly"
                        )
                        if results and isinstance(results, dict):
                            state["knowledge"] = json.dumps(results, ensure_ascii=False, indent=2)
                else:
                    # æœªå•Ÿç”¨ç·šç´¢æå–
                    results, _ = get_structured_pdf_results(
                        results_with_scores,
                        query,
                        show_scores=False,
                        format_type="llm_friendly"
                    )
                    if results and isinstance(results, dict):
                        state["knowledge"] = json.dumps(results, ensure_ascii=False, indent=2)

                # æå–ä¾†æº
                state["original_sources"] = extract_sources_from_knowledge(state["knowledge"])

    except Exception as e:
        print(f"âš ï¸ è¡›æ•™åœ’åœ°æª¢ç´¢å¤±æ•—: {e}")
        state["knowledge"] = ""

    return state


async def check_procedure_node(state: RetrievalState) -> RetrievalState:
    """æª¢æŸ¥æ˜¯å¦ç‚ºé†«ç™‚ç¨‹åº"""
    query = state["query"]

    try:
        msg = [
            SystemMessage(content=MEDICAL_PROCEDURE_CHECK_PROMPT),
            HumanMessage(content=f"å•é¡Œï¼š{query}")
        ]
        resp = await llm.ainvoke(msg)
        is_procedure = remove_think_tags(resp.content.strip()).lower() == "true"
        state["is_procedure"] = is_procedure
        print(f"ğŸ”§ [CHECK] æ˜¯å¦é†«ç™‚ç¨‹åº: {is_procedure}")
    except:
        state["is_procedure"] = False

    return state


def route_after_procedure_check(state: RetrievalState) -> Literal["procedure", "none"]:
    """é†«ç™‚ç¨‹åºæª¢æŸ¥å¾Œçš„è·¯ç”±"""
    if state["is_procedure"]:
        print("ğŸ”€ [ROUTE] æ˜¯é†«ç™‚ç¨‹åº â†’ ç¨‹åºæª¢ç´¢")
        return "procedure"
    else:
        print("ğŸ”€ [ROUTE] éé†«ç™‚ç¨‹åº â†’ çµæŸæª¢ç´¢")
        return "none"


async def retrieve_procedure_node(state: RetrievalState) -> RetrievalState:
    """é†«ç™‚ç¨‹åºæª¢ç´¢ç¯€é» - ä½¿ç”¨ medical_kb_jsonl è³‡æ–™æº"""
    query = state["query"]

    print(f"ğŸ” [PROCEDURE] æª¢ç´¢é†«ç™‚ç¨‹åº: {query}")

    try:
        # ä½¿ç”¨ medical_kb_jsonl è³‡æ–™æºæª¢ç´¢
        from medical_knowledge_fetcher import get_final_answer_from_query_async

        knowledge = (await get_final_answer_from_query_async(query)).strip()

        state["knowledge"] = knowledge
        state["original_sources"] = extract_sources_from_knowledge(knowledge)

        # æå–åŸå§‹æ–‡æª”å…§å®¹
        if not state["original_docs_dict"]:
            if "**åƒè€ƒä¾æ“š**" in knowledge or "ã€Š" in knowledge:
                for source in state["original_sources"]:
                    pattern = rf'ã€Š{re.escape(source)}ã€‹(.*?)(?=ã€Š|ä¾†æºï¼š|---|\Z)'
                    matches = re.findall(pattern, knowledge, re.DOTALL)
                    if matches:
                        all_content = []
                        seen_content = set()
                        for match_content in matches:
                            content = match_content.strip()
                            if content and content not in seen_content:
                                all_content.append(content)
                                seen_content.add(content)

                        if all_content:
                            state["original_docs_dict"][source] = '\n'.join(all_content)

    except Exception as e:
        print(f"âš ï¸ é†«ç™‚ç¨‹åºæª¢ç´¢å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        state["knowledge"] = ""

    return state


async def no_retrieval_node(state: RetrievalState) -> RetrievalState:
    """ç„¡æª¢ç´¢ç¯€é»ï¼šè¿”å›ç©ºçµæœ"""
    print("ğŸ”š [END] éé†«ç™‚ç›¸é—œå•é¡Œï¼Œä¸é€²è¡Œæª¢ç´¢")
    return state


def extract_sources_from_retrieval_summaries(pdf_summary: str, rag_summary: str, knowledge: str) -> List[str]:
    """å¾æª¢ç´¢æ‘˜è¦å’Œç”Ÿæˆçš„çŸ¥è­˜æ–‡æœ¬ä¸­æå–ä¾†æº"""
    sources_from_retrieval = []

    # å¾ pdf_summary æå–
    if pdf_summary:
        try:
            pdf_data = json.loads(pdf_summary)
            if isinstance(pdf_data, dict) and 'documents' in pdf_data:
                for doc in pdf_data['documents']:
                    if 'source' in doc and 'pdf_file' in doc['source']:
                        sources_from_retrieval.append(doc['source']['pdf_file'])
        except:
            sources_from_pdf = extract_sources_from_knowledge(pdf_summary)
            sources_from_retrieval.extend(sources_from_pdf)

    # å¾ rag_summary æå–
    if rag_summary:
        try:
            rag_data = json.loads(rag_summary)
            if isinstance(rag_data, dict) and 'documents' in rag_data:
                for doc in rag_data['documents']:
                    if 'source' in doc and 'reference' in doc['source']:
                        sources_from_retrieval.append(doc['source']['reference'])
        except:
            sources_from_rag = extract_sources_from_knowledge(rag_summary)
            sources_from_retrieval.extend(sources_from_rag)

    # å¾ç”Ÿæˆçš„ knowledge æ–‡æœ¬ä¸­æå–
    sources_from_text = extract_sources_from_knowledge(knowledge)

    # åˆä½µä¸¦å»é‡
    return list(set(sources_from_retrieval + sources_from_text))


# ==================== æ§‹å»ºæª¢ç´¢åœ– ====================

def create_retrieval_graph() -> StateGraph:
    """
    å‰µå»ºæª¢ç´¢æµç¨‹åœ–
    ğŸ†• æ–°ç‰ˆæœ¬ï¼šä½¿ç”¨å¤šè³‡æ–™æºæ¶æ§‹
    """
    workflow = StateGraph(RetrievalState)

    # æ·»åŠ ç¯€é»
    workflow.add_node("initialize", initialize_retrieval_node)
    workflow.add_node("multi_source", retrieve_multi_source_node)  # ğŸ†• æ–°çš„å¤šè³‡æ–™æºç¯€é»
    workflow.add_node("check_procedure", check_procedure_node)
    workflow.add_node("retrieve_procedure", retrieve_procedure_node)
    workflow.add_node("no_retrieval", no_retrieval_node)

    # å‘å¾Œå…¼å®¹ï¼šä¿ç•™èˆŠç¯€é»ï¼ˆä½†ä¸å†ä½¿ç”¨ï¼‰
    # workflow.add_node("retrieve_medical_kb", retrieve_medical_kb_node)
    # workflow.add_node("retrieve_public_rag", retrieve_public_rag_node)

    # è¨­ç½®å…¥å£é»
    workflow.set_entry_point("initialize")

    # ğŸ†• æ–°çš„æ¢ä»¶é‚Šï¼šä½¿ç”¨å¤šè³‡æ–™æºæ¶æ§‹
    workflow.add_conditional_edges(
        "initialize",
        route_retrieval_strategy,
        {
            "multi_source": "multi_source",      # ğŸ†• é†«ç™‚å•é¡Œ â†’ å¤šè³‡æ–™æºæª¢ç´¢
            "check_procedure": "check_procedure"  # éé†«ç™‚ â†’ æª¢æŸ¥ç¨‹åº
        }
    )

    # å¤šè³‡æ–™æºæª¢ç´¢å¾ŒçµæŸ
    workflow.add_edge("multi_source", END)

    # ç¨‹åºæª¢æŸ¥å¾Œçš„æ¢ä»¶è·¯ç”±
    workflow.add_conditional_edges(
        "check_procedure",
        route_after_procedure_check,
        {
            "procedure": "retrieve_procedure",
            "none": "no_retrieval"
        }
    )

    # ç¨‹åºæª¢ç´¢å¾ŒçµæŸ
    workflow.add_edge("retrieve_procedure", END)

    # ç„¡æª¢ç´¢å¾ŒçµæŸ
    workflow.add_edge("no_retrieval", END)

    return workflow.compile()


# å‰µå»ºå…¨å±€æª¢ç´¢åœ–å¯¦ä¾‹
_retrieval_graph = create_retrieval_graph()


# ==================== ä¸»è¦æª¢ç´¢å‡½æ•¸ ====================

@observe(as_type="retriever")
async def retrieve_single_query(
    query: str,
    main_question: str = None,
    use_clue_extraction: bool = None,
    store = None,
    user_id: str = "default",
    datasource_ids: Optional[List[str]] = None,  # æŒ‡å®šè¦ä½¿ç”¨çš„è³‡æ–™æº ID åˆ—è¡¨
    _langfuse_handler = None,  # ä¿ç•™ä»¥å…¼å®¹èˆŠç‰ˆèª¿ç”¨ï¼ˆç›®å‰æœªä½¿ç”¨ï¼‰
    sub_question_index: int = None,  # ğŸ†• å­å•é¡Œç·¨è™Ÿï¼ˆç”¨æ–¼è¿½è¹¤ï¼‰
    is_main_question: bool = False  # ğŸ”’ æ¨™è¨˜æ˜¯å¦ç‚ºä¸»å•é¡Œï¼ˆä¸»å•é¡Œä¸å¿«å–ï¼Œé¿å…å€‹äººä¿¡æ¯æ³„éœ²ï¼‰
) -> tuple[str, list[str], dict]:
    """
    å–®ä¸€æŸ¥è©¢æª¢ç´¢ï¼ˆä½¿ç”¨ LangGraph å¤šè³‡æ–™æºæ¶æ§‹ï¼‰
    å¯é¸æ“‡æ€§åœ°ä½¿ç”¨ç·šç´¢æå–ä¾†å„ªåŒ–æª¢ç´¢çµæœ

    Args:
        query: æŸ¥è©¢å•é¡Œ(å¯èƒ½æ˜¯å­å•é¡Œ)
        main_question: ç”¨æˆ¶çš„ä¸»å•é¡Œ(ç”¨æ–¼åˆ¤æ–·ç›¸é—œæ€§,é è¨­èˆ‡queryç›¸åŒ)
        use_clue_extraction: æ˜¯å¦ä½¿ç”¨ç·šç´¢æå–ï¼ˆNone=ä½¿ç”¨é…ç½®æª”è¨­å®šï¼‰
        store: LangGraph BaseStore ç”¨æ–¼å¿«å–ï¼ˆå¯é¸ï¼‰
        user_id: ç”¨æˆ¶IDï¼ˆç”¨æ–¼å¿«å–å‘½åç©ºé–“ï¼‰
        datasource_ids: æŒ‡å®šè¦ä½¿ç”¨çš„è³‡æ–™æº ID åˆ—è¡¨ï¼Œå¦‚ ["medical_kb_jsonl", "public_health"]
                       None = è‡ªå‹•æ ¹æ“šå•é¡Œé¡å‹é¸æ“‡
        langfuse_handler: Langfuse CallbackHandlerï¼ˆä¿ç•™ä»¥å…¼å®¹ï¼‰
        sub_question_index: å­å•é¡Œç·¨è™Ÿï¼ˆç”¨æ–¼è¿½è¹¤ï¼‰

    Returns:
        (knowledge_text, source_list, used_sources_dict, matched_tables)

    ç¯„ä¾‹:
        # è‡ªå‹•é¸æ“‡è³‡æ–™æº
        knowledge, sources, docs, tables = await retrieve_single_query("ç³–å°¿ç—…çš„ç—‡ç‹€")

        # æŒ‡å®šä½¿ç”¨ç‰¹å®šè³‡æ–™æº
        knowledge, sources, docs, tables = await retrieve_single_query(
            "ç³–å°¿ç—…çš„ç—‡ç‹€",
            datasource_ids=["medical_kb_jsonl", "diabetes_education"]
        )

        # åƒ…ä½¿ç”¨è¡›æ•™åœ’åœ°
        knowledge, sources, docs, tables = await retrieve_single_query(
            "é«˜è¡€å£“é é˜²",
            datasource_ids=["public_health"]
        )
    """

    # ğŸ†• ä½¿ç”¨ @observe è£é£¾å™¨å¾Œï¼Œè‡ªå‹•å‰µå»º observation
    # æ‰‹å‹•è¨­ç½® name å’Œ metadata
    if LANGFUSE_AVAILABLE and get_langfuse_client:
        span_name = f"sub_question_{sub_question_index}_retrieval" if sub_question_index else "single_query_retrieval"
        try:
            get_langfuse_client().update_current_span(
                name=span_name,
                metadata={
                    "query": query,
                    "main_question": main_question,
                    "datasource_ids": datasource_ids,
                    "sub_question_index": sub_question_index
                }
            )
        except Exception as e:
            print(f"âš ï¸ æ›´æ–° Langfuse observation å¤±æ•—: {e}")

    # ğŸ†• Step 1: å˜—è©¦å¾ Retrieval Cache ç²å–
    # ğŸ”§ å±€éƒ¨å°å…¥ä»¥é¿å…å¾ªç’°å°å…¥
    from graph.graph_nodes import get_cache_manager

    cache_manager = get_cache_manager()
    # cache_manager.print_stats()

    # ğŸ”’ ä¸»å•é¡Œä¸ä½¿ç”¨ Retrieval Cacheï¼ˆé¿å…å€‹äººä¿¡æ¯æ³„éœ²ï¼‰
    if not is_main_question and cache_manager:
        cached_result = cache_manager.get_retrieval_cache(
            query,
            datasource_ids=datasource_ids
        )

        if cached_result:
            print(f"âœ… Retrieval Cache Hit: {query[:50]}...")

            # ğŸ†• æ›´æ–° Langfuse metadataï¼ˆæ¨™è¨˜ç‚ºå¿«å–å‘½ä¸­ï¼‰
            if LANGFUSE_AVAILABLE and get_langfuse_client:
                try:
                    get_langfuse_client().update_current_span(
                        metadata={
                            "cache_hit": True,
                            "sources_count": len(cached_result[1]) if len(cached_result) > 1 else 0,
                            "has_knowledge": bool(cached_result[0]) if cached_result else False
                        }
                    )
                except Exception as e:
                    print(f"âš ï¸ æ›´æ–° Langfuse observation metadata å¤±æ•—: {e}")

            # ç›´æ¥è¿”å›å¿«å–çµæœ (knowledge, sources, docs_dict, tables)
            return cached_result
    elif is_main_question:
        print(f"ğŸ”’ ä¸»å•é¡Œè·³é Retrieval Cacheï¼ˆéš±ç§ä¿è­·ï¼‰: {query[:50]}...")

    # æ§‹å»ºåˆå§‹ç‹€æ…‹ (æ³¨æ„: store ä¸èƒ½æ”¾åœ¨ç‹€æ…‹ä¸­,å› ç‚ºç„¡æ³•åºåˆ—åŒ–)
    initial_state: RetrievalState = {
        "query": query,
        "main_question": main_question,
        "use_clue_extraction": use_clue_extraction,
        "user_id": user_id or "default",
        "datasource_ids": datasource_ids,
        # ä»¥ä¸‹å­—æ®µå°‡ç”±ç¯€é»å¡«å……
        "actual_query": "",
        "matched_disease": "",
        "is_medical": False,
        "is_procedure": False,
        "retrieval_strategy": "none",
        "knowledge": "",
        "original_sources": [],
        "original_docs_dict": {},
        "matched_table_images": [],  # ğŸ†• è¡¨æ ¼åœ–ç‰‡åŒ¹é…çµæœåˆå§‹åŒ–
        "retrieval_summaries": {},  # ğŸ†• æ”¹ç‚ºå­—å…¸
        "json_results": {},
        "pdf_results": {}
    }

    # æ§‹å»ºé…ç½® (é€šé config å‚³éä¸å¯åºåˆ—åŒ–çš„å°è±¡,å¦‚ store)
    run_config = {
        "configurable": {
            "store": store
        }
    }

    # åŸ·è¡Œæª¢ç´¢åœ–
    final_state = await _retrieval_graph.ainvoke(initial_state, run_config)

    # ğŸ†• Step 2: å„²å­˜åˆ° Retrieval Cache
    result = (
        final_state["knowledge"],
        final_state["original_sources"],
        final_state["original_docs_dict"],
        final_state.get("matched_table_images", [])  # ğŸ†• è¿”å›è¡¨æ ¼åœ–ç‰‡åŒ¹é…çµæœ
    )

    # ğŸ”’ ä¸»å•é¡Œä¸å¯«å…¥ Retrieval Cacheï¼ˆé¿å…å€‹äººä¿¡æ¯æ³„éœ²ï¼‰
    if not is_main_question and cache_manager:
        cache_manager.set_retrieval_cache(
            query,
            result,
            datasource_ids=datasource_ids
        )

    # ğŸ†• è¨˜éŒ„æª¢ç´¢çµæœçµ±è¨ˆåˆ° Langfuse metadataï¼ˆ@observe æœƒè‡ªå‹•è¨˜éŒ„ returnï¼‰
    if LANGFUSE_AVAILABLE and get_langfuse_client:
        try:
            get_langfuse_client().update_current_span(
                metadata={
                    "cache_hit": False,  # ğŸ†• æ¨™è¨˜ç‚ºå¿«å–æœªå‘½ä¸­ï¼ˆåŸ·è¡Œäº†æª¢ç´¢ï¼‰
                    "sources_count": len(final_state["original_sources"]),
                    "sources": final_state["original_sources"],
                    "has_knowledge": bool(final_state["knowledge"]),
                    "knowledge_length": len(final_state["knowledge"]) if final_state["knowledge"] else 0,
                    "matched_tables_count": len(final_state.get("matched_table_images", []))
                }
            )
        except Exception as e:
            print(f"âš ï¸ æ›´æ–° Langfuse observation metadata å¤±æ•—: {e}")

    # è¿”å›çµæœï¼ˆåŒ…å«è¡¨æ ¼åœ–ç‰‡åŒ¹é…çµæœï¼‰
    return result

# ==================== æŸ¥è©¢è™•ç†å‡½æ•¸ ====================

async def expand_entity_to_query(user_query: str, entity: str) -> str:
    """å¯¦é«”æ“´å±•ç‚ºæŸ¥è©¢"""
    prompt = ENTITY_QUERY_EXPANSION_PROMPT.format(user_query=user_query, entity=entity)
    try:
        response = await llm.ainvoke(prompt)
        expanded_query = response.content.strip()
        print(f"ğŸ” å¯¦é«”æ“´å±•: '{entity}' â†’ '{expanded_query}'")
        return expanded_query
    except Exception as e:
        print(f"âš ï¸ å¯¦é«”æ“´å±•å¤±æ•—: {e}ï¼Œä½¿ç”¨åŸå§‹å¯¦é«”")
        return entity


async def check_query_complexity(user_query: str, history_context: str = "") -> dict:
    """æª¢æŸ¥æŸ¥è©¢è¤‡é›œåº¦ï¼Œå›å‚³æ¨™æº–åŒ–çµæœ"""
    # ä½¿ç”¨æ­£ç¢ºçš„ Pydantic æ¨¡å‹è§£æå™¨
    parser = PydanticOutputParser(pydantic_object=QueryComplexityResult)
    
    # åœ¨ prompt æœ«å°¾åŠ å…¥æ ¼å¼æŒ‡ç¤ºï¼ˆå¼·çƒˆå»ºè­°ï¼‰
    full_prompt = (
        QUERY_COMPLEXITY_CHECK_PROMPT_V2
        + "\n\n{format_instructions}"
    ).format(
        user_query=user_query,
        history_context=history_context,
        format_instructions=parser.get_format_instructions()
    )

    try:
        response = await llm.ainvoke(full_prompt)
        result_text = response.content.strip()
        # å˜—è©¦ç”¨ Pydantic è§£æï¼ˆè‡ªå‹•è™•ç† JSONã€é©—è­‰ã€è½‰å‹ï¼‰
        try:
            parsed: QueryComplexityResult = parser.parse(result_text)
            return {
                "is_complex": parsed.is_complex,
                "search_strategy": parsed.search_strategy,
                "extracted_entities": parsed.extracted_entities,
                "reasoning": parsed.reasoning,
                "analysis": result_text  # åŸå§‹æ–‡å­—å‚™æŸ¥
            }
        except Exception as parse_err:
            print(f"âš ï¸ Pydantic è§£æå¤±æ•—: {parse_err}")
            # å›é€€ï¼šå˜—è©¦æ‰‹å‹• JSON è§£æï¼ˆä¿å®ˆåšæ³•ï¼‰
            try:
                raw_dict = json.loads(result_text.strip('` \n'))
                return {
                    "is_complex": bool(raw_dict.get("is_complex", False)),
                    "search_strategy": raw_dict.get("search_strategy", "simple"),
                    "extracted_entities": raw_dict.get("extracted_entities", []),
                    "reasoning": raw_dict.get("reasoning", "ç„¡æ³•è§£æ reasoning"),
                    "analysis": result_text
                }
            except Exception as json_err:
                print(f"âš ï¸ JSON å›é€€ä¹Ÿå¤±æ•—: {json_err}")
                # æœ€çµ‚å›é€€ï¼šä¸€å¾‹ç•¶ç°¡å–®æŸ¥è©¢
                return {
                    "is_complex": False,
                    "search_strategy": "simple",
                    "extracted_entities": [],
                    "reasoning": "LLM å›æ‡‰ç„¡æ³•è§£æï¼Œé è¨­ä½¿ç”¨ simple ç­–ç•¥",
                    "analysis": result_text
                }

    except Exception as e:
        print(f"âš ï¸ æŸ¥è©¢è¤‡é›œåº¦æª¢æŸ¥ç•°å¸¸: {e}")
        return {
            "is_complex": False,
            "search_strategy": "simple",
            "extracted_entities": [],
            "reasoning": f"ç³»çµ±éŒ¯èª¤ï¼Œé è¨­ simple: {str(e)}",
            "analysis": str(e)
        }

async def generate_sub_questions(user_query: str, missing_info: str, reasoning: str = "") -> list[str]:
    """
    ç”Ÿæˆå­å•é¡Œç”¨æ–¼å¤šæ¬¡æª¢ç´¢

    Args:
        user_query: ç”¨æˆ¶çš„åŸå§‹å•é¡Œ
        missing_info: ç¼ºå¤±çš„è³‡è¨Šæè¿°
        reasoning: å•é¡Œæ„åœ–åˆ†æï¼ˆä¾†è‡ª QUESTION_TYPE_CHECK_PROMPT çš„ reasoning æ¬„ä½ï¼‰

    Returns:
        å­å•é¡Œåˆ—è¡¨
    """
    try:
        # å¦‚æœæ²’æœ‰æä¾› reasoningï¼Œä½¿ç”¨é è¨­å€¼
        if not reasoning:
            reasoning = "ç„¡å•é¡Œæ„åœ–åˆ†æ"

        prompt_text = SUB_QUESTIONS_GENERATION_PROMPT.format(
            user_query=user_query,
            missing_info=missing_info,
            reasoning=reasoning
        )
        response = await llm.ainvoke(prompt_text)

        # è§£æ JSON éŸ¿æ‡‰
        content = remove_think_tags(response.content.strip())

        # ç§»é™¤å¯èƒ½çš„ markdown ä»£ç¢¼å¡Šæ¨™è¨˜
        if content.startswith("```json"):
            content = content[7:]
        if content.startswith("```"):
            content = content[3:]
        if content.endswith("```"):
            content = content[:-3]
        content = content.strip()

        # è§£æ JSON
        parsed = json.loads(content)
        sub_questions = parsed.get("sub_questions", [])

        if not sub_questions:
            # å¦‚æœè§£æå¤±æ•—ï¼Œå›é€€åˆ°ç°¡å–®æ‹†åˆ†
            print(f"âš ï¸ æœªèƒ½å¾ JSON ä¸­æå–å­å•é¡Œï¼Œä½¿ç”¨åŸæŸ¥è©¢")
            return [f"{user_query} {missing_info}"]

        print(f"ğŸ“ ç”Ÿæˆäº† {len(sub_questions)} å€‹å­å•é¡Œ:")
        for i, q in enumerate(sub_questions, 1):
            print(f"   {i}. {q}")

        return sub_questions

    except json.JSONDecodeError as e:
        print(f"âš ï¸ JSON è§£æå¤±æ•—: {e}ï¼Œä½¿ç”¨åŸæŸ¥è©¢")
        return [f"{user_query} {missing_info}"]
    except Exception as e:
        print(f"âš ï¸ å­å•é¡Œç”Ÿæˆå¤±æ•—: {e}ï¼Œä½¿ç”¨åŸæŸ¥è©¢")
        return [f"{user_query} {missing_info}"]
