"""
æ–‡ç»ç·šç´¢æå–æ¨¡çµ„
é‡å°å­å•é¡Œå¾æª¢ç´¢åˆ°çš„æ–‡ç»ä¸­æå–çœŸæ­£æœ‰ç”¨çš„ç·šç´¢ï¼Œè€Œéç›´æ¥ä½¿ç”¨æ•´å€‹æ–‡ç»å…§å®¹
"""

from typing import List, Dict, Tuple, Any
from langchain_core.documents import Document
from langchain_core.language_models import BaseChatModel
from langgraph.store.base import BaseStore
import json
import hashlib
from core.config import llm, CLUE_EXTRACTION_CONFIG
from core.prompt_config import CLUE_EXTRACTION_PROMPT, CLUE_SUMMARY_PROMPT
from rag_system.unified_metadata import normalize_source_filename
import asyncio
import re
from bs4 import BeautifulSoup


# ==================== åƒè€ƒæ–‡ç»éæ¿¾ ====================

def is_references_section(text: str) -> Tuple[bool, int]:
    """
    æª¢æ¸¬æ–‡æœ¬æ˜¯å¦åŒ…å«åƒè€ƒæ–‡ç»å€å¡Š

    Returns:
        tuple: (is_references: bool, references_start_pos: int or None)
    """
    if not text:
        return False, None

    # åƒè€ƒæ–‡ç»æ¨™é¡Œæ¨¡å¼
    ref_title_patterns = [
        r'(^|\n)\s*#{0,3}\s*åƒè€ƒæ–‡[ç»çŒ®]',
        r'(^|\n)\s*#{0,3}\s*References?\s*\n',
        r'(^|\n)\s*#{0,3}\s*åƒè€ƒè³‡æ–™',
        r'(^|\n)\s*#{0,3}\s*å¼•ç”¨æ–‡[ç»çŒ®]',
        r'(^|\n)\s*#{0,3}\s*Bibliography',
        r'(^|\n)\s*#{0,3}\s*æ–‡[ç»çŒ®]\s*\n',
    ]

    for pattern in ref_title_patterns:
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            return True, match.start()

    # æª¢æ¸¬æ˜¯å¦å¤§éƒ¨åˆ†å…§å®¹éƒ½æ˜¯å¼•ç”¨æ ¼å¼
    lines = text.strip().split('\n')
    citation_count = 0
    total_lines = 0

    citation_patterns = [
        r'^\d+\.\s*[A-Z][a-z]+\s+[A-Z]{1,2}',  # 1. Smith AB...
        r'^\d+\.\s*[A-Z]{2,}[\.\s]',  # 1. WHO...
        r'^\[\d+\]',  # [1]
        r'\d{4}[;:]\d+[-â€“]\d+',  # å¹´ä»½;é ç¢¼
    ]

    for line in lines:
        line = line.strip()
        if len(line) < 5:
            continue
        total_lines += 1
        for pattern in citation_patterns:
            if re.search(pattern, line):
                citation_count += 1
                break

    # å¦‚æœè¶…é 60% çš„è¡Œæ˜¯å¼•ç”¨æ ¼å¼ï¼Œåˆ¤å®šç‚ºåƒè€ƒæ–‡ç»
    if total_lines > 5 and citation_count / total_lines > 0.6:
        return True, 0

    return False, None


def remove_references_from_content(text: str) -> str:
    """
    å¾æ–‡æœ¬ä¸­ç§»é™¤åƒè€ƒæ–‡ç»å€å¡Š

    Returns:
        str: ç§»é™¤åƒè€ƒæ–‡ç»å¾Œçš„æ–‡æœ¬
    """
    if not text:
        return text

    is_ref, start_pos = is_references_section(text)

    if is_ref and start_pos is not None:
        if start_pos == 0:
            # æ•´æ®µéƒ½æ˜¯åƒè€ƒæ–‡ç»ï¼Œè¿”å›ç©º
            return ""
        # åªä¿ç•™åƒè€ƒæ–‡ç»ä¹‹å‰çš„å…§å®¹
        return text[:start_pos].strip()

    return text


# ==================== æ ¸å¿ƒå‡½æ•¸ ====================

async def extract_clues_from_document(
    sub_question: str,
    document: Document,
    score: float,
    main_question: str = None,
    llm_model: BaseChatModel = None
) -> Dict[str, Any]:
    """
    å¾å–®å€‹æ–‡ç»ä¸­æå–é‡å°ä¸»å•é¡Œçš„æœ‰ç”¨ç·šç´¢

    Args:
        sub_question: å­å•é¡Œ(ç”¨æ–¼è¼”åŠ©æœå°‹)
        document: LangChain Document ç‰©ä»¶
        score: æª¢ç´¢ç›¸ä¼¼åº¦åˆ†æ•¸
        main_question: ç”¨æˆ¶çš„ä¸»å•é¡Œ(ç”¨æ–¼åˆ¤æ–·ç›¸é—œæ€§,é è¨­èˆ‡sub_questionç›¸åŒ)
        llm_model: ä½¿ç”¨çš„ LLM æ¨¡å‹ï¼ˆé è¨­ä½¿ç”¨ config.llmï¼‰

    Returns:
        {
            'document_name': str,
            'is_relevant': bool,
            'clues': List[str],
            'score': float,
            'metadata': dict,
            'original_content': str,
            'has_table': bool,        # ğŸ†• æ–°å¢
            'table_images': list      # ğŸ†• æ–°å¢
        }
    """
    if llm_model is None:
        llm_model = llm

    if main_question is None:
        main_question = sub_question

    metadata = document.metadata

    # ğŸ†• æå–è¡¨æ ¼è³‡è¨Šï¼ˆç„¡è«–ç·šç´¢æå–æ˜¯å¦æˆåŠŸéƒ½è¦ä¿ç•™ï¼‰
    has_table = metadata.get('has_table', False)
    table_images = metadata.get('table_images', [])
    
    # è™•ç† table_images å¯èƒ½æ˜¯å­—ä¸²çš„æƒ…æ³
    if isinstance(table_images, str):
        try:
            table_images = json.loads(table_images)
        except:
            table_images = []

    # CDC è³‡æ–™ç‰¹æ®Šè™•ç†
    if metadata.get('source_url') and metadata.get('data_source') == 'taiwan_cdc':
        document_name = metadata['source_url']
    else:
        document_name = (
            metadata.get('pdf_file') or
            metadata.get('source_file') or
            metadata.get('source') or
            metadata.get('disease_name') or
            'Unknown'
        )
        document_name = normalize_source_filename(document_name)
    
    original_full_content = metadata.get('original_full_text', metadata.get('original_text', document.page_content))

    # ğŸ†• é è™•ç†ï¼šç§»é™¤åƒè€ƒæ–‡ç»å€å¡Š
    content = remove_references_from_content(original_full_content)

    # å¦‚æœç§»é™¤åƒè€ƒæ–‡ç»å¾Œå…§å®¹ç‚ºç©ºï¼Œæ¨™è¨˜ç‚ºä¸ç›¸é—œ
    if not content or len(content.strip()) < 20:
        return {
            'document_name': document_name,
            'is_relevant': False,
            'clues': [],
            'score': score,
            'metadata': metadata,
            'original_content': "",  # ä¸ä¿ç•™åƒè€ƒæ–‡ç»å…§å®¹
            'has_table': has_table,
            'table_images': table_images
        }

    # åŒæ™‚æ›´æ–° original_full_contentï¼ˆç”¨æ–¼å¾ŒçºŒé¡¯ç¤ºï¼‰
    original_full_content = content

    max_content_length = CLUE_EXTRACTION_CONFIG['max_content_length']
    if len(content) > max_content_length:
        content = content[:max_content_length] + "\n...[å…§å®¹éé•·å·²æˆªæ–·]"

    try:
        prompt = CLUE_EXTRACTION_PROMPT.format(
            main_question=main_question,
            sub_question=sub_question,
            document_name=document_name,
            document_content=content
        )

        response = await llm_model.ainvoke(prompt)
        result_text = response.content.strip()

        if result_text.startswith('```'):
            result_text = result_text.split('```')[1]
            if result_text.startswith('json'):
                result_text = result_text[4:]

        result = json.loads(result_text)

        is_relevant = result.get('is_relevant', False)
        clues = result.get('clues', [])
        
        return {
            'document_name': document_name,
            'is_relevant': is_relevant,
            'clues': clues,
            'score': score,
            'metadata': metadata,
            'original_content': original_full_content,
            'has_table': has_table,          # ğŸ†• ä¿ç•™è¡¨æ ¼è³‡è¨Š
            'table_images': table_images     # ğŸ†• ä¿ç•™è¡¨æ ¼åœ–ç‰‡è·¯å¾‘
        }

    except Exception as e:
        print(f"âš ï¸ ç·šç´¢æå–å¤±æ•— ({document_name}): {e}")
        fallback_clue = content[:200] + "..." if len(content) > 200 else content
        return {
            'document_name': document_name,
            'is_relevant': False,
            'clues': [fallback_clue],
            'score': score,
            'metadata': metadata,
            'original_content': original_full_content,
            'has_table': has_table,          # ğŸ†• å³ä½¿å¤±æ•—ä¹Ÿä¿ç•™
            'table_images': table_images     # ğŸ†• å³ä½¿å¤±æ•—ä¹Ÿä¿ç•™
        }

async def extract_clues_from_multiple_documents(
    sub_question: str,
    documents_with_scores: List[Tuple[Document, float]],
    main_question: str = None,
    llm_model: BaseChatModel = None
) -> List[Dict[str, Any]]:
    """
    å¾å¤šå€‹æ–‡ç»ä¸­ä¸¦è¡Œæå–ç·šç´¢

    ä¿®æ”¹ï¼šåªä¿ç•™ç›¸é—œçš„æ–‡ç»ï¼Œä¸å› ç‚ºæœ‰è¡¨æ ¼å°±ä¿ç•™ä¸ç›¸é—œæ–‡ç»
    """

    tasks = [
        extract_clues_from_document(sub_question, doc, score, main_question, llm_model)
        for doc, score in documents_with_scores
    ]

    all_results = await asyncio.gather(*tasks)

    # åªä¿ç•™ç›¸é—œçš„æ–‡ç»
    relevant_results = [
        r for r in all_results
        if r['is_relevant']
    ]

    return relevant_results

def format_clues_to_markdown(clue_results: List[Dict[str, Any]]) -> Tuple[str, List[str], Dict[str, Any]]:
    """
    å°‡æå–çš„ç·šç´¢æ ¼å¼åŒ–ç‚º Markdownï¼Œä½†ã€åƒè€ƒä¾æ“šã€‘é¡¯ç¤ºåŸå§‹æ–‡ç»å…§å®¹ï¼ˆè€Œéç·šç´¢ï¼‰

    Returns:
        Tuple[str, List[str], Dict[str, Any]]:
            - formatted_text: æ ¼å¼åŒ–çš„ Markdown æ–‡æœ¬
            - sources: ä¾†æºæ–‡ä»¶åˆ—è¡¨
            - docs_dict: {æ–‡ä»¶å: {'content': str, 'score': float, 'has_table': bool, 'table_images': list}}
    """
    if not clue_results:
        return "", [], {}

    sources = list(set(r['document_name'] for r in clue_results))
    docs_dict: Dict[str, Dict[str, Any]] = {}

    for result in clue_results:
        doc_name = result['document_name']
        original_content = result.get('original_content', '').strip()
        score = result.get('score', 999.0)
        
        # ğŸ†• å„ªå…ˆå¾ result ç›´æ¥å–å¾—è¡¨æ ¼è³‡è¨Šï¼ˆæ›´å¯é ï¼‰
        has_table = result.get('has_table', False)
        table_images = result.get('table_images', [])
        
        # ğŸ†• å¦‚æœ result æ²’æœ‰ï¼Œå†å¾ metadata å–å¾—ï¼ˆå‚™ç”¨ï¼‰
        if not has_table and not table_images:
            metadata = result.get('metadata', {})
            has_table = metadata.get('has_table', False)
            table_images = metadata.get('table_images', [])

        if not original_content:
            continue

        if doc_name not in docs_dict:
            docs_dict[doc_name] = {
                'content': original_content,
                'score': score,
                'has_table': has_table,
                'table_images': table_images
            }
        else:
            # åˆä½µå…§å®¹
            existing_content = docs_dict[doc_name]['content']
            if original_content not in existing_content:
                docs_dict[doc_name]['content'] = existing_content + "\n" + original_content
            
            # ä¿ç•™æœ€å°åˆ†æ•¸
            docs_dict[doc_name]['score'] = min(docs_dict[doc_name]['score'], score)

            # ğŸ†• åˆä½µè¡¨æ ¼åœ–ç‰‡ï¼ˆå»é‡ï¼‰
            if table_images:
                existing_images = set(docs_dict[doc_name].get('table_images', []))
                new_images = set(table_images)
                docs_dict[doc_name]['table_images'] = list(existing_images | new_images)

            # ğŸ†• æ›´æ–° has_table
            if has_table:
                docs_dict[doc_name]['has_table'] = True

    # æ ¼å¼åŒ–ç‚º Markdown
    formatted_parts = ["**åƒè€ƒä¾æ“š**"]
    for doc_name, doc_info in docs_dict.items():
        formatted_parts.append(f"ã€Š{doc_name}ã€‹")
        formatted_parts.append(f"{doc_info['content']}")
        formatted_parts.append("")

    formatted_text = "\n".join(formatted_parts).strip()
    return formatted_text, sources, docs_dict



def convert_html_tables_to_markdown(content: str) -> str:
    """
    å°‡ HTML è¡¨æ ¼è½‰æ›ç‚º Markdown æ ¼å¼ï¼Œå¤§å¹…æ¸›å°‘ token æ•¸
    """
    def html_table_to_markdown(table_html: str) -> str:
        try:
            soup = BeautifulSoup(table_html, 'html.parser')
            rows = soup.find_all('tr')
            if not rows:
                return table_html
            
            markdown_rows = []
            for i, row in enumerate(rows):
                cells = row.find_all(['td', 'th'])
                cell_texts = [cell.get_text(strip=True).replace('|', 'ï½œ') for cell in cells]
                markdown_rows.append('| ' + ' | '.join(cell_texts) + ' |')
                
                # åœ¨ç¬¬ä¸€è¡Œå¾ŒåŠ å…¥åˆ†éš”ç·š
                if i == 0:
                    markdown_rows.append('|' + '---|' * len(cells))
            
            return '\n'.join(markdown_rows)
        except Exception as e:
            print(f"âš ï¸ è¡¨æ ¼è½‰æ›å¤±æ•—: {e}")
            return table_html
    
    # æ‰¾å‡ºæ‰€æœ‰ <table>...</table>
    table_pattern = re.compile(r'<table[^>]*>.*?</table>', re.DOTALL | re.IGNORECASE)
    
    def replacer(match):
        return html_table_to_markdown(match.group(0))
    
    return table_pattern.sub(replacer, content)

async def summarize_clues_if_needed(
    question: str,
    formatted_clues: str,
    max_length: int = None,
    llm_model: BaseChatModel = None
) -> str:
    """
    å¦‚æœç·šç´¢å…§å®¹éé•·ï¼Œä½¿ç”¨ LLM é€²è¡Œæ‘˜è¦
    """
    
    # ğŸ†• é è™•ç†ï¼šè½‰æ› HTML è¡¨æ ¼
    original_length = len(formatted_clues)
    formatted_clues = convert_html_tables_to_markdown(formatted_clues)  # æˆ–ç”¨ to_text
    if len(formatted_clues) < original_length:
        print(f"ğŸ“Š è¡¨æ ¼è½‰æ›: {original_length} â†’ {len(formatted_clues)} å­—å…ƒ (æ¸›å°‘ {original_length - len(formatted_clues)})")
    
    # å¾é…ç½®è®€å–æœ€å¤§é•·åº¦
    if max_length is None:
        from core.config import RETRIEVAL_CONFIG
        max_length = RETRIEVAL_CONFIG.get('clue_max_length', 3000)

    if len(formatted_clues) <= max_length:
        return formatted_clues

    print(f"âš ï¸ ç·šç´¢å…§å®¹éé•· ({len(formatted_clues)} å­—å…ƒ)ï¼Œé€²è¡Œæ‘˜è¦...")

    if llm_model is None:
        llm_model = llm

    try:
        prompt = CLUE_SUMMARY_PROMPT.format(
            question=question,
            all_clues=formatted_clues
        )
        # ğŸ†• åŠ å…¥è¶…æ™‚æ§åˆ¶ï¼ˆ60 ç§’ï¼‰
        response = await asyncio.wait_for(
            llm_model.ainvoke(prompt),
            timeout=300
        )

        summarized = response.content.strip()
        print(f"âœ… æ‘˜è¦å®Œæˆ - æ‘˜è¦å¾Œé•·åº¦: {len(summarized)} å­—å…ƒ")
        return summarized

    except asyncio.TimeoutError:
        print(f"âš ï¸ æ‘˜è¦è¶…æ™‚ï¼ˆ60ç§’ï¼‰ï¼Œä½¿ç”¨æˆªæ–·")
        return formatted_clues[:max_length] + "\n\n[... å…§å®¹éé•·å·²æˆªæ–· ...]"
    except Exception as e:
        print(f"âš ï¸ æ‘˜è¦å¤±æ•—: {e}ï¼Œä½¿ç”¨æˆªæ–·")
        return formatted_clues[:max_length] + "\n\n[... å…§å®¹éé•·å·²æˆªæ–· ...]"


# ==================== InMemoryStore å¿«å– ====================

def generate_cache_key(question: str, document_ids: List[str]) -> str:
    """
    ç”Ÿæˆå¿«å–éµå€¼ï¼ˆåŸºæ–¼å•é¡Œå’Œæ–‡ç»IDçš„é›œæ¹Šï¼‰

    Args:
        question: å•é¡Œæ–‡æœ¬
        document_ids: æ–‡ç»IDåˆ—è¡¨ï¼ˆæ’åºå¾Œï¼‰

    Returns:
        å¿«å–éµå€¼ï¼ˆSHA256 é›œæ¹Šï¼‰
    """
    sorted_ids = sorted(document_ids)
    content = f"{question}||{'|'.join(sorted_ids)}"
    return hashlib.sha256(content.encode()).hexdigest()


async def get_cached_clues(
    store: BaseStore,
    namespace: Tuple[str, str],
    cache_key: str
) -> Dict[str, Any] | None:
    """
    å¾ InMemoryStore è®€å–å¿«å–çš„ç·šç´¢

    Args:
        store: LangGraph BaseStore å¯¦ä¾‹
        namespace: å‘½åç©ºé–“ï¼ˆå¦‚ ("clue_cache", "user_123")ï¼‰
        cache_key: å¿«å–éµå€¼

    Returns:
        å¿«å–çš„è³‡æ–™æˆ– None
    """
    try:
        # InMemoryStore.aget() è¿”å› Item ç‰©ä»¶ï¼ˆè€Œéåˆ—è¡¨ï¼‰
        item = await store.aget(namespace, cache_key)
        if item:
            return item.value
        return None
    except Exception as e:
        print(f"âš ï¸ è®€å–å¿«å–å¤±æ•—: {e}")
        return None


async def save_cached_clues(
    store: BaseStore,
    namespace: Tuple[str, str],
    cache_key: str,
    data: Dict[str, Any]
) -> None:
    """
    å°‡ç·šç´¢ä¿å­˜åˆ° InMemoryStore

    Args:
        store: LangGraph BaseStore å¯¦ä¾‹
        namespace: å‘½åç©ºé–“
        cache_key: å¿«å–éµå€¼
        data: è¦å¿«å–çš„è³‡æ–™
    """
    try:
        await store.aput(namespace, cache_key, data)
        #print(f"ğŸ’¾ å·²å¿«å–ç·šç´¢ (key: {cache_key[:16]}...)")
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜å¿«å–å¤±æ•—: {e}")


# ==================== æ•´åˆå‡½æ•¸ ====================

async def process_retrieval_with_clue_extraction(
    sub_question: str,
    documents_with_scores: List[Tuple[Document, float]],
    main_question: str = None,
    store: BaseStore = None,
    user_id: str = "default",
    llm_model: BaseChatModel = None
) -> Tuple[str, List[str], Dict[str, str]]:
    """
    å®Œæ•´çš„æª¢ç´¢è™•ç†æµç¨‹ï¼ˆå¸¶ç·šç´¢æå–ï¼‰

    å·¥ä½œæµç¨‹ï¼š
    1. æª¢æŸ¥å¿«å–
    2. æå–ç·šç´¢ï¼ˆå¦‚æœæ²’æœ‰å¿«å–ï¼‰
    3. æ ¼å¼åŒ–
    4. æ‘˜è¦ï¼ˆå¦‚æœéœ€è¦ï¼‰
    5. å¿«å–çµæœ

    Args:
        sub_question: å­å•é¡Œ(ç”¨æ–¼è¼”åŠ©æœå°‹)
        documents_with_scores: æª¢ç´¢åˆ°çš„æ–‡ç»åˆ—è¡¨
        main_question: ç”¨æˆ¶çš„ä¸»å•é¡Œ(ç”¨æ–¼åˆ¤æ–·ç›¸é—œæ€§,é è¨­èˆ‡sub_questionç›¸åŒ)
        store: LangGraph BaseStoreï¼ˆå¯é¸ï¼Œç”¨æ–¼å¿«å–ï¼‰
        user_id: ç”¨æˆ¶IDï¼ˆç”¨æ–¼å‘½åç©ºé–“ï¼‰
        llm_model: ä½¿ç”¨çš„ LLM æ¨¡å‹

    Returns:
        (formatted_knowledge, list_of_sources, dict_of_used_sources_with_original_content)
        - formatted_knowledge: æ ¼å¼åŒ–çš„æª¢ç´¢çµæœï¼ˆç·šç´¢æ‘˜è¦ï¼‰
        - list_of_sources: ä¾†æºæ–‡ä»¶åˆ—è¡¨
        - dict_of_used_sources_with_original_content: {æ–‡ä»¶å: åŸå§‹å®Œæ•´å…§å®¹}
    """
    if not documents_with_scores:
        return "", [], {}

    # å¦‚æœæ²’æœ‰æä¾›ä¸»å•é¡Œ,å‰‡ä½¿ç”¨å­å•é¡Œä½œç‚ºä¸»å•é¡Œ
    if main_question is None:
        main_question = sub_question

    # ç”Ÿæˆå¿«å–éµå€¼(åŒ…å«ä¸»å•é¡Œå’Œå­å•é¡Œ)
    doc_ids = [
        f"{doc.metadata.get('pdf_file', doc.metadata.get('source', 'unknown'))}_p{doc.metadata.get('page', 0)}"
        for doc, _ in documents_with_scores
    ]
    cache_key = generate_cache_key(f"{main_question}|{sub_question}", doc_ids)
    
    # å˜—è©¦è®€å–å¿«å–
    cached_data = None
    if store:
        namespace = ("clue_cache", user_id)
        cached_data = await get_cached_clues(store, namespace, cache_key)

        if cached_data:
            # print(f"âœ… ä½¿ç”¨å¿«å–çš„ç·šç´¢ (key: {cache_key[:16]}...)")
            return cached_data['knowledge'], cached_data['sources'], cached_data.get('used_sources_dict', {})

    # ç„¡å¿«å–ï¼Œé€²è¡Œæå–
    # print(f"ğŸ” é–‹å§‹æå–ç·šç´¢ï¼ˆä¸»å•é¡Œï¼š{main_question[:40]}...ï¼Œå­å•é¡Œï¼š{sub_question[:40]}...ï¼‰")

    # 1. æå–ç·šç´¢
    clue_results = await extract_clues_from_multiple_documents(
        sub_question, documents_with_scores, main_question, llm_model
    )
    if not clue_results:
        print("âš ï¸ æ²’æœ‰æ‰¾åˆ°ç›¸é—œç·šç´¢")
        return "", [], {}

    # print(f"âœ… æå–åˆ° {len(clue_results)} å€‹ç›¸é—œæ–‡ç»çš„ç·šç´¢")

    # 2. æ ¼å¼åŒ–ï¼ˆç¾åœ¨è¿”å›ä¸‰å€‹å€¼ï¼‰
    formatted_knowledge, sources, used_sources_dict = format_clues_to_markdown(clue_results)
    # 3. æ‘˜è¦ï¼ˆå¦‚æœéœ€è¦ï¼‰
    formatted_knowledge = await summarize_clues_if_needed(
        sub_question, formatted_knowledge, max_length=4096, llm_model=llm_model
    )
    # print(f"âœ… æœ€çµ‚æ ¼å¼åŒ–çŸ¥è­˜é•·åº¦: {len(formatted_knowledge)} å­—å…ƒ")  # ğŸ”§ ä¿®å¾©ï¼šæ‡‰è©²æ˜¯ len()
    # 4. å¿«å–çµæœ
    if store:
        cache_data = {
            'knowledge': formatted_knowledge,
            'sources': sources,
            'used_sources_dict': used_sources_dict,  # æ–°å¢ï¼šä¿å­˜ä½¿ç”¨çš„åƒè€ƒæ–‡ç»å­—å…¸
            'timestamp': None  # å¯ä»¥åŠ ä¸Šæ™‚é–“æˆ³
        }
        await save_cached_clues(store, namespace, cache_key, cache_data)

    for doc_name, doc_info in used_sources_dict.items():
        # doc_info æ˜¯ {'content': str, 'score': float} æ ¼å¼
        content_text = doc_info['content'] if isinstance(doc_info, dict) else doc_info
        print(f"      ğŸ“„ {doc_name}: {len(content_text)} å­—å…ƒ")

    return formatted_knowledge, sources, used_sources_dict
