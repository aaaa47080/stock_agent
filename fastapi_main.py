"""FastAPI å¾Œç«¯ - æ•´åˆ Langfuse è¿½è¹¤"""

from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse, FileResponse, HTMLResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Optional
import asyncio
import json
import opencc
from datetime import datetime
from langchain_core.messages import HumanMessage, AIMessage

from core.dataclass import MedicalResponse

from langgraph.checkpoint.memory import InMemorySaver
from langgraph.graph import StateGraph, START, END
from core.config import llm, WORKFLOW_LIMITS, SHORT_TERM_MEMORY_CONFIG, MESSAGE_SUMMARIZATION_CONFIG, LONG_TERM_MEMORY_CONFIG, RETRIEVAL_CONFIG, TOOLS_CONFIG
from core.prompt_config import MEMORY_DEDUCTION_PROMPT

# åˆå§‹åŒ– OpenCC è½‰æ›å™¨ (ç°¡é«” -> ç¹é«”)
converter = opencc.OpenCC('s2tw.json')

# å°å…¥ Langfuse è¿½è¹¤
from monitoring.langfuse_integration import get_langfuse_config, update_trace_io

# å°å…¥ langmem çš„ SummarizationNode
from langmem.short_term import SummarizationNode
from langchain_core.messages.utils import count_tokens_approximately
from graph.graph_routing import (
    FullState,
    route_after_memory,
    route_after_classification,
    route_after_generate,
    route_after_check_question_type,
    route_after_validation
)
from graph.graph_nodes import (
    extract_current_query,
    retrieve_memory,
    answer_from_memory,
    classify_query_type,
    planning_node,  # æ–°å¢ï¼šPlanning ç¯€é»
    retrieve_knowledge,
    supplement_with_realtime_info,  # å³æ™‚æœå°‹è£œå……ç¯€é»ï¼ˆç”±ä½¿ç”¨è€…é¸æ“‡æ˜¯å¦å•Ÿç”¨ï¼‰
    generate_response,
    validate_answer,
    refine_answer,
    check_question_type,
    set_out_of_scope_message,
    memory_client
)

# ===== åˆå§‹åŒ–å·¥å…·ç³»çµ± =====
from tools_init import initialize_all_tools, get_active_tools

app = FastAPI(title="é†«ç™‚è«®è©¢ç³»çµ± API")
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # å…è¨±æ‰€æœ‰ä¾†æº,æ–¹ä¾¿åŒç¶²æ®µç”¨æˆ¶è¨ªå•
    allow_credentials=False,  # è¨­ç‚º False ä»¥æ”¯æ´ file:// å”è­°
    allow_methods=["*"],
    allow_headers=["*"],
)

checkpointer = InMemorySaver()

# ===== åˆå§‹åŒ–å·¥å…·ç³»çµ± =====
# æ ¹æ“šé…ç½®åˆå§‹åŒ–å·¥å…·ç³»çµ±
active_tools = []
if TOOLS_CONFIG.get('enabled', True):
    try:
        print("\n" + "="*60)
        print("ğŸ”§ åˆå§‹åŒ–å·¥å…·ç³»çµ±...")
        print("="*60)
        initialize_all_tools()
        active_tools = get_active_tools()
        if active_tools:
            print(f"\nâœ… å·¥å…·ç³»çµ±å·²å•Ÿç”¨ï¼Œè¼‰å…¥ {len(active_tools)} å€‹å·¥å…·")
        else:
            print(f"\nâš ï¸ å·¥å…·ç³»çµ±å·²å•Ÿç”¨ï¼Œä½†æœªè¼‰å…¥ä»»ä½•å·¥å…·")
        print("="*60 + "\n")
    except Exception as e:
        print(f"\nâš ï¸ å·¥å…·ç³»çµ±åˆå§‹åŒ–å¤±æ•—: {e}")
        print("ç³»çµ±å°‡ç¹¼çºŒé‹è¡Œï¼Œä½†å·¥å…·åŠŸèƒ½å°‡ä¸å¯ç”¨\n")
        active_tools = []
else:
    print("\nâ­ï¸ å·¥å…·ç³»çµ±å·²åœç”¨ï¼ˆå¯åœ¨ config.py çš„ TOOLS_CONFIG ä¸­å•Ÿç”¨ï¼‰\n")

# ===== é…ç½® SummarizationNodeï¼ˆå¦‚æœå•Ÿç”¨ï¼‰=====
summarization_node = None
if MESSAGE_SUMMARIZATION_CONFIG.get('enabled', True):
    print("âœ… å•Ÿç”¨ Message Summarization")
    # å‰µå»ºç”¨æ–¼æ‘˜è¦çš„ LLMï¼ˆç¶å®š max_tokens ä»¥æ§åˆ¶æ‘˜è¦é•·åº¦ï¼‰
    summarization_model = llm.bind(max_tokens=MESSAGE_SUMMARIZATION_CONFIG['max_summary_tokens'])

    # å‰µå»º SummarizationNode
    summarization_node = SummarizationNode(
        token_counter=count_tokens_approximately,
        model=summarization_model,
        max_tokens=MESSAGE_SUMMARIZATION_CONFIG['max_tokens'],
        max_tokens_before_summary=MESSAGE_SUMMARIZATION_CONFIG['max_tokens_before_summary'],
        max_summary_tokens=MESSAGE_SUMMARIZATION_CONFIG['max_summary_tokens'],
        input_messages_key="messages",
        output_messages_key="messages",  # ğŸ”‘ é—œéµï¼šè¨­ç‚º "messages" ä»¥æ›¿æ›åŸæ¶ˆæ¯åˆ—è¡¨
    )
    print(f"ğŸ“Š æ‘˜è¦é…ç½®: max_tokens={MESSAGE_SUMMARIZATION_CONFIG['max_tokens']}, "
          f"è§¸ç™¼é–¾å€¼={MESSAGE_SUMMARIZATION_CONFIG['max_tokens_before_summary']}")
else:
    print("â­ï¸ Message Summarization å·²åœç”¨")

graph = StateGraph(FullState)

graph.add_node("extract_current_query", extract_current_query)
graph.add_node("retrieve_memory", retrieve_memory)
graph.add_node("answer_from_memory", answer_from_memory)
graph.add_node("classify_query_type", classify_query_type)
graph.add_node("planning", planning_node)  # ğŸ†• Planning ç¯€é»
graph.add_node("retrieve_knowledge", retrieve_knowledge)
graph.add_node("supplement_realtime", supplement_with_realtime_info)  # å³æ™‚æœå°‹è£œå……ï¼ˆç”±ä½¿ç”¨è€…é€é enabled_tool_ids æ§åˆ¶ï¼‰
graph.add_node("generate_response", generate_response)
graph.add_node("validate_answer", validate_answer)
graph.add_node("refine_answer", refine_answer)
graph.add_node("check_question_type", check_question_type)
graph.add_node("set_out_of_scope_message", set_out_of_scope_message)

# å¦‚æœå•Ÿç”¨ summarizationï¼Œæ·»åŠ  summarization ç¯€é»
if summarization_node:
    graph.add_node("summarize", summarization_node)
    print("âœ… å·²æ·»åŠ  'summarize' ç¯€é»åˆ° Graph")

# å®šç¾©é‚Š
# å¦‚æœå•Ÿç”¨ summarizationï¼Œåœ¨é€²å…¥ä¸»æµç¨‹å‰å…ˆç¶“éæ‘˜è¦ç¯€é»
if summarization_node:
    graph.add_edge(START, "summarize")
    graph.add_edge("summarize", "extract_current_query")
    print("âœ… æµç¨‹: START â†’ summarize â†’ extract_current_query")
else:
    graph.add_edge(START, "extract_current_query")
    print("âœ… æµç¨‹: START â†’ extract_current_query")
graph.add_edge("extract_current_query", "retrieve_memory")
graph.add_edge("retrieve_memory", "answer_from_memory")

# æ¢ä»¶è·¯ç”±
graph.add_conditional_edges(
    "answer_from_memory",
    route_after_memory,
    {
        "classify_query_type": "classify_query_type",
        "set_out_of_scope_message": "set_out_of_scope_message"
    }
)
graph.add_conditional_edges(
    "classify_query_type",
    route_after_classification,
    {
        "retrieve_knowledge": "planning",  # ğŸ†• æ”¹ç‚ºå…ˆé€²å…¥ Planning
        "generate_response": "generate_response",
        "set_out_of_scope_message": "set_out_of_scope_message"
    }
)
# ğŸ†• Planning å¾Œç›´æ¥é€²å…¥æª¢ç´¢
graph.add_edge("planning", "retrieve_knowledge")
# å·¥ä½œæµç¨‹ï¼šretrieve_knowledge â†’ supplement_realtime â†’ generate_response
# supplement_realtime ç¯€é»æœƒæ ¹æ“šä½¿ç”¨è€…é¸æ“‡çš„ enabled_tool_ids æ±ºå®šæ˜¯å¦ä½¿ç”¨å¤–éƒ¨æœå°‹
graph.add_edge("retrieve_knowledge", "supplement_realtime")
graph.add_edge("supplement_realtime", "generate_response")
graph.add_conditional_edges(
    "generate_response",
    route_after_generate,
    {
        "check_question_type": "check_question_type",
        "set_out_of_scope_message": "set_out_of_scope_message",
        "END": END
    }
)
graph.add_conditional_edges(
    "check_question_type",
    route_after_check_question_type,
    {
        "validate_answer": "validate_answer",
        "set_out_of_scope_message": "set_out_of_scope_message",
        "END": END
    }
)
graph.add_conditional_edges(
    "validate_answer",
    route_after_validation,
    {
        "refine_answer": "refine_answer",
        "retrieve_knowledge": "retrieve_knowledge",
        "set_out_of_scope_message": "set_out_of_scope_message",
        "END": END
    }
)
graph.add_edge("refine_answer", "validate_answer")
graph.add_edge("set_out_of_scope_message", END)

langgraph_app = graph.compile(checkpointer=checkpointer)

class ChatRequest(BaseModel):
    """
    èŠå¤©è«‹æ±‚åƒæ•¸

    å¿…å¡«åƒæ•¸ï¼š
    - user_id: ç”¨æˆ¶å”¯ä¸€è­˜åˆ¥ç¢¼ï¼ˆç”¨æ–¼éš”é›¢ä¸åŒç”¨æˆ¶çš„å¿«å–å’Œè¨˜æ†¶ï¼‰
    - message: ç”¨æˆ¶å•é¡Œ

    å¯é¸åƒæ•¸ï¼š
    - session_id: å°è©±æœƒè©± IDï¼ˆç”¨æ–¼è¿½è¹¤åŒä¸€å°è©±çš„å¤šè¼ªå•ç­”ï¼‰
    - enable_short_term_memory: æ˜¯å¦å•Ÿç”¨çŸ­æœŸè¨˜æ†¶ï¼ˆå°è©±æ­·å²ï¼‰
    - enable_long_term_memory: æ˜¯å¦å•Ÿç”¨é•·æœŸè¨˜æ†¶ï¼ˆmem0 å€‹äººç—…å²ï¼‰
    - datasource_ids: æŒ‡å®šè¦ä½¿ç”¨çš„è³‡æ–™æºï¼ˆçŸ¥è­˜åº«ï¼‰åˆ—è¡¨
    - enabled_tool_ids: æŒ‡å®šè¦ä½¿ç”¨çš„å¤–éƒ¨å·¥å…·ï¼ˆå¦‚ CDC å³æ™‚æœå°‹ï¼‰åˆ—è¡¨
    """
    user_id: str
    message: str
    session_id: Optional[str] = "default_session"

    # è¨˜æ†¶æ§åˆ¶åƒæ•¸
    enable_short_term_memory: Optional[bool] = False  # çŸ­æœŸè¨˜æ†¶ï¼ˆå°è©±æ­·å²ï¼‰
    enable_long_term_memory: Optional[bool] = False  # é•·æœŸè¨˜æ†¶ï¼ˆå€‹äººç—…å²ï¼‰

    # è³‡æ–™æºæ§åˆ¶åƒæ•¸ï¼ˆå¯ç”¨é¸é …è¦‹ GET /api/configï¼‰
    datasource_ids: Optional[list[str]] = None  # None = ä½¿ç”¨ç³»çµ±é è¨­

    # å·¥å…·æ§åˆ¶åƒæ•¸ï¼ˆå¯ç”¨é¸é …è¦‹ GET /api/configï¼‰
    enabled_tool_ids: Optional[list[str]] = None  # None = ä½¿ç”¨ç³»çµ±é è¨­

def create_sse_message(event_type: str, content: str) -> str:
    data = {"type": event_type, "content": content}
    return f"data: {json.dumps(data, ensure_ascii=False)}\n\n"

conversation_store = {}

@app.get("/")
async def root():
    return {"status": "ok", "timestamp": datetime.now().isoformat()}

@app.get("/test", response_class=HTMLResponse)
async def test_page():
    """æä¾›æ¸¬è©¦é é¢"""
    import os
    html_path = os.path.join(os.path.dirname(__file__), "test_chat.html")
    with open(html_path, "r", encoding="utf-8") as f:
        return f.read()

@app.get("/api/config")
async def get_api_config():
    """
    ç²å– API å¯ç”¨çš„é…ç½®é¸é …

    è¿”å›ï¼š
    - datasources: å¯ç”¨çš„è³‡æ–™æºï¼ˆçŸ¥è­˜åº«ï¼‰åˆ—è¡¨
    - tools: å¯ç”¨çš„å¤–éƒ¨å·¥å…·åˆ—è¡¨
    - memory_options: è¨˜æ†¶ç³»çµ±é…ç½®é¸é …
    - default_settings: ç³»çµ±é è¨­è¨­å®š
    """
    from core.datasource_config import get_registry
    from tools_config import get_all_tools

    # ç²å–æ‰€æœ‰å¯ç”¨çš„è³‡æ–™æº
    registry = get_registry()
    all_datasources = registry.get_all()
    enabled_datasources = registry.get_enabled()

    datasources_info = [
        {
            "id": ds.id,
            "name": ds.name,
            "description": f"{ds.name} - {ds.source_type.upper()} æ ¼å¼",
            "enabled": ds.enabled,
            "default_k": ds.default_k,
            "support_medical": ds.support_medical,
            "support_procedure": ds.support_procedure,
            "metadata": ds.metadata
        }
        for ds in all_datasources
    ]

    # ç²å–æ‰€æœ‰å¯ç”¨çš„å·¥å…·
    all_tools = get_all_tools()  # è¿”å› dict
    tools_info = [
        {
            "id": tool_config.id,
            "name": tool_config.name,
            "description": tool_config.description,
            "enabled": tool_config.enabled,
            "support_medical": tool_config.support_medical,
            "support_general": tool_config.support_general,
            "timeout": tool_config.timeout,
            "metadata": tool_config.metadata
        }
        for tool_config in all_tools.values()  # è¿­ä»£ dict çš„ values
    ]

    return {
        "datasources": {
            "available": datasources_info,
            "enabled_ids": [ds.id for ds in enabled_datasources],
            "default_ids": RETRIEVAL_CONFIG.get('default_datasource_ids'),
            "description": "å¯ç”¨çš„çŸ¥è­˜åº«è³‡æ–™æºï¼Œå¯åœ¨è«‹æ±‚ä¸­é€é datasource_ids åƒæ•¸æŒ‡å®š"
        },
        "tools": {
            "available": tools_info,
            "enabled_ids": [tool_config.id for tool_config in all_tools.values() if tool_config.enabled],
            "default_ids": TOOLS_CONFIG.get('default_tools'),
            "description": "å¯ç”¨çš„å¤–éƒ¨å·¥å…·ï¼ˆå¦‚å³æ™‚æœå°‹ï¼‰ï¼Œå¯åœ¨è«‹æ±‚ä¸­é€é enabled_tool_ids åƒæ•¸æŒ‡å®š"
        },
        "memory_options": {
            "short_term_memory": {
                "description": "çŸ­æœŸè¨˜æ†¶ï¼ˆå°è©±æ­·å²ï¼‰ï¼Œä¿ç•™ç•¶å‰æœƒè©±çš„å•ç­”è¨˜éŒ„",
                "default": SHORT_TERM_MEMORY_CONFIG.get('enabled', True),
                "privacy": "ğŸ”’ éš”é›¢ï¼šæ¯å€‹ session_id ç¨ç«‹ï¼Œä¸è·¨æœƒè©±å…±äº«"
            },
            "long_term_memory": {
                "description": "é•·æœŸè¨˜æ†¶ï¼ˆå€‹äººç—…å²ï¼‰ï¼Œè¨˜éŒ„ç”¨æˆ¶çš„å¥åº·è³‡è¨Šï¼ˆå¦‚éæ•å²ã€ç—…å²ï¼‰",
                "default": LONG_TERM_MEMORY_CONFIG.get('enabled', False),
                "privacy": "ğŸ”’ éš”é›¢ï¼šæ¯å€‹ user_id ç¨ç«‹ï¼Œä¸è·¨ç”¨æˆ¶å…±äº«",
                "note": "âš ï¸ ç›®å‰é è¨­åœç”¨ï¼Œå¦‚éœ€ä½¿ç”¨è«‹è¯ç¹«ç®¡ç†å“¡"
            }
        },
        "privacy_protection": {
            "cache_strategy": {
                "query_cache": "å®Œå…¨éš”é›¢ï¼ˆåŒ…å« user_idï¼‰",
                "planning_cache": "å®Œå…¨éš”é›¢ï¼ˆåŒ…å« user_idï¼‰",
                "retrieval_cache": "ä¸»å•é¡Œä¸å¿«å–ï¼Œå­å•é¡Œå¯è·¨ç”¨æˆ¶å…±äº«ï¼ˆåƒ…å¿«å–å…¬é–‹é†«ç™‚çŸ¥è­˜ï¼‰"
            },
            "description": "ç³»çµ±å·²å¯¦æ–½ä¸‰å±¤å¿«å–éš±ç§ä¿è­·æ©Ÿåˆ¶ï¼Œç¢ºä¿ç”¨æˆ¶å€‹äººä¿¡æ¯ä¸æœƒæ³„éœ²"
        },
        "default_settings": {
            "enable_short_term_memory": True,
            "enable_long_term_memory": False,
            "datasource_ids": RETRIEVAL_CONFIG.get('default_datasource_ids'),
            "enabled_tool_ids": TOOLS_CONFIG.get('default_tools')
        }
    }

@app.post("/chat/stream")
async def chat_stream(request: ChatRequest):
    """ä¸²æµå¼èŠå¤©æ¥å£"""

    async def event_generator():
        try:
            # æº–å‚™å°è©±æ­·å²
            session_id = request.session_id
            if session_id not in conversation_store:
                conversation_store[session_id] = []

            # ç²å– Langfuse é…ç½®

            langfuse_cfg, langfuse_handler = get_langfuse_config(
                user_id=request.user_id,
                session_id=session_id,  # ä½¿ç”¨å°è©±çš„ session_id
                tags=["fastapi", "stream", "medical", "rag"],
                metadata={
                    "query": request.message,
                    "source": "fastapi_stream",
                    "short_term_memory_enabled": request.enable_short_term_memory,
                    "long_term_memory_enabled": request.enable_long_term_memory,
                    "datasource_ids": request.datasource_ids,
                    "enabled_tool_ids": request.enabled_tool_ids
                }
            )

            # æ§‹å»º messages åˆ—è¡¨

            messages = []

            # æª¢æŸ¥æ˜¯å¦å•Ÿç”¨çŸ­æœŸè¨˜æ†¶

            use_short_term_memory = request.enable_short_term_memory if request.enable_short_term_memory is not None else SHORT_TERM_MEMORY_CONFIG.get('enabled', True)
            if use_short_term_memory:
                for user_q, asst_a in conversation_store[session_id]:
                    messages.append(HumanMessage(content=user_q))

                    messages.append(AIMessage(content=asst_a))

            # æ·»åŠ ç•¶å‰ç”¨æˆ¶å•é¡Œ

            messages.append(HumanMessage(content=request.message))

            # æ§‹å»º state

            initial_state = {
                "messages": messages,
                "user_id": request.user_id,
                "query_type": "",
                "knowledge": "",
                "current_query": "",
                "memory_summary": "",
                "memory_response": "",
                "memory_source": "",
                "final_answer": "",
                "validation_result": "",
                "validation_feedback": "",
                "retry_count": 0,
                "query_type_history": [],
                "knowledge_retrieval_count": 0,
                "validation_need_supplement_info": "",
                "question_category": "",
                "question_type_reasoning": "",
                "suggested_sub_questions": [],
                "iteration_count": 0,
                "used_sources_dict": {},
                "original_docs_dict": {},
                "matched_table_images": [],
                "matched_educational_images": [],  # ğŸ†• è¡›æ•™åœ–ç‰‡
                "context": {},
                "planning_result": None,  # ğŸ†• Planning çµæœ
                "retrieval_steps": [],  # ğŸ†• æª¢ç´¢æ­¥é©Ÿ
                "current_step": 0,  # ğŸ†• ç•¶å‰æ­¥é©Ÿ
                "enable_short_term_memory": use_short_term_memory,
                "enable_long_term_memory": request.enable_long_term_memory,
                "datasource_ids": request.datasource_ids if request.datasource_ids is not None else RETRIEVAL_CONFIG.get('default_datasource_ids'),
                "enabled_tool_ids": request.enabled_tool_ids if request.enabled_tool_ids is not None else TOOLS_CONFIG.get('default_tools')
            }

            # é‹è¡Œ graph
            # ä½¿ç”¨ user_id + session_id çµ„åˆç¢ºä¿ä¸åŒç”¨æˆ¶çš„æœƒè©±å®Œå…¨éš”é›¢
            thread_id = f"{request.user_id}_{session_id}"
            config_with_limit = {
                "configurable": {"thread_id": thread_id},
                "recursion_limit": WORKFLOW_LIMITS["langgraph_recursion_limit"],
                **langfuse_cfg
            }

            current_query_type = None
            matched_table_images_buffer = []
            matched_educational_images_buffer = []  # ğŸ†• è¡›æ•™åœ–ç‰‡ç·©è¡å€

            final_answer_buffer = ""
            async for event in langgraph_app.astream(initial_state, config_with_limit):
                for node_name, node_output in event.items():
                    if node_name == "classify_query_type":
                        query_type = node_output.get("query_type", "")
                        if query_type:
                            current_query_type = query_type
                    if "final_answer" in node_output and node_output["final_answer"]:
                        final_answer_buffer = node_output["final_answer"]
                    if "matched_table_images" in node_output and node_output["matched_table_images"]:
                        matched_table_images_buffer = node_output["matched_table_images"]
                    # ğŸ†• æ•ç²è¡›æ•™åœ–ç‰‡
                    if "matched_educational_images" in node_output and node_output["matched_educational_images"]:
                        matched_educational_images_buffer = node_output["matched_educational_images"]
            if final_answer_buffer:
                # ä½¿ç”¨ OpenCC å°‡å›ç­”è½‰æ›ç‚ºç¹é«”ä¸­æ–‡
                final_answer_buffer = converter.convert(final_answer_buffer)

                # æ›´æ–° trace

                if langfuse_handler:
                    update_trace_io(
                        handler=langfuse_handler,
                        user_query=request.message,
                        final_answer=final_answer_buffer,
                        additional_metadata={"query_type": current_query_type}
                    )

                # ä¿å­˜åˆ°è¨˜æ†¶

                should_save_to_memory = (
                    current_query_type not in ["greet", "out_of_scope", "conversation_history"] and
                    final_answer_buffer not in [
                        "æŠ±æ­‰ï¼Œé€™å€‹å•é¡Œè¶…å‡ºäº†æˆ‘çš„å°ˆæ¥­ç¯„åœã€‚",
                        "æŠ±æ­‰ï¼Œé€™å€‹å•é¡Œè¶…å‡ºäº†æˆ‘çš„å°ˆæ¥­ç¯„åœã€‚æˆ‘å°ˆæ³¨æ–¼æä¾›é†«ç™‚æ„ŸæŸ“ç®¡åˆ¶ç›¸é—œçš„è«®è©¢æœå‹™ã€‚",
                        "æŠ±æ­‰ï¼Œæ‚¨çš„å•é¡Œè¶…å‡ºäº†æˆ‘çš„èªçŸ¥ç¯„åœï¼Œè«‹æ‚¨æ›å€‹æ–¹å¼å†æå•ä¸€æ¬¡ï¼Œæˆ–è€…è¯çµ¡ç›¸é—œå°ˆæ¥­äººå“¡ç‚ºæ‚¨æœå‹™ã€‚",
                        "æŠ±æ­‰ï¼Œè³‡æ–™åº«ä¸­æœªæ‰¾åˆ°å¯å›ç­”æ­¤å•é¡Œçš„é†«ç™‚è³‡è¨Šã€‚è«‹è«®è©¢å°ˆæ¥­é†«ç™‚äººå“¡æˆ–ç›¸é—œç§‘å®¤ã€‚",
                        "æŠ±æ­‰ï¼Œç„¡æ³•ç”Ÿæˆå›ç­”",
                        "æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ™‚ç™¼ç”ŸéŒ¯èª¤ã€‚"
                    ]
                )

                if should_save_to_memory:
                    use_long_term_memory = request.enable_long_term_memory if request.enable_long_term_memory is not None else LONG_TERM_MEMORY_CONFIG.get('enabled', False)
                    if use_long_term_memory:
                        try:
                            qa_pair = f"ç”¨æˆ¶å•é¡Œï¼š{request.message}\nåŠ©æ‰‹å›ç­”ï¼š{final_answer_buffer}"

                            memory_client.add(
                                qa_pair,
                                user_id=request.user_id,
                                metadata={"category": "medical_chat"},
                                prompt=MEMORY_DEDUCTION_PROMPT
                            )

                        except Exception as e:
                            pass

                # ğŸ”§ ä¿®æ­£ï¼šå°‡çµ•å°è·¯å¾‘è½‰æ›ç‚ºæª”å
                import os
                processed_table_images = []
                for table in matched_table_images_buffer:
                    processed_table = table.copy()
                    if 'image_path' in processed_table and processed_table['image_path']:
                        processed_table['image_path'] = os.path.basename(processed_table['image_path'])
                    processed_table_images.append(processed_table)

                try:
                    medical_response = MedicalResponse.parse_from_text(final_answer_buffer, query_type=current_query_type)

                    yield create_sse_message("structured_data", json.dumps(medical_response.to_dict(), ensure_ascii=False))

                except Exception:
                    pass

                # ğŸ†• è¿”å›è¡¨æ ¼åœ–ç‰‡è³‡è¨Š
                if processed_table_images:
                    yield create_sse_message("table_images", json.dumps(processed_table_images, ensure_ascii=False))

                # ğŸ†• è¿”å›è¡›æ•™åœ–ç‰‡è³‡è¨Š
                if matched_educational_images_buffer:
                    processed_edu_images = []
                    for img in matched_educational_images_buffer:
                        processed_edu_images.append({
                            'filename': img.get('filename', ''),
                            'image_path': img.get('image_path', ''),
                            'health_topic': img.get('health_topic', ''),
                            'core_message': img.get('core_message', ''),
                            'score': img.get('score', 0.0)
                        })
                    yield create_sse_message("educational_images", json.dumps(processed_edu_images, ensure_ascii=False))

                for char in final_answer_buffer:
                    yield create_sse_message("token", char)

                    await asyncio.sleep(0.005)

                if should_save_to_memory:
                    if use_short_term_memory:
                        conversation_store[session_id].append((request.message, final_answer_buffer))

            yield create_sse_message("done", "success")

        except Exception as e:
            yield create_sse_message("error", str(e))

    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"
        }
    )

@app.post("/chat")
async def chat(request: ChatRequest):
    try:
        session_id = request.session_id
        if session_id not in conversation_store:
            conversation_store[session_id] = []

        # ç²å– Langfuse é…ç½®
        # session_id ç”¨æ–¼åœ¨ Langfuse ä¸­åˆ†çµ„åŒä¸€æœƒè©±çš„æ‰€æœ‰å°è©±
        # æ¯æ¬¡èª¿ç”¨ CallbackHandler æœƒè‡ªå‹•å‰µå»ºæ–°çš„ trace
        langfuse_cfg, langfuse_handler = get_langfuse_config(
            user_id=request.user_id,
            session_id=session_id,  # ä½¿ç”¨å°è©±çš„ session_id,è€Œä¸æ˜¯æ¯æ¬¡ç”Ÿæˆæ–°çš„
            tags=["fastapi", "chat", "medical", "rag"],
            metadata={
                "query": request.message,
                "source": "fastapi_chat",
                "short_term_memory_enabled": request.enable_short_term_memory,
                "long_term_memory_enabled": request.enable_long_term_memory,
                "datasource_ids": request.datasource_ids,
                "enabled_tool_ids": request.enabled_tool_ids
            }
        )

        # æ§‹å»º messages åˆ—è¡¨
        messages = []

        # æª¢æŸ¥æ˜¯å¦å•Ÿç”¨çŸ­æœŸè¨˜æ†¶
        use_short_term_memory = request.enable_short_term_memory if request.enable_short_term_memory is not None else SHORT_TERM_MEMORY_CONFIG.get('enabled', True)
        if use_short_term_memory:
            for user_q, asst_a in conversation_store[session_id]:
                messages.append(HumanMessage(content=user_q))
                messages.append(AIMessage(content=asst_a))

        # æ·»åŠ ç•¶å‰ç”¨æˆ¶å•é¡Œ
        messages.append(HumanMessage(content=request.message))
        initial_state = {
            "messages": messages,
            "user_id": request.user_id,
            "query_type": "",
            "knowledge": "",
            "current_query": "",
            "memory_summary": "",
            "memory_response": "",
            "memory_source": "",
            "final_answer": "",
            "validation_result": "",
            "validation_feedback": "",
            "retry_count": 0,
            "query_type_history": [],
            "knowledge_retrieval_count": 0,
            "validation_need_supplement_info": "",
            "question_category": "",
            "question_type_reasoning": "",
            "suggested_sub_questions": [],
            "iteration_count": 0,
            "used_sources_dict": {},
            "original_docs_dict": {},
            "matched_table_images": [],
            "matched_educational_images": [],  # ğŸ†• è¡›æ•™åœ–ç‰‡
            "context": {},
            "planning_result": None,  # ğŸ†• Planning çµæœ
            "retrieval_steps": [],  # ğŸ†• æª¢ç´¢æ­¥é©Ÿ
            "current_step": 0,  # ğŸ†• ç•¶å‰æ­¥é©Ÿ
            "enable_short_term_memory": use_short_term_memory,
            "enable_long_term_memory": request.enable_long_term_memory,
            "datasource_ids": request.datasource_ids if request.datasource_ids is not None else RETRIEVAL_CONFIG.get('default_datasource_ids'),
            "enabled_tool_ids": request.enabled_tool_ids if request.enabled_tool_ids is not None else TOOLS_CONFIG.get('default_tools')
        }
        # é‹è¡Œ graph
        # ä½¿ç”¨ user_id + session_id çµ„åˆç¢ºä¿ä¸åŒç”¨æˆ¶çš„æœƒè©±å®Œå…¨éš”é›¢
        thread_id = f"{request.user_id}_{session_id}"
        config_with_limit = {
            "configurable": {"thread_id": thread_id},
            "recursion_limit": WORKFLOW_LIMITS["langgraph_recursion_limit"],
            **langfuse_cfg
        }
        result = await langgraph_app.ainvoke(initial_state, config_with_limit)
        answer = result.get("final_answer", "æŠ±æ­‰ï¼Œç„¡æ³•ç”Ÿæˆå›ç­”")
        
        # ä½¿ç”¨ OpenCC å°‡å›ç­”è½‰æ›ç‚ºç¹é«”ä¸­æ–‡
        if answer:
            answer = converter.convert(answer)

        query_type = result.get("query_type", "")

        # æ›´æ–° trace
        if langfuse_handler:
            update_trace_io(
                handler=langfuse_handler,
                user_query=request.message,
                final_answer=answer,
                additional_metadata={"query_type": query_type}
            )

        # ä¿å­˜åˆ°è¨˜æ†¶
        should_save_to_memory = (
            query_type not in ["greet", "out_of_scope", "conversation_history"] and
            answer not in [
                "æŠ±æ­‰ï¼Œé€™å€‹å•é¡Œè¶…å‡ºäº†æˆ‘çš„å°ˆæ¥­ç¯„åœã€‚",
                "æŠ±æ­‰ï¼Œé€™å€‹å•é¡Œè¶…å‡ºäº†æˆ‘çš„å°ˆæ¥­ç¯„åœã€‚æˆ‘å°ˆæ³¨æ–¼æä¾›é†«ç™‚æ„ŸæŸ“ç®¡åˆ¶ç›¸é—œçš„è«®è©¢æœå‹™ã€‚",
                "æŠ±æ­‰ï¼Œæ‚¨çš„å•é¡Œè¶…å‡ºäº†æˆ‘çš„èªçŸ¥ç¯„åœï¼Œè«‹æ‚¨æ›å€‹æ–¹å¼å†æå•ä¸€æ¬¡ï¼Œæˆ–è€…è¯çµ¡ç›¸é—œå°ˆæ¥­äººå“¡ç‚ºæ‚¨æœå‹™ã€‚",
                "æŠ±æ­‰ï¼Œè³‡æ–™åº«ä¸­æœªæ‰¾åˆ°å¯å›ç­”æ­¤å•é¡Œçš„é†«ç™‚è³‡è¨Šã€‚è«‹è«®è©¢å°ˆæ¥­é†«ç™‚äººå“¡æˆ–ç›¸é—œç§‘å®¤ã€‚",
                "æŠ±æ­‰ï¼Œç„¡æ³•ç”Ÿæˆå›ç­”",
                "æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ™‚ç™¼ç”ŸéŒ¯èª¤ã€‚"
            ]
        )

        if should_save_to_memory:
            use_long_term_memory = request.enable_long_term_memory if request.enable_long_term_memory is not None else LONG_TERM_MEMORY_CONFIG.get('enabled', False)
            if use_long_term_memory:
                try:
                    qa_pair = f"ç”¨æˆ¶å•é¡Œï¼š{request.message}\nåŠ©æ‰‹å›ç­”ï¼š{answer}"
                    memory_client.add(
                        qa_pair,
                        user_id=request.user_id,
                        metadata={"category": "medical_chat"},
                        prompt=MEMORY_DEDUCTION_PROMPT
                    )
                except Exception as e:
                    pass

        if should_save_to_memory:
            if use_short_term_memory:
                conversation_store[session_id].append((request.message, answer))

        matched_table_images = result.get("matched_table_images", [])
        matched_educational_images = result.get("matched_educational_images", [])

        # ğŸ”§ ä¿®æ­£ï¼šå°‡çµ•å°è·¯å¾‘è½‰æ›ç‚ºæª”åï¼Œä¾›å‰ç«¯ä½¿ç”¨ /api/table-image/{filename}
        # åŒæ™‚éæ¿¾æ‰ä¸éœ€è¦çš„æ¬„ä½ï¼ˆmatched, table_contentï¼‰
        import os
        processed_table_images = []
        for table in matched_table_images:
            # åªä¿ç•™å‰ç«¯éœ€è¦çš„æ¬„ä½
            processed_table = {
                'image_path': os.path.basename(table.get('image_path', '')) if table.get('image_path') else '',
                'similarity': table.get('similarity', 1.0),
                'source': table.get('source', 'matching')
            }
            processed_table_images.append(processed_table)

        # ğŸ†• è™•ç†è¡›æ•™åœ–ç‰‡
        processed_edu_images = []
        for img in matched_educational_images:
            processed_edu_images.append({
                'filename': img.get('filename', ''),
                'image_path': img.get('image_path', ''),
                'health_topic': img.get('health_topic', ''),
                'core_message': img.get('core_message', ''),
                'score': img.get('score', 0.0)
            })

        try:
            medical_response = MedicalResponse.parse_from_text(answer, query_type=query_type)
            return {
                "status": "success",
                "answer": answer,
                "query_type": query_type,
                "structured_response": medical_response.to_dict(),
                "matched_table_images": processed_table_images,
                "matched_educational_images": processed_edu_images  # ğŸ†•
            }
        except Exception:
            return {
                "status": "success",
                "answer": answer,
                "query_type": query_type,
                "matched_table_images": processed_table_images,
                "matched_educational_images": processed_edu_images  # ğŸ†•
            }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/memory/clear/short_term")
async def clear_short_term_memory(user_id: str):
    try:
        keys_to_delete = [k for k in conversation_store.keys() if k.startswith(user_id)]
        for key in keys_to_delete:
            del conversation_store[key]
        return {"status": "success"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/memory/clear/long_term")
async def clear_long_term_memory(user_id: str):
    try:
        from core.config import LONG_TERM_MEMORY_CONFIG
        if LONG_TERM_MEMORY_CONFIG.get('enabled', True):
            memory_client.delete_all(user_id=user_id)
            return {"status": "success", "message": "é•·æœŸè¨˜æ†¶å·²æ¸…é™¤"}
        else:
            return {"status": "success", "message": "é•·æœŸè¨˜æ†¶å·²åœç”¨ï¼Œç„¡éœ€æ¸…é™¤"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/memory/clear/all")
async def clear_all_memory(user_id: str):
    try:
        # æ¸…é™¤çŸ­æœŸè¨˜æ†¶
        keys_to_delete = [k for k in conversation_store.keys() if k.startswith(user_id)]
        for key in keys_to_delete:
            del conversation_store[key]

        # æ ¹æ“šé…ç½®æ±ºå®šæ˜¯å¦æ¸…é™¤é•·æœŸè¨˜æ†¶
        from core.config import LONG_TERM_MEMORY_CONFIG
        if LONG_TERM_MEMORY_CONFIG.get('enabled', True):
            memory_client.delete_all(user_id=user_id)
            return {"status": "success", "message": "æ‰€æœ‰è¨˜æ†¶å·²æ¸…é™¤"}
        else:
            return {"status": "success", "message": "çŸ­æœŸè¨˜æ†¶å·²æ¸…é™¤ï¼ˆé•·æœŸè¨˜æ†¶å·²åœç”¨ï¼‰"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/table-image/{filename}")
async def get_table_image(filename: str):
    """
    æä¾›è¡¨æ ¼åœ–ç‰‡æœå‹™

    Args:
        filename: åœ–ç‰‡æª”åï¼Œä¾‹å¦‚ 'åœ‹äººè†³é£Ÿç‡Ÿé¤Šç´ åƒè€ƒæ”å–é‡_p10_t1.jpg'

    Returns:
        FileResponse: åœ–ç‰‡æª”æ¡ˆ

    Raises:
        HTTPException: 404 å¦‚æœæª”æ¡ˆä¸å­˜åœ¨
    """
    import os

    # è¡¨æ ¼ç›®éŒ„
    from core.config import EXTRACTED_TABLES_DIR

    print(f"\nğŸ” DEBUG: æ”¶åˆ°åœ–ç‰‡è«‹æ±‚ - filename = {filename}")

    # é©—è­‰æª”åï¼ˆé˜²æ­¢è·¯å¾‘éæ­·æ”»æ“Šï¼‰
    if ".." in filename or "/" in filename or "\\" in filename:
        print(f"âŒ ç„¡æ•ˆçš„æª”æ¡ˆåç¨±: {filename}")
        raise HTTPException(status_code=400, detail="ç„¡æ•ˆçš„æª”æ¡ˆåç¨±")

    # åªå…è¨±åœ–ç‰‡æª”æ¡ˆ
    if not filename.lower().endswith(('.jpg', '.jpeg', '.png')):
        raise HTTPException(status_code=400, detail="åƒ…æ”¯æ´åœ–ç‰‡æª”æ¡ˆ")

    # æœå°‹æª”æ¡ˆï¼ˆæ”¯æ´å­ç›®éŒ„ï¼‰
    # æ ¼å¼ç¯„ä¾‹: åœ‹äººè†³é£Ÿç‡Ÿé¤Šç´ åƒè€ƒæ”å–é‡_p10_t1.jpg
    # å¯èƒ½ä½æ–¼: extracted_tables/åœ‹äººè†³é£Ÿç‡Ÿé¤Šç´ åƒè€ƒæ”å–é‡/åœ‹äººè†³é£Ÿç‡Ÿé¤Šç´ åƒè€ƒæ”å–é‡_p10_t1.jpg

    # å˜—è©¦ 1: ç›´æ¥åœ¨æ ¹ç›®éŒ„
    file_path = os.path.join(EXTRACTED_TABLES_DIR, filename)
    print(f"ğŸ” å˜—è©¦ 1: {file_path}")
    if os.path.exists(file_path) and os.path.isfile(file_path):
        print(f"âœ… æ‰¾åˆ°åœ–ç‰‡ï¼ˆæ ¹ç›®éŒ„ï¼‰: {file_path}")
        return FileResponse(
            file_path,
            media_type="image/jpeg" if filename.lower().endswith('.jpg') or filename.lower().endswith('.jpeg') else "image/png",
            filename=filename
        )

    # å˜—è©¦ 2: åœ¨çµ±ä¸€çš„ images è³‡æ–™å¤¾ä¸­å°‹æ‰¾
    images_path = os.path.join(EXTRACTED_TABLES_DIR, "images", filename)
    print(f"ğŸ” å˜—è©¦ 2: {images_path}")
    if os.path.exists(images_path) and os.path.isfile(images_path):
        print(f"âœ… æ‰¾åˆ°åœ–ç‰‡ï¼ˆimages ç›®éŒ„ï¼‰: {images_path}")
        return FileResponse(
            images_path,
            media_type="image/jpeg" if filename.lower().endswith('.jpg') or filename.lower().endswith('.jpeg') else "image/png",
            filename=filename
        )

    # å˜—è©¦ 3: éæ­·æ‰€æœ‰å­ç›®éŒ„æœå°‹
    print(f"ğŸ” å˜—è©¦ 3: éæ­·æ‰€æœ‰å­ç›®éŒ„...")
    for root, _, files in os.walk(EXTRACTED_TABLES_DIR):
        if filename in files:
            found_path = os.path.join(root, filename)
            print(f"âœ… æ‰¾åˆ°åœ–ç‰‡ï¼ˆéæ­·ï¼‰: {found_path}")
            return FileResponse(
                found_path,
                media_type="image/jpeg" if filename.lower().endswith('.jpg') or filename.lower().endswith('.jpeg') else "image/png",
                filename=filename
            )

    # æ‰¾ä¸åˆ°æª”æ¡ˆ
    print(f"âŒ æ‰¾ä¸åˆ°åœ–ç‰‡: {filename}")
    raise HTTPException(status_code=404, detail=f"æ‰¾ä¸åˆ°åœ–ç‰‡: {filename}")


@app.get("/api/educational-image/{filename}")
async def get_educational_image(filename: str):
    """
    æä¾›è¡›æ•™åœ–ç‰‡æœå‹™

    Args:
        filename: åœ–ç‰‡æª”åï¼Œä¾‹å¦‚ 'Bå‹è‚ç‚è¡›æ•™_p1_img1.jpg'

    Returns:
        FileResponse: åœ–ç‰‡æª”æ¡ˆ

    Raises:
        HTTPException: 404 å¦‚æœæª”æ¡ˆä¸å­˜åœ¨
    """
    import os

    print(f"\nğŸ” DEBUG: æ”¶åˆ°è¡›æ•™åœ–ç‰‡è«‹æ±‚ - filename = {filename}")

    # é©—è­‰æª”åï¼ˆé˜²æ­¢è·¯å¾‘éæ­·æ”»æ“Šï¼‰
    if ".." in filename or "/" in filename or "\\" in filename:
        print(f"âŒ ç„¡æ•ˆçš„æª”æ¡ˆåç¨±: {filename}")
        raise HTTPException(status_code=400, detail="ç„¡æ•ˆçš„æª”æ¡ˆåç¨±")

    # åªå…è¨±åœ–ç‰‡æª”æ¡ˆ
    if not filename.lower().endswith(('.jpg', '.jpeg', '.png')):
        raise HTTPException(status_code=400, detail="åƒ…æ”¯æ´åœ–ç‰‡æª”æ¡ˆ")

    # è¡›æ•™åœ–ç‰‡ç›®éŒ„ï¼ˆå¾ç’°å¢ƒè®Šæ•¸è®€å–ï¼Œé€—è™Ÿåˆ†éš”ï¼‰
    import os
    image_dirs_env = os.getenv("IMAGE_DIRS", "/home/danny/AI-agent/high_value_images,/home/danny/AI-agent/image_search")
    possible_dirs = [d.strip() for d in image_dirs_env.split(",") if d.strip()]

    for base_dir in possible_dirs:
        # å˜—è©¦ç›´æ¥åœ¨ç›®éŒ„ä¸­å°‹æ‰¾
        file_path = os.path.join(base_dir, filename)
        if os.path.exists(file_path) and os.path.isfile(file_path):
            print(f"âœ… æ‰¾åˆ°è¡›æ•™åœ–ç‰‡: {file_path}")
            return FileResponse(
                file_path,
                media_type="image/jpeg" if filename.lower().endswith(('.jpg', '.jpeg')) else "image/png",
                filename=filename
            )

        # éæ­·å­ç›®éŒ„
        if os.path.exists(base_dir):
            for root, _, files in os.walk(base_dir):
                if filename in files:
                    found_path = os.path.join(root, filename)
                    print(f"âœ… æ‰¾åˆ°è¡›æ•™åœ–ç‰‡ï¼ˆéæ­·ï¼‰: {found_path}")
                    return FileResponse(
                        found_path,
                        media_type="image/jpeg" if filename.lower().endswith(('.jpg', '.jpeg')) else "image/png",
                        filename=filename
                    )

    # æ‰¾ä¸åˆ°æª”æ¡ˆ
    print(f"âŒ æ‰¾ä¸åˆ°è¡›æ•™åœ–ç‰‡: {filename}")
    raise HTTPException(status_code=404, detail=f"æ‰¾ä¸åˆ°è¡›æ•™åœ–ç‰‡: {filename}")


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="172.23.37.2", port=8100, log_level="info")
