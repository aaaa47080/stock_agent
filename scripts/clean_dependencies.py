"""
ä¾èµ–åŒ…æ¸…ç†åˆ†æè„šæœ¬
åˆ†æé¡¹ç›®å®é™…ä½¿ç”¨çš„åŒ…ï¼Œç”Ÿæˆç²¾ç®€çš„ requirements.txt
"""
import ast
import os
from pathlib import Path
from typing import Set
import re

# å·²çŸ¥çš„æ˜ç¡®æœªä½¿ç”¨çš„åŒ…ï¼ˆé€šè¿‡åˆ†æç¡®è®¤ï¼‰
UNUSED_PACKAGES = {
    'backtrader',  # æœªåœ¨ä»£ç ä¸­æ‰¾åˆ°å¼•ç”¨
    'gradio',  # UIåº“ï¼Œæœªä½¿ç”¨
    'gradio_client',  # Gradio å®¢æˆ·ç«¯
    'ipython',  # å¼€å‘å·¥å…·ï¼Œç”Ÿäº§ç¯å¢ƒä¸éœ€è¦
    'ipython_pygments_lexers',
    'ipywidgets',
    'jupyterlab_widgets',
    'widgetsnbextension',
    'jupyter',
    'ffmpy',  # éŸ³é¢‘å¤„ç†ï¼Œæœªä½¿ç”¨
    'pydub',  # éŸ³é¢‘å¤„ç†ï¼Œæœªä½¿ç”¨
    'ImageIO',  # å›¾åƒå¤„ç†ï¼Œæœªä½¿ç”¨ï¼ˆæ³¨æ„å¤§å°å†™ï¼‰
    'imageio',
    'anywidget',
    'groovy',  # æœªæ‰¾åˆ°å¼•ç”¨
    'vectorbt',  # å›æµ‹åº“ï¼Œæœªä½¿ç”¨
    'matplotlib-inline',  # Jupyter ä¸“ç”¨
    'tradingpattern',  # æœªæ‰¾åˆ°å¼•ç”¨
    'hf-xet',  # Hugging Face å·¥å…·ï¼Œæœªä½¿ç”¨
}

# å®é™…ä½¿ç”¨çš„æ ¸å¿ƒåŒ…ï¼ˆä» import åˆ†æ)
CORE_PACKAGES = {
    # FastAPI æ ¸å¿ƒ
    'fastapi',
    'uvicorn',
    'gunicorn',  # æ–°å¢çš„ç”Ÿäº§ç¯å¢ƒä¾èµ–
    'starlette',
    'pydantic',
    'pydantic_core',
    'pydantic-settings',
    'python-multipart',
    
    # ASGI/å¼‚æ­¥
    'h11',
    'httpcore',
    'httpx',
    'httpx-sse',
    'aiohttp',
    'aiohappyeyeballs',
    'aiosignal',
    'aiofiles',
    'aiosqlite',
    'anyio',
    'sniffio',
    'websockets',
    
    # æ•°æ®åº“
    'SQLAlchemy',
    'sqlite-vec',
    
    # LangChain ç”Ÿæ€
    'langchain',
    'langchain-core',
    'langchain-community',
    'langchain-openai',
    'langchain-google-genai',
    'langchain-text-splitters',
    'langgraph',
    'langgraph-checkpoint',
    'langgraph-checkpoint-sqlite',
    'langgraph-prebuilt',
    'langgraph-sdk',
    'langsmith',
    
    # LLM æä¾›å•†
    'openai',
    'google-genai',
    'google-generativeai',
    'google-ai-generativelanguage',
    'google-api-core',
    'google-api-python-client',
    'google-auth',
    'google-auth-httplib2',
    
    # æ•°æ®å¤„ç†
    'pandas',
    'numpy',
    'pandas-ta',  # æŠ€æœ¯æŒ‡æ ‡
    'scipy',
    'scikit-learn',
    'seaborn',
    
    # å›¾è¡¨
    'matplotlib',
    'plotly',
    'contourpy',
    'cycler',
    'fonttools',
    'kiwisolver',
    
    # æ—¶é—´å¤„ç†
    'python-dateutil',
    'pytz',
    'tzdata',
    'tzlocal',
    'dateparser',
    
    # å·¥å…·åº“
    'requests',
    'requests-toolbelt',
    'click',
    'rich',
    'typer',
    'typer-slim',
    
    # ç¼“å­˜
    'cachetools',
    
    # ç¯å¢ƒå˜é‡
    'python-dotenv',
    
    # JSONå¤„ç†
    'orjson',
    'dirtyjson',
    'jiter',
    
    # åºåˆ—åŒ–
    'marshmallow',
    'dataclasses-json',
    
    # åè®®å’Œæ ¼å¼
    'protobuf',
    'proto-plus',
    'googleapis-common-protos',
    'grpcio',
    'grpcio-status',
    
    # Token å¤„ç†
    'tiktoken',
    
    # ç›‘æ§æ—¥å¿—
    'filelock',
    'fsspec',
    
    # ç½‘ç»œè¯·æ±‚
    'certifi',
    'charset-normalizer',
    'idna',
    'urllib3',
    'httplib2',
    
    # å‹ç¼©
    'brotli',
    'zstandard',
    
    # å…¶ä»–å·¥å…·
    'packaging',
    'Jinja2',
    'MarkupSafe',
    'regex',
    'six',
    'decorator',
    'attrs',
    'frozenlist',
    'multidict',
    'propcache',
    'yarl',
    'tqdm',
    'joblib',
    'threadpoolctl',
    'dill',
   'pillow',
    'pyparsing',
    'tenacity',
    'annotated-types',
    'typing_extensions',
    'typing-inspect',
    'typing-inspection',
    'mypy_extensions',
    'distro',
    'shellingham',
    'tomlkit',
    'PyYAML',
    'markdown-it-py',
    'mdurl',
    'Pygments',
    'pyasn1',
    'pyasn1_modules',
    'rsa',
    'uritemplate',
    'uuid_utils',
    'semantic-version',
    'narwhals',
    'safehttpx',
    'jsonpatch',
    'jsonpointer',
    'filetype',
    'ormsgpack',
    'schedule',
    'xxhash',
    'greenlet',
}

def analyze_imports(project_root: Path) -> Set[str]:
    """åˆ†æé¡¹ç›®ä¸­å®é™…å¯¼å…¥çš„åŒ…"""
    imports = set()
    
    for py_file in project_root.rglob("*.py"):
        # è·³è¿‡è™šæ‹Ÿç¯å¢ƒå’Œç¼“å­˜
        if any(p in py_file.parts for p in ['.venv', '__pycache__', '.git']):
            continue
        
        try:
            with open(py_file, 'r', encoding='utf-8') as f:
                content = f.read()
                
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå– import
            import_patterns = [
                r'^import\s+(\w+)',
                r'^from\s+(\w+)',
            ]
            
            for pattern in import_patterns:
                for match in re.finditer(pattern, content, re.MULTILINE):
                    imports.add(match.group(1))
                    
        except Exception as e:
            print(f"Error reading {py_file}: {e}")
    
    return imports

def generate_clean_requirements():
    """ç”Ÿæˆæ¸…ç†åçš„ requirements.txt"""
    # å®šä½é¡¹ç›®æ ¹ç›®å½• (è„šæœ¬åœ¨ scripts/ ä¸‹)
    project_root = Path(__file__).parent.parent
    req_file = project_root / 'requirements.txt'
    
    if not req_file.exists():
        print(f"âŒ æ‰¾ä¸åˆ° requirements.txt: {req_file}")
        return
    
    # è¯»å–ç°æœ‰ requirements
    with open(req_file, 'r', encoding='utf-8') as f:
        lines = f.readlines()
    
    # åˆ†æä½¿ç”¨çš„åŒ…
    used_imports = analyze_imports(project_root)
    
    print("ğŸ” å®é™…å¼•ç”¨çš„åŒ… (å‰20ä¸ª):")
    for imp in sorted(list(used_imports)[:20]):
        print(f"  - {imp}")
    print(f"  ... å…± {len(used_imports)} ä¸ª")
    
    # æ¸…ç†åçš„ä¾èµ–
    clean_lines = []
    removed_packages = []
    
    for line in lines:
        line_stripped = line.strip()
        if not line_stripped or line_stripped.startswith('#'):
            clean_lines.append(line)
            continue
        
        # æå–åŒ…å
        package_name = line_stripped.split('==')[0].split('[')[0].lower()
        
        # æ£€æŸ¥æ˜¯å¦åœ¨åˆ é™¤åˆ—è¡¨ä¸­
        if any(unused.lower() in package_name for unused in UNUSED_PACKAGES):
            removed_packages.append(line_stripped)
            print(f"âŒ ç§»é™¤: {line_stripped}")
        else:
            clean_lines.append(line)
    
    # å†™å…¥å¤‡ä»½
    backup_file = project_root / 'requirements.txt.backup'
    with open(backup_file, 'w', encoding='utf-8') as f:
        f.writelines(lines)
    print(f"\nâœ… åŸæ–‡ä»¶å¤‡ä»½åˆ°: {backup_file}")
    
    # å†™å…¥æ¸…ç†åçš„æ–‡ä»¶
    with open(req_file, 'w', encoding='utf-8') as f:
        f.writelines(clean_lines)
    
    print(f"\nğŸ“Š æ¸…ç†æ€»ç»“:")
    print(f"  åŸå§‹åŒ…æ•°: {len([l for l in lines if l.strip() and not l.strip().startswith('#')])}")
    print(f"  ç§»é™¤åŒ…æ•°: {len(removed_packages)}")
    print(f"  ä¿ç•™åŒ…æ•°: {len([l for l in clean_lines if l.strip() and not l.strip().startswith('#')])}")
    
    print(f"\nğŸ—‘ï¸  å·²ç§»é™¤çš„åŒ…:")
    for pkg in removed_packages:
        print(f"  - {pkg}")


if __name__ == "__main__":
    print("=" * 60)
    print("ä¾èµ–åŒ…æ¸…ç†å·¥å…·")
    print("=" * 60)
    generate_clean_requirements()
    print("\nâœ… å®Œæˆï¼è¯·æµ‹è¯•åº”ç”¨æ˜¯å¦æ­£å¸¸è¿è¡Œã€‚")
