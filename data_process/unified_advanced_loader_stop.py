"""
çµ±ä¸€é€²éšè¼‰å…¥å™¨ - æ•´åˆç‰ˆ
åŠŸèƒ½ï¼š
1. æ”¯æ´ JSONL å•ç­”è³‡æ–™è¼‰å…¥
2. æ”¯æ´å¤šè³‡æ–™å¤¾ PDF æ‰¹æ¬¡è™•ç†
3. å¼·å¤§çš„ OCR + è¡¨æ ¼/åœ–ç‰‡æ“·å–ï¼ˆä¾†è‡ª update_pdf_to_db_v2.pyï¼‰
4. è‡ªå‹•å»ºç«‹å¤šå€‹ collectionï¼ˆä¾è³‡æ–™å¤¾å‘½åï¼‰
"""
import os
import sys
import json
from pathlib import Path
import tempfile
import time
import re
import gc
from io import StringIO, BytesIO

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import torch
import fitz
from PIL import Image
from transformers import AutoModel, AutoTokenizer

from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_postgres import PGVector
import psycopg2
from psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT

from core.config import (
    DB_HOST, DB_NAME, DB_PORT, DB_PASSWORD, DB_USER,
    embeddings, get_reference_mapping
)


# ==================== é…ç½® ====================
os.environ["CUDA_VISIBLE_DEVICES"] = '0'
os.environ["CUDA_LAUNCH_BLOCKING"] = '1'

OCR_MODEL_PATH = '/home/danny/AI-agent/deepseek_ocr'
from core.config import EXTRACTED_TABLES_DIR
EXTRACTED_TABLES_BASE_DIR = EXTRACTED_TABLES_DIR

# OCR åƒæ•¸
OCR_IMAGE_SIZE = 1024
HIGH_RES_DPI = 100
MIN_IMAGE_SIZE = 100
SKIP_FIRST_PAGE_IMAGES = False


# ==================== GPU æ¸…ç† ====================
def cleanup_gpu():
    """æ¸…ç† GPU è¨˜æ†¶é«”"""
    gc.collect()
    torch.cuda.empty_cache()
    torch.cuda.synchronize()


# ==================== OCR è¼”åŠ©å‡½æ•¸ ====================
def capture_model_output():
    """æ•ç²æ¨¡å‹çš„æ¨™æº–è¼¸å‡º"""
    old_stdout = sys.stdout
    sys.stdout = captured_output = StringIO()
    return old_stdout, captured_output


def restore_stdout(old_stdout):
    """æ¢å¾©æ¨™æº–è¼¸å‡º"""
    sys.stdout = old_stdout


def extract_from_stdout(captured_text):
    """å¾æ•ç²çš„è¼¸å‡ºä¸­æå–è¡¨æ ¼æ¨™è¨˜"""
    start_marker = '<|ref|>'
    if start_marker not in captured_text:
        return None

    start_pos = captured_text.find(start_marker)
    end_marker = '===============save results:==============='
    end_pos = captured_text.find(end_marker)

    if end_pos == -1:
        relevant_text = captured_text[start_pos:]
    else:
        relevant_text = captured_text[start_pos:end_pos]

    return relevant_text.strip()


def extract_tables_and_images_from_result(result_text):
    """å¾ OCR çµæœä¸­æå–è¡¨æ ¼å’Œåœ–ç‰‡å€åŸŸ"""
    all_objects = []

    # é€šç”¨æ¨¡å¼:åŒ¹é… <|ref|>TYPE<|/ref|><|det|>[[x1, y1, x2, y2]]<|/det|>
    object_pattern = r'<\|ref\|>(\w+)<\|/ref\|><\|det\|>\[\[(\d+),\s*(\d+),\s*(\d+),\s*(\d+)\]\]<\|/det\|>'
    object_matches = re.findall(object_pattern, result_text)

    for match in object_matches:
        obj_type = match[0]
        x1, y1, x2, y2 = int(match[1]), int(match[2]), int(match[3]), int(match[4])

        # åªè™•ç†è¡¨æ ¼å’Œåœ–ç‰‡ç›¸é—œé¡å‹
        if obj_type in ['table', 'image', 'figure', 'chart', 'diagram']:
            obj_data = {
                'bbox': (x1, y1, x2, y2),
                'type': obj_type,
                'html': None
            }

            # å¦‚æœæ˜¯è¡¨æ ¼,å˜—è©¦æå– HTML
            if obj_type == 'table':
                search_str = f'[[{x1}, {y1}, {x2}, {y2}]]'
                search_start = result_text.find(search_str)
                if search_start != -1:
                    table_start = result_text.find('<table>', search_start)
                    if table_start != -1:
                        table_end = result_text.find('</table>', table_start)
                        if table_end != -1:
                            obj_data['html'] = result_text[table_start:table_end + 8]

            all_objects.append(obj_data)

    # æå–æ¨™é¡Œ(table_caption, figure_caption ç­‰)
    caption_pattern = r'<\|ref\|>(\w+_caption)<\|/ref\|><\|det\|>\[\[(\d+),\s*(\d+),\s*(\d+),\s*(\d+)\]\]<\|/det\|>\s*\n?(.*?)(?=\n\n|\n<\||$)'
    caption_matches = re.findall(caption_pattern, result_text, re.DOTALL)

    captions = []
    for match in caption_matches:
        captions.append({
            'bbox': (int(match[1]), int(match[2]), int(match[3]), int(match[4])),
            'text': match[5].strip(),
            'type': match[0]
        })

    return all_objects, captions


def merge_overlapping_objects(objects_data, overlap_threshold=0.3):
    """åˆä½µé‡ç–Šçš„è¡¨æ ¼ç‰©ä»¶"""
    if not objects_data:
        return objects_data

    tables = [obj for obj in objects_data if obj['type'] == 'table']
    other_objects = [obj for obj in objects_data if obj['type'] != 'table']

    if len(tables) <= 1:
        return objects_data

    tables.sort(key=lambda obj: obj['bbox'][1])

    merged_tables = []
    skip_indices = set()

    for i, table1 in enumerate(tables):
        if i in skip_indices:
            continue

        bbox1 = table1['bbox']
        x1_1, y1_1, x2_1, y2_1 = bbox1

        for j in range(i + 1, len(tables)):
            if j in skip_indices:
                continue

            table2 = tables[j]
            bbox2 = table2['bbox']
            x1_2, y1_2, x2_2, y2_2 = bbox2

            y_overlap_start = max(y1_1, y1_2)
            y_overlap_end = min(y2_1, y2_2)
            y_overlap = max(0, y_overlap_end - y_overlap_start)

            height1 = y2_1 - y1_1
            height2 = y2_2 - y1_2
            smaller_height = min(height1, height2)

            vertical_overlap_ratio = y_overlap / smaller_height if smaller_height > 0 else 0
            horizontal_gap = min(abs(x2_1 - x1_2), abs(x2_2 - x1_1))

            if vertical_overlap_ratio > overlap_threshold and horizontal_gap < 100:
                merged_bbox = (
                    min(x1_1, x1_2),
                    min(y1_1, y1_2),
                    max(x2_1, x2_2),
                    max(y2_1, y2_2)
                )

                table1['bbox'] = merged_bbox
                skip_indices.add(j)

        merged_tables.append(table1)

    return merged_tables + other_objects


def find_related_caption(object_bbox, object_type, captions, max_distance=100):
    """å°‹æ‰¾ç‰©ä»¶å°æ‡‰çš„æ¨™é¡Œ"""
    object_y1 = object_bbox[1]
    best_caption = None
    min_distance = max_distance

    caption_type_map = {
        'table': 'table_caption',
        'figure': 'figure_caption',
        'image': 'figure_caption',
        'chart': 'figure_caption',
        'diagram': 'figure_caption',
    }
    target_caption_type = caption_type_map.get(object_type, f'{object_type}_caption')

    for caption in captions:
        if caption['type'] != target_caption_type:
            continue

        caption_y2 = caption['bbox'][3]
        if caption_y2 < object_y1:
            distance = object_y1 - caption_y2
            if distance < min_distance:
                min_distance = distance
                best_caption = caption['text']

    return best_caption


def is_references_section(text):
    """
    æª¢æ¸¬æ–‡æœ¬æ˜¯å¦ç‚ºåƒè€ƒæ–‡ç»å€å¡Š

    Returns:
        tuple: (is_references: bool, references_start_pos: int or None)
               å¦‚æœæ˜¯åƒè€ƒæ–‡ç»å€å¡Šï¼Œè¿”å› (True, é–‹å§‹ä½ç½®)
               å¦‚æœä¸æ˜¯ï¼Œè¿”å› (False, None)
    """
    if not text:
        return False, None

    # åƒè€ƒæ–‡ç»æ¨™é¡Œæ¨¡å¼
    ref_title_patterns = [
        r'(^|\n)\s*#{0,3}\s*åƒè€ƒæ–‡[ç»çŒ®]',
        r'(^|\n)\s*#{0,3}\s*References?\s*\n',
        r'(^|\n)\s*#{0,3}\s*åƒè€ƒè³‡æ–™',
        r'(^|\n)\s*#{0,3}\s*å¼•ç”¨æ–‡[ç»çŒ®]',
        r'(^|\n)\s*#{0,3}\s*Bibliography',
        r'(^|\n)\s*#{0,3}\s*æ–‡[ç»çŒ®]',
    ]

    for pattern in ref_title_patterns:
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            return True, match.start()

    # æª¢æ¸¬æ˜¯å¦æ•´é éƒ½æ˜¯å¼•ç”¨æ ¼å¼ï¼ˆæ²’æœ‰æ¨™é¡Œä½†å…§å®¹æ˜¯å¼•ç”¨ï¼‰
    lines = text.strip().split('\n')
    citation_count = 0
    total_lines = 0

    citation_patterns = [
        r'^\d+\.\s*[A-Z][a-z]+\s+[A-Z]{1,2}',  # 1. Smith AB...
        r'^\d+\.\s*[A-Z]{2,}[\.\s]',  # 1. WHO...
        r'^\[\d+\]',  # [1]
        r'J\s+(Clin\s+)?Endocrinol',  # Journal names
        r'\d{4}[;:]\d+[-â€“]\d+',  # å¹´ä»½;é ç¢¼ å¦‚ 2004;35:241-249
    ]

    for line in lines:
        line = line.strip()
        if len(line) < 5:
            continue
        total_lines += 1
        for pattern in citation_patterns:
            if re.search(pattern, line):
                citation_count += 1
                break

    # å¦‚æœè¶…é 60% çš„è¡Œæ˜¯å¼•ç”¨æ ¼å¼ï¼Œåˆ¤å®šç‚ºåƒè€ƒæ–‡ç»é 
    if total_lines > 5 and citation_count / total_lines > 0.6:
        return True, 0

    return False, None


def remove_references_section(text):
    """
    å¾æ–‡æœ¬ä¸­ç§»é™¤åƒè€ƒæ–‡ç»å€å¡Š

    Returns:
        str: ç§»é™¤åƒè€ƒæ–‡ç»å¾Œçš„æ–‡æœ¬
    """
    is_ref, start_pos = is_references_section(text)

    if is_ref and start_pos is not None:
        # å¦‚æœæ•´é éƒ½æ˜¯åƒè€ƒæ–‡ç»ï¼ˆstart_pos == 0ï¼‰ï¼Œè¿”å›ç©º
        if start_pos == 0:
            return ""
        # å¦å‰‡åªä¿ç•™åƒè€ƒæ–‡ç»ä¹‹å‰çš„å…§å®¹
        return text[:start_pos].strip()

    return text


def clean_ocr_output(text):
    """æ¸…ç† OCR è¼¸å‡º"""
    if not text:
        return ""

    text = re.sub(r'\\\(([^)]*)\\\)', r'\1', text)
    text = re.sub(r'\\mathrm\s*\{([^}]*)\}', r'\1', text)
    text = re.sub(r'\\text\s*\{([^}]*)\}', r'\1', text)
    text = re.sub(r'\\textbf\s*\{([^}]*)\}', r'\1', text)
    text = re.sub(r'\\sim', '~', text)
    text = re.sub(r'\\times', 'Ã—', text)
    text = re.sub(r'\\pm', 'Â±', text)
    text = re.sub(r'\\leq', 'â‰¤', text)
    text = re.sub(r'\\geq', 'â‰¥', text)

    greek_letters = {
        r'\\alpha': 'Î±', r'\\beta': 'Î²', r'\\gamma': 'Î³', r'\\delta': 'Î´',
        r'\\mu': 'Î¼', r'\\sigma': 'Ïƒ', r'\\omega': 'Ï‰', r'\\pi': 'Ï€',
    }
    for latex, char in greek_letters.items():
        text = re.sub(latex, char, text)
    text = re.sub(r'\\([a-zA-Z]+)', r'\1', text)

    text = re.sub(r'<sup>([^<]*)</sup>', r'^\1', text)
    text = re.sub(r'<sub>([^<]*)</sub>', r'_\1', text)

    # ç§»é™¤ HTML æ¨™ç±¤
    text = re.sub(r'<table>.*?</table>', '', text, flags=re.DOTALL)
    text = re.sub(r'<[^>]+>', '', text)

    text = re.sub(r'\n{3,}', '\n\n', text)
    text = re.sub(r' {2,}', ' ', text)

    # ğŸ†• ç§»é™¤åƒè€ƒæ–‡ç»å€å¡Š
    text = remove_references_section(text)

    return text.strip()


def extract_text_from_html_table(html_content):
    """å¾ HTML è¡¨æ ¼ä¸­æå–ç´”æ–‡å­—"""
    if not html_content:
        return ""

    from html import unescape

    text = re.sub(r'<br\s*/?>', ' ', html_content)
    text = re.sub(r'<sup>([^<]*)</sup>', r'^\1', text)
    text = re.sub(r'<sub>([^<]*)</sub>', r'_\1', text)
    text = re.sub(r'</tr>', '\n', text)
    text = re.sub(r'</td>|</th>', ' | ', text)
    text = re.sub(r'<[^>]+>', '', text)
    text = unescape(text)

    lines = [line.strip() for line in text.split('\n') if line.strip()]
    text = '\n'.join(lines)

    return text.strip()


def merge_short_pages(page_results, min_length=30):
    """å°‡å­—æ•¸éçŸ­çš„é é¢èˆ‡å‰å¾Œé é¢åˆä½µ"""
    if not page_results:
        return []

    merged = []
    i = 0
    while i < len(page_results):
        page_num, content = page_results[i]
        content_len = len(content.strip())

        if content_len >= min_length:
            merged.append((page_num, content))
            i += 1
            continue

        if i + 1 < len(page_results):
            next_page_num, next_content = page_results[i + 1]
            combined = f"{content}\n\n{next_content}"
            merged.append((page_num, combined))
            i += 2
        elif merged:
            prev_page_num, prev_content = merged[-1]
            combined = f"{prev_content}\n\n{content}"
            merged[-1] = (prev_page_num, combined)
            i += 1
        else:
            merged.append((page_num, content))
            i += 1
    return merged


# ==================== çµ±ä¸€é€²éšè¼‰å…¥å™¨é¡åˆ¥ ====================
class UnifiedAdvancedLoader:
    def __init__(self, db_connection_string, pdf_base_path, jsonl_folder_path=None):
        self.db_connection_string = db_connection_string
        self.pdf_base_path = Path(pdf_base_path)
        self.jsonl_folder_path = Path(jsonl_folder_path) if jsonl_folder_path else None
        self.embeddings = embeddings

        # æ–‡å­—åˆ†å‰²å™¨
        self.text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
            chunk_size=1200,
            chunk_overlap=200
        )

        # è³‡æ–™å¤¾æ˜ å°„
        self.folder_mapping = get_reference_mapping()

        # OCR æ¨¡å‹
        self.ocr_model = None
        self.ocr_tokenizer = None

        # çµ±è¨ˆè³‡æ–™
        self.processing_stats = {
            'pdf_processed': [],
            'pdf_failed': [],
            'ocr_stats': []
        }

        print("âœ… çµ±ä¸€é€²éšè¼‰å…¥å™¨åˆå§‹åŒ–å®Œæˆ")

    def _load_ocr_model(self):
        """è¼‰å…¥ OCR æ¨¡å‹"""
        if self.ocr_model is None:
            print("\nğŸ“¥ è¼‰å…¥ OCR æ¨¡å‹...")
            try:
                self.ocr_tokenizer = AutoTokenizer.from_pretrained(
                    OCR_MODEL_PATH,
                    trust_remote_code=True
                )
                self.ocr_model = AutoModel.from_pretrained(
                    OCR_MODEL_PATH,
                    trust_remote_code=True,
                    use_safetensors=True,
                    torch_dtype=torch.bfloat16
                ).eval().cuda()
                print("âœ… OCR æ¨¡å‹è¼‰å…¥å®Œæˆ\n")
            except Exception as e:
                print(f"âŒ OCR æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
                raise

    def get_english_collection_name(self, folder_name):
        """å–å¾—è‹±æ–‡ collection åç¨±"""
        return self.folder_mapping.get(
            folder_name,
            f"unknown_{folder_name.lower().replace(' ', '_')}"
        )

    # ==================== OCR PDF è™•ç†ï¼ˆé€²éšç‰ˆï¼‰====================
    def ocr_pdf_advanced(self, pdf_path, folder_name, table_output_dir):
        """
        ä½¿ç”¨é€²éš OCR è™•ç† PDFï¼ŒåŒ…å«è¡¨æ ¼/åœ–ç‰‡æ“·å–
        ä¾†è‡ª update_pdf_to_db_v2.py çš„å®Œæ•´åŠŸèƒ½
        """
        pdf_name = Path(pdf_path).stem
        print(f"\n{'='*70}")
        print(f"ğŸ“„ OCR è™•ç†: {pdf_name}.pdf")
        print(f"{'='*70}")

        start_time = time.time()
        page_results = []
        tables_extracted = []

        # ğŸ†• ç‹€æ…‹è¿½è¸ªï¼šæ˜¯å¦å·²é€²å…¥åƒè€ƒæ–‡ç»å€å¡Š
        in_references_section = False

        try:
            doc = fitz.open(str(pdf_path))
            total_pages = len(doc)
            print(f"å…± {total_pages} é ")

            with tempfile.TemporaryDirectory() as tmp_dir:
                for page_num in range(1, total_pages + 1):
                    i = page_num - 1
                    print(f"  ç¬¬ {page_num}/{total_pages} é ...", end=' ', flush=True)

                    try:
                        page = doc[i]
                        page_width = page.rect.width
                        page_height = page.rect.height

                        # OCR é è¦½åœ–
                        ocr_scale = OCR_IMAGE_SIZE / max(page_width, page_height)
                        ocr_mat = fitz.Matrix(ocr_scale, ocr_scale)
                        ocr_pix = page.get_pixmap(matrix=ocr_mat)
                        ocr_width = ocr_pix.width
                        ocr_height = ocr_pix.height

                        img_path = os.path.join(tmp_dir, f"page_{page_num}.png")
                        ocr_pix.save(img_path)

                        result_file = os.path.join(tmp_dir, "result.mmd")
                        if os.path.exists(result_file):
                            os.remove(result_file)

                        # åŸ·è¡Œ OCR æ¨¡å‹
                        old_stdout, captured_output = capture_model_output()
                        self.ocr_model.infer(
                            self.ocr_tokenizer,
                            prompt="<image>\n<|grounding|>Convert the document to markdown.",
                            image_file=img_path,
                            output_path=tmp_dir,
                            base_size=OCR_IMAGE_SIZE,
                            image_size=OCR_IMAGE_SIZE,
                            crop_mode=False,
                            save_results=True,
                            test_compress=True
                        )
                        restore_stdout(old_stdout)
                        captured_text = captured_output.getvalue()

                        # è™•ç†æ–‡å­—
                        page_text_content = ""

                        # ğŸ†• å¦‚æœå·²é€²å…¥åƒè€ƒæ–‡ç»å€å¡Šï¼Œè·³éå¾ŒçºŒé é¢çš„æ–‡å­—è™•ç†
                        if in_references_section:
                            print(f"â­ï¸ è·³éï¼ˆåƒè€ƒæ–‡ç»å€å¡Šï¼‰", end=' ', flush=True)
                        elif os.path.exists(result_file):
                            with open(result_file, 'r', encoding='utf-8') as f:
                                raw_text = f.read()

                                # ğŸ†• å…ˆæª¢æŸ¥æ˜¯å¦é€²å…¥åƒè€ƒæ–‡ç»å€å¡Šï¼ˆåœ¨æ¸…ç†ä¹‹å‰ï¼‰
                                is_ref, ref_start = is_references_section(raw_text)
                                if is_ref:
                                    in_references_section = True
                                    print(f"ğŸ“š æª¢æ¸¬åˆ°åƒè€ƒæ–‡ç»å€å¡Š", end=' ', flush=True)

                                    # å¦‚æœåƒè€ƒæ–‡ç»å¾é é¢ä¸­é–“é–‹å§‹ï¼Œä¿ç•™ä¹‹å‰çš„å…§å®¹
                                    if ref_start and ref_start > 0:
                                        partial_text = raw_text[:ref_start]
                                        cleaned = clean_ocr_output(partial_text)
                                        if cleaned.strip():
                                            page_text_content = cleaned
                                    # å¦‚æœæ•´é éƒ½æ˜¯åƒè€ƒæ–‡ç»ï¼Œä¸è™•ç†
                                else:
                                    cleaned = clean_ocr_output(raw_text)
                                    if cleaned.strip():
                                        page_text_content = cleaned

                        # è™•ç†è¡¨æ ¼èˆ‡åœ–ç‰‡
                        object_count = 0
                        tables_text_content = []

                        # ğŸ†• å¦‚æœå·²é€²å…¥åƒè€ƒæ–‡ç»å€å¡Šï¼Œä¹Ÿè·³éè¡¨æ ¼è™•ç†
                        if not in_references_section:
                            raw_result = extract_from_stdout(captured_text)
                            if raw_result:
                                objects_data, captions = extract_tables_and_images_from_result(raw_result)

                                # åˆä½µé‡ç–Š
                                if objects_data:
                                    objects_data = merge_overlapping_objects(objects_data, overlap_threshold=0.3)

                                if objects_data:
                                    # ç”Ÿæˆé«˜è§£æåº¦åœ–ç‰‡
                                    high_res_scale = HIGH_RES_DPI / 72.0
                                    high_res_mat = fitz.Matrix(high_res_scale, high_res_scale)
                                    high_res_pix = page.get_pixmap(matrix=high_res_mat)
                                    high_res_width = high_res_pix.width
                                    high_res_height = high_res_pix.height
                                    high_res_img_data = high_res_pix.tobytes("png")
                                    high_res_image = Image.open(BytesIO(high_res_img_data))

                                    scale_x = high_res_width / ocr_width
                                    scale_y = high_res_height / ocr_height

                                    for idx, obj in enumerate(objects_data, 1):
                                        bbox = obj['bbox']
                                        obj_type = obj['type']
                                        title = find_related_caption(bbox, obj_type, captions)

                                        if SKIP_FIRST_PAGE_IMAGES and page_num == 1 and obj_type != 'table':
                                            continue

                                        # åº§æ¨™æ›ç®—
                                        x1_hr = int(bbox[0] * scale_x)
                                        y1_hr = int(bbox[1] * scale_y)
                                        x2_hr = int(bbox[2] * scale_x)
                                        y2_hr = int(bbox[3] * scale_y)
                                        obj_width = x2_hr - x1_hr
                                        obj_height = y2_hr - y1_hr

                                        if obj_type != 'table':
                                            if obj_width < MIN_IMAGE_SIZE or obj_height < MIN_IMAGE_SIZE:
                                                continue

                                        # å…¨å¯¬åº¦è£åˆ‡
                                        x1_crop = 0
                                        x2_crop = high_res_width
                                        padding_top = max(int(obj_height * 0.25), 50)
                                        padding_bottom = max(int(obj_height * 0.25), 50)
                                        y1_crop = max(0, y1_hr - padding_top)
                                        y2_crop = min(high_res_height, y2_hr + padding_bottom)

                                        obj_img = high_res_image.crop((x1_crop, y1_crop, x2_crop, y2_crop))

                                        type_abbr = {'table': 't', 'image': 'i', 'figure': 'f', 'chart': 'c', 'diagram': 'd'}
                                        abbr = type_abbr.get(obj_type, 'o')

                                        # å„²å­˜åœ–ç‰‡åˆ°å…¨å±€çµ±ä¸€çš„ images è³‡æ–™å¤¾
                                        # çµæ§‹: EXTRACTED_TABLES_BASE_DIR/images/
                                        global_images_dir = os.path.join(EXTRACTED_TABLES_BASE_DIR, "images")
                                        os.makedirs(global_images_dir, exist_ok=True)

                                        jpg_filename = f"{pdf_name}_p{page_num}_{abbr}{idx}.jpg"
                                        jpg_path = os.path.join(global_images_dir, jpg_filename)
                                        obj_img.save(jpg_path, "JPEG", quality=100)

                                        # å°åœ–ç‰‡é€²è¡Œ OCR
                                        image_ocr_text = ""
                                        if obj_type in ['image', 'figure', 'chart', 'diagram']:
                                            try:
                                                import io as io_module
                                                old_img_stdout = sys.stdout
                                                sys.stdout = io_module.StringIO()
                                                self.ocr_model.infer(
                                                    self.ocr_tokenizer,
                                                    prompt="<image>\nExtract all text from this image.",
                                                    image_file=jpg_path,
                                                    output_path=tmp_dir,
                                                    base_size=1024,
                                                    image_size=1024,
                                                    crop_mode=False,
                                                    save_results=True,
                                                    test_compress=False
                                                )
                                                sys.stdout = old_img_stdout

                                                result_files = [f for f in os.listdir(tmp_dir) if f.endswith('.mmd')]
                                                if result_files:
                                                    latest_result = max([os.path.join(tmp_dir, f) for f in result_files],
                                                                      key=os.path.getmtime)
                                                    with open(latest_result, 'r', encoding='utf-8') as rf:
                                                        image_ocr_text = clean_ocr_output(rf.read())
                                                    os.remove(latest_result)
                                            except Exception:
                                                pass

                                        # æº–å‚™ HTML (æ”¾åœ¨èˆ‡ images ç›¸åŒçš„ç›®éŒ„)
                                        html_filename = f"{pdf_name}_p{page_num}_{abbr}{idx}.html"
                                        html_path = os.path.join(global_images_dir, html_filename)

                                        ocr_text_content = ""
                                        if obj_type == 'table' and obj.get('html'):
                                            ocr_text_content = extract_text_from_html_table(obj['html'])
                                        elif image_ocr_text:
                                            ocr_text_content = image_ocr_text

                                        # åœ¨ OCR æ–‡å­—å‰åŠ å…¥æª”æ¡ˆåç¨±ï¼Œæå‡æª¢ç´¢æº–ç¢ºåº¦
                                        if ocr_text_content:
                                            ocr_text_content_with_filename = f"æª”æ¡ˆ: {jpg_filename}\n\n{ocr_text_content}"
                                        else:
                                            ocr_text_content_with_filename = f"æª”æ¡ˆ: {jpg_filename}"

                                        with open(html_path, 'w', encoding='utf-8') as f:
                                            if title:
                                                f.write(f"<h2>{title}</h2>\n")
                                            f.write(f'<div class="{obj_type}">\n')
                                            f.write(f'    <img src="{jpg_filename}" alt="{title or obj_type}" style="max-width: 100%;">\n')
                                            if ocr_text_content_with_filename:
                                                f.write(f'    <p class="ocr-text">{ocr_text_content_with_filename}</p>\n')
                                            f.write(f'</div>\n')

                                        # æ§‹å»ºå‘é‡è³‡æ–™åº«å…§å®¹ï¼ˆä½¿ç”¨åŒ…å«æª”æ¡ˆåç¨±çš„ç‰ˆæœ¬ï¼‰
                                        section_title = title if title else f"{obj_type} #{idx}"
                                        db_content_block = f"\n## {section_title}\n"

                                        if ocr_text_content:
                                            # ä½¿ç”¨åŒ…å«æª”æ¡ˆåç¨±çš„ OCR æ–‡å­—
                                            db_content_block += ocr_text_content_with_filename
                                        else:
                                            db_content_block += f"æª”æ¡ˆ: {jpg_filename}\n\n(æ­¤å€å¡ŠåŒ…å«åœ–ç‰‡/åœ–è¡¨è³‡æ–™ï¼Œè«‹åƒè€ƒé™„ä»¶)"

                                        tables_text_content.append(db_content_block)

                                        tables_extracted.append({
                                            'page': page_num,
                                            'object_idx': idx,
                                            'object_type': obj_type,
                                            'title': title,
                                            'jpg_path': jpg_path,
                                            'html_path': html_path
                                        })
                                        object_count += 1

                        # åˆä½µé é¢å…§å®¹
                        final_page_content = page_text_content
                        if tables_text_content:
                            final_page_content += "\n" + "\n".join(tables_text_content)

                        if final_page_content.strip():
                            page_results.append((page_num, final_page_content))
                            print(f"âœ“ ({len(final_page_content)}å­— + {object_count}ç‰©ä»¶)", flush=True)
                        else:
                            print("âš  ç„¡å…§å®¹", flush=True)

                        cleanup_gpu()

                    except Exception as e:
                        print(f"âœ— éŒ¯èª¤: {e}", flush=True)
                        continue

            doc.close()

            if page_results:
                page_results = merge_short_pages(page_results, min_length=30)

            elapsed = time.time() - start_time
            print(f"âœ… å®Œæˆï¼è€—æ™‚ {elapsed:.1f} ç§’")

            # è¨˜éŒ„çµ±è¨ˆ
            self.processing_stats['ocr_stats'].append({
                'file': pdf_path.name,
                'folder': folder_name,
                'pages': len(page_results),
                'objects': len(tables_extracted),
                'time': elapsed
            })

            return page_results, tables_extracted

        except Exception as e:
            print(f"âœ— OCR å¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
            return [], []

    # ==================== JSONL è™•ç† ====================
    def load_jsonl_files(self):
        """è¼‰å…¥ JSONL å•ç­”è³‡æ–™ï¼ˆä¸åˆ†å‰²ï¼‰"""
        if not self.jsonl_folder_path or not self.jsonl_folder_path.exists():
            print("âš ï¸ JSONL è³‡æ–™å¤¾æœªè¨­å®šæˆ–ä¸å­˜åœ¨")
            return []

        jsonl_files = list(self.jsonl_folder_path.glob("*.jsonl"))
        if not jsonl_files:
            print("âš ï¸ æ²’æœ‰æ‰¾åˆ° JSONL æª”æ¡ˆ")
            return []

        documents = []
        seen_contents = set()

        for file_path in jsonl_files:
            print(f"è¼‰å…¥ JSONL: {file_path.name}")
            with open(file_path, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    if not line:
                        continue

                    try:
                        data = json.loads(line)

                        question_text = data.get('question', '').strip()
                        answer_text = data.get('text', '').strip()
                        original_full_text = data.get('text', '').strip()

                        # è§£ææ ¼å¼
                        if not question_text and answer_text:
                            if 'å•é¡Œ:' in answer_text and 'ç­”æ¡ˆ:' in answer_text:
                                parts = answer_text.split('\n')
                                for part in parts:
                                    if part.strip().startswith('å•é¡Œ:'):
                                        question_text = part.replace('å•é¡Œ:', '').strip()
                                    elif part.strip().startswith('ç­”æ¡ˆ:'):
                                        answer_text = part.replace('ç­”æ¡ˆ:', '').strip()
                        elif question_text and not ('å•é¡Œ:' in original_full_text):
                            original_full_text = f"å•é¡Œ: {question_text}\nç­”æ¡ˆ: {answer_text}"

                        if not question_text or not answer_text:
                            continue

                        # å»é‡
                        content_key = answer_text[:200]
                        if content_key in seen_contents:
                            continue
                        seen_contents.add(content_key)

                        # å»ºç«‹æ–‡æª”ï¼ˆåªåµŒå…¥å•é¡Œï¼‰
                        metadata = data.get('metadata', {})
                        content_parts = [question_text]
                        if metadata.get('keywords'):
                            content_parts.append(f"é—œéµå­—: {metadata['keywords']}")
                        if metadata.get('reference'):
                            content_parts.append(f"åƒè€ƒ: {metadata['reference']}")

                        page_content = "\n".join(content_parts)
                        doc = Document(
                            page_content=page_content,
                            metadata={
                                "id": data.get('id', f"{file_path.stem}_{line_num}"),
                                "source_file": metadata.get('source_file', file_path.name),
                                "sheet_name": metadata.get('sheet_name', ''),
                                "title": question_text,
                                "keywords": metadata.get('keywords', ''),
                                "reference": metadata.get('reference', ''),
                                "original_text": answer_text,
                                "original_full_text": original_full_text,
                                "source_type": "jsonl"
                            }
                        )
                        documents.append(doc)

                    except Exception as e:
                        print(f"  JSONL {file_path.name} ç¬¬ {line_num} è¡ŒéŒ¯èª¤: {e}")
                        continue

        print(f"âœ… ç¸½å…±è¼‰å…¥ {len(documents)} ç­† JSONL è³‡æ–™")
        return documents

    # ==================== PDF è³‡æ–™å¤¾è™•ç† ====================
    def load_pdf_folder(self, folder_path):
        """è™•ç†å–®ä¸€ PDF è³‡æ–™å¤¾ï¼ˆä½¿ç”¨é€²éš OCRï¼‰"""
        pdf_files = list(folder_path.glob("*.pdf"))
        if not pdf_files:
            return []

        print(f"\næ‰¾åˆ° {len(pdf_files)} å€‹ PDF æª”æ¡ˆ")
        all_docs = []

        for pdf_file in pdf_files:
            try:
                print(f"\næ­£åœ¨è™•ç†: {pdf_file.name}")

                # ä½¿ç”¨é€²éš OCR è™•ç†ï¼ˆä¸éœ€è¦å»ºç«‹å€‹åˆ¥è³‡æ–™å¤¾ï¼Œåœ–ç‰‡çµ±ä¸€å­˜åœ¨ images è³‡æ–™å¤¾ï¼‰
                page_results, tables_extracted = self.ocr_pdf_advanced(
                    pdf_file,
                    folder_path.name,
                    EXTRACTED_TABLES_BASE_DIR  # åªå‚³éåŸºç¤è·¯å¾‘
                )

                if not page_results:
                    print(f"  âš ï¸ ç„¡æœ‰æ•ˆå…§å®¹")
                    self.processing_stats['pdf_failed'].append({
                        'file': pdf_file.name,
                        'folder': folder_path.name,
                        'error': 'ç„¡æœ‰æ•ˆå…§å®¹'
                    })
                    continue

                # å»ºç«‹æ˜ å°„
                page_table_map = {}
                for table_info in tables_extracted:
                    page_num = table_info['page']
                    if page_num not in page_table_map:
                        page_table_map[page_num] = []
                    page_table_map[page_num].append(os.path.basename(table_info['jpg_path']))

                # å»ºç«‹æ–‡æª”
                for page_num, page_content in page_results:
                    table_images = page_table_map.get(page_num, [])
                    has_table = (len(table_images) > 0) or ('<table>' in page_content or '|' in page_content)

                    doc = Document(
                        page_content=page_content,
                        metadata={
                            'source_file': pdf_file.name,
                            'source_type': 'ocr_pdf_advanced',
                            'page': page_num,
                            'original_text': page_content,
                            'folder': folder_path.name,
                            'page_label': str(page_num),
                            'has_table': has_table,
                            'table_images': table_images,
                            'pdf_file': pdf_file.name
                        }
                    )
                    all_docs.append(doc)

                print(f"  âœ… æˆåŠŸè™•ç† {len(page_results)} é ï¼Œ{len(tables_extracted)} å€‹ç‰©ä»¶")
                self.processing_stats['pdf_processed'].append({
                    'file': pdf_file.name,
                    'folder': folder_path.name,
                    'pages': len(page_results),
                    'objects': len(tables_extracted)
                })

            except Exception as e:
                print(f"  âŒ è™•ç†å¤±æ•—: {pdf_file.name}")
                print(f"     éŒ¯èª¤: {str(e)[:200]}")
                self.processing_stats['pdf_failed'].append({
                    'file': pdf_file.name,
                    'folder': folder_path.name,
                    'error': str(e)[:200]
                })
                continue

        return all_docs

    # ==================== çµ±ä¸€è¼‰å…¥ ====================
    def load_all_data_unified(self, pdf_batch_size=20, jsonl_batch_size=50):
        """çµ±ä¸€è¼‰å…¥æ‰€æœ‰è³‡æ–™ï¼ˆPDF + JSONLï¼‰"""
        # é‡ç½®çµ±è¨ˆ
        self.processing_stats = {
            'pdf_processed': [],
            'pdf_failed': [],
            'ocr_stats': []
        }

        print(f"\n{'='*80}")
        print("ğŸš€ çµ±ä¸€é€²éšè¼‰å…¥é–‹å§‹")
        print(f"{'='*80}")

        # è¼‰å…¥ OCR æ¨¡å‹ï¼ˆPDF éœ€è¦ï¼‰
        self._load_ocr_model()

        total_collections = 0

        # === 1. è¼‰å…¥ JSONL ===
        jsonl_docs = self.load_jsonl_files()
        if jsonl_docs:
            cleaned_jsonl = [self._validate_and_clean_document(doc) for doc in jsonl_docs]
            cleaned_jsonl = [d for d in cleaned_jsonl if d is not None]

            if cleaned_jsonl:
                saved = self._batch_save_to_vectordb(
                    cleaned_jsonl,
                    "medical_knowledge_base",
                    jsonl_batch_size
                )
                if saved > 0:
                    total_collections += 1
                    print(f"âœ… JSONL å·²å„²å­˜è‡³ 'medical_knowledge_base' ({saved} ç­†)")
        else:
            print("â­ï¸ è·³é JSONL è¼‰å…¥")

        # === 2. è¼‰å…¥ PDF è³‡æ–™å¤¾ï¼ˆé€²éšæ¨¡å¼ï¼‰===
        pdf_folders = [d for d in self.pdf_base_path.iterdir() if d.is_dir()]
        if pdf_folders:
            print(f"\nğŸ“ æ‰¾åˆ° {len(pdf_folders)} å€‹ PDF è³‡æ–™å¤¾")

            for i, folder in enumerate(sorted(pdf_folders), 1):
                print(f"\n{'='*70}")
                print(f"è™•ç†è³‡æ–™å¤¾ {i}/{len(pdf_folders)}: {folder.name}")
                print(f"{'='*70}")

                pdf_docs = self.load_pdf_folder(folder)
                if not pdf_docs:
                    print(f"  â­ï¸ è·³éï¼ˆç„¡æœ‰æ•ˆå…§å®¹ï¼‰")
                    continue

                # åˆ†å‰²æ–‡æª”
                final_docs = []
                for doc in pdf_docs:
                    splits = self.text_splitter.split_documents([doc])
                    final_docs.extend(splits)

                # å„²å­˜åˆ°è³‡æ–™åº«
                collection_name = self.get_english_collection_name(folder.name)
                saved = self._batch_save_to_vectordb(
                    final_docs,
                    collection_name,
                    pdf_batch_size
                )

                if saved > 0:
                    total_collections += 1
                    print(f"  âœ… å·²å„²å­˜ {saved} å€‹ç‰‡æ®µè‡³ '{collection_name}'")

                cleanup_gpu()
        else:
            print("â­ï¸ ç„¡ PDF è³‡æ–™å¤¾")

        print(f"\n{'='*80}")
        print(f"ğŸ‰ çµ±ä¸€è¼‰å…¥å®Œæˆï¼ç¸½å…±å»ºç«‹ {total_collections} å€‹ collections")
        print(f"{'='*80}")

        # é¡¯ç¤ºå ±å‘Š
        self.print_processing_report()

    # ==================== è¼”åŠ©æ–¹æ³• ====================
    def _validate_and_clean_document(self, doc):
        """é©—è­‰ä¸¦æ¸…ç†æ–‡æª”"""
        try:
            if not isinstance(doc.page_content, str):
                doc.page_content = str(doc.page_content) if doc.page_content else ""

            doc.page_content = doc.page_content.strip()
            if not doc.page_content or len(doc.page_content) < 10:
                return None

            # æ¸…ç† metadata
            if doc.metadata:
                cleaned_meta = {}
                for k, v in doc.metadata.items():
                    if v is None:
                        cleaned_meta[k] = ""
                    elif isinstance(v, (str, int, float, bool, list)):
                        cleaned_meta[k] = v
                    else:
                        cleaned_meta[k] = str(v)
                doc.metadata = cleaned_meta

            return doc
        except Exception as e:
            print(f"æ¸…ç†æ–‡æª”éŒ¯èª¤: {e}")
            return None

    def _batch_save_to_vectordb(self, documents, collection_name, batch_size):
        """æ‰¹æ¬¡å„²å­˜åˆ°å‘é‡è³‡æ–™åº«"""
        print(f"\nğŸ’¾ å„²å­˜ {len(documents)} ç­†è‡³ '{collection_name}'...")

        cleaned = [self._validate_and_clean_document(d) for d in documents]
        cleaned = [d for d in cleaned if d is not None]

        if not cleaned:
            print("âš ï¸ ç„¡æœ‰æ•ˆæ–‡æª”")
            return 0

        total_saved = 0
        for i in range(0, len(cleaned), batch_size):
            batch = cleaned[i:i+batch_size]
            try:
                if i == 0:
                    PGVector.from_documents(
                        documents=batch,
                        embedding=self.embeddings,
                        connection=self.db_connection_string,
                        collection_name=collection_name,
                        pre_delete_collection=True,
                    )
                else:
                    vs = PGVector(
                        embeddings=self.embeddings,
                        connection=self.db_connection_string,
                        collection_name=collection_name,
                    )
                    vs.add_documents(batch)
                total_saved += len(batch)
                print(f"  æ‰¹æ¬¡ {i//batch_size + 1}: âœ“ ({len(batch)} ç­†)")
            except Exception as e:
                print(f"  æ‰¹æ¬¡ {i//batch_size + 1}: âœ— éŒ¯èª¤: {str(e)[:100]}")
                continue

        return total_saved

    def print_processing_report(self):
        """é¡¯ç¤ºè™•ç†å ±å‘Š"""
        print(f"\n{'='*80}")
        print("ğŸ“Š è™•ç†å ±å‘Š")
        print(f"{'='*80}")

        # OCR çµ±è¨ˆ
        if self.processing_stats['ocr_stats']:
            total_time = sum(s['time'] for s in self.processing_stats['ocr_stats'])
            total_pages = sum(s['pages'] for s in self.processing_stats['ocr_stats'])
            total_objects = sum(s['objects'] for s in self.processing_stats['ocr_stats'])

            print(f"\nğŸ” OCR çµ±è¨ˆ ({len(self.processing_stats['ocr_stats'])} å€‹ PDF):")
            print("-" * 80)
            for item in self.processing_stats['ocr_stats']:
                print(f"  ğŸ“„ {item['folder']}/{item['file']}")
                print(f"     é æ•¸: {item['pages']}, ç‰©ä»¶: {item['objects']}, è€—æ™‚: {item['time']:.1f}ç§’")
            print(f"\n  ç¸½è¨ˆ: {total_pages} é , {total_objects} ç‰©ä»¶, {total_time:.1f} ç§’")

        # æˆåŠŸè™•ç†
        if self.processing_stats['pdf_processed']:
            print(f"\nâœ… æˆåŠŸè™•ç† ({len(self.processing_stats['pdf_processed'])} å€‹ PDF):")
            print("-" * 80)
            for item in self.processing_stats['pdf_processed']:
                print(f"  {item['folder']}/{item['file']}: {item['pages']}é , {item['objects']}ç‰©ä»¶")

        # å¤±æ•—è¨˜éŒ„
        if self.processing_stats['pdf_failed']:
            print(f"\nâŒ è™•ç†å¤±æ•— ({len(self.processing_stats['pdf_failed'])} å€‹ PDF):")
            print("-" * 80)
            for item in self.processing_stats['pdf_failed']:
                print(f"  {item['folder']}/{item['file']}")
                print(f"     éŒ¯èª¤: {item['error']}")

        print(f"\n{'='*80}\n")

    # ==================== è³‡æ–™åº«ç®¡ç† ====================
    def list_collections(self):
        """åˆ—å‡ºæ‰€æœ‰ collections"""
        try:
            conn = psycopg2.connect(self.db_connection_string)
            cursor = conn.cursor()
            cursor.execute("SELECT name FROM langchain_pg_collection ORDER BY name;")
            collections = [row[0] for row in cursor.fetchall()]
            print(f"\nå·²å»ºç«‹çš„ Collections ({len(collections)} å€‹):")
            for i, name in enumerate(collections, 1):
                print(f"{i:2d}. {name}")
            cursor.close()
            conn.close()
        except Exception as e:
            print(f"åˆ—å‡º collections å¤±æ•—: {e}")

    def reset_database(self):
        """é‡ç½®è³‡æ–™åº«"""
        try:
            conn = psycopg2.connect(self.db_connection_string)
            conn.set_isolation_level(ISOLATION_LEVEL_AUTOCOMMIT)
            cursor = conn.cursor()
            confirm = input("âš ï¸ è¼¸å…¥ 'DELETE' ç¢ºèªé‡ç½®è³‡æ–™åº«: ").strip()
            if confirm == 'DELETE':
                for table in ['langchain_pg_embedding', 'langchain_pg_collection']:
                    cursor.execute(f"DROP TABLE IF EXISTS {table} CASCADE;")
                    print(f"å·²åˆªé™¤ {table}")
                print("âœ… è³‡æ–™åº«é‡ç½®å®Œæˆ")
            cursor.close()
            conn.close()
        except Exception as e:
            print(f"é‡ç½®å¤±æ•—: {e}")


# ==================== ä¸»ç¨‹å¼ ====================
def main():
    os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

    db_connection_string = f"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}"
    PDF_DATA_PATH = "/home/danny/AI-agent/DataSet"
    JSONL_DATA_PATH = "/home/danny/AI-agent/RAG_JSONL"

    print("\n" + "="*80)
    print("ğŸš€ çµ±ä¸€é€²éšè¼‰å…¥å™¨ (æ•´åˆç‰ˆ)")
    print("="*80)
    print("\nåŠŸèƒ½ç‰¹è‰²:")
    print("  âœ… JSONL å•ç­”è³‡æ–™è¼‰å…¥")
    print("  âœ… é€²éš OCR + è¡¨æ ¼/åœ–ç‰‡æ“·å–")
    print("  âœ… è‡ªå‹•å»ºç«‹å¤šå€‹ collection")
    print("  âœ… å®Œæ•´çš„è™•ç†å ±å‘Š")
    print("="*80)

    loader = UnifiedAdvancedLoader(
        db_connection_string,
        PDF_DATA_PATH,
        JSONL_DATA_PATH
    )

    while True:
        print("\né¸é …:")
        print("1. ğŸš€ çµ±ä¸€è¼‰å…¥æ‰€æœ‰è³‡æ–™ï¼ˆPDF + JSONLï¼‰")
        print("2. ğŸ“‹ åˆ—å‡ºæ‰€æœ‰ Collections")
        print("3. ğŸ§¹ é‡ç½®è³‡æ–™åº«")
        print("4. â›” é€€å‡º")

        choice = input("\nè«‹é¸æ“‡ (1-4): ").strip()

        if choice == '1':
            pdf_batch = int(input("PDF æ‰¹æ¬¡å¤§å° (é è¨­ 20): ") or "20")
            jsonl_batch = int(input("JSONL æ‰¹æ¬¡å¤§å° (é è¨­ 50): ") or "50")
            confirm = input("\nâš ï¸ ç¢ºå®šé–‹å§‹è¼‰å…¥ï¼Ÿé€™å°‡è¦†è“‹ç¾æœ‰è³‡æ–™ (y/N): ").strip().lower()
            if confirm == 'y':
                loader.load_all_data_unified(pdf_batch, jsonl_batch)

        elif choice == '2':
            loader.list_collections()

        elif choice == '3':
            loader.reset_database()

        elif choice == '4':
            print("ğŸ‘‹ çµæŸ")
            break

        else:
            print("âŒ ç„¡æ•ˆé¸é …")


if __name__ == "__main__":
    main()
