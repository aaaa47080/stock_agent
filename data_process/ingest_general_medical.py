import os
import json
from pathlib import Path
import sys
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
# æ·»åŠ ç•¶å‰ç›®éŒ„ä»¥ä¾¿å°å…¥åŒç´šæ¨¡çµ„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
from core.config import embeddings

from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_postgres import PGVector
from langchain_community.document_loaders import PyPDFLoader
import psycopg2
from psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT
from core.config import DB_HOST, DB_NAME, DB_PORT, DB_PASSWORD, DB_USER, embeddings, get_reference_mapping

# OCR ç›¸é—œå°å…¥
import tempfile
import time
import re
from PIL import Image
import io
try:
    import fitz  # PyMuPDF
    from transformers import AutoModel, AutoTokenizer
    import torch
    OCR_AVAILABLE = True
except ImportError:
    OCR_AVAILABLE = False
    print("âš ï¸ OCR æ¨¡çµ„æœªå®‰è£ï¼Œå°‡è·³éæƒæç‰ˆ PDF è™•ç†")

class UnifiedVectorDBLoader:
    def __init__(self, db_connection_string, pdf_base_path, jsonl_folder_path=None, enable_ocr=False, force_ocr_all=False, ocr_model_path=None):
        self.db_connection_string = db_connection_string
        self.pdf_base_path = Path(pdf_base_path)
        self.jsonl_folder_path = Path(jsonl_folder_path) if jsonl_folder_path else None
        self.embeddings = embeddings

        # PDF ä½¿ç”¨è‡ªå®šç¾©å¥å­åˆ†å‰²å™¨ï¼ˆä»¥å¥è™Ÿç‚ºåˆ†å‰²é»ï¼Œä¿æŒå¥å­å®Œæ•´æ€§ï¼‰
        # ğŸ”§ å¢åŠ  chunk_size å’Œ overlap é¿å…è·¨é å…§å®¹è¢«åˆ‡æ•£
        self.pdf_text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
            chunk_size=1024,  # ğŸ‘ˆ å¾ 512 å¢åŠ åˆ° 1024
            chunk_overlap=256,  # ğŸ‘ˆ å¾ 50 å¢åŠ åˆ° 200 é¿å…è·¨é å…§å®¹è¢«åˆ‡æ•£
            separators=[
                "\n\n",   # æ®µè½åˆ†éš”ï¼ˆå„ªå…ˆï¼‰
                "ã€‚\n",   # å¥è™Ÿ+æ›è¡Œ
                "ã€‚ ",    # å¥è™Ÿ+ç©ºæ ¼
                "ï¼\n",   # é©šå˜†è™Ÿ+æ›è¡Œ
                "ï¼ ",    # é©šå˜†è™Ÿ+ç©ºæ ¼
                "ï¼Ÿ\n",   # å•è™Ÿ+æ›è¡Œ
                "ï¼Ÿ ",    # å•è™Ÿ+ç©ºæ ¼
                "\n",     # æ›è¡Œ
                " ",      # ç©ºæ ¼
                ""        # å­—ç¬¦ï¼ˆæœ€å¾Œæ‰‹æ®µï¼‰
            ],
            keep_separator=True  # ä¿ç•™åˆ†éš”ç¬¦ï¼Œå¥è™Ÿæœƒç•™åœ¨å‰ä¸€å€‹chunkæœ«å°¾
        )

        self.folder_mapping = get_reference_mapping()

        # ç”¨æ–¼è¨˜éŒ„è™•ç†ç‹€æ…‹
        self.empty_pdfs = []  # è¨˜éŒ„ç©ºç™½PDF
        self.failed_pdfs = []  # è¨˜éŒ„è™•ç†å¤±æ•—çš„PDF
        self.ocr_processed_pdfs = []  # è¨˜éŒ„ç¶“éOCRè™•ç†çš„PDF

        # OCR è¨­å®š
        self.enable_ocr = enable_ocr and OCR_AVAILABLE
        self.force_ocr_all = force_ocr_all and OCR_AVAILABLE  # å¼·åˆ¶æ‰€æœ‰ PDF ä½¿ç”¨ OCR
        self.ocr_model = None
        self.ocr_tokenizer = None
        self.ocr_model_path = ocr_model_path or '/home/danny/AI-agent/deepseek_ocr'

        if self.force_ocr_all:
            if not OCR_AVAILABLE:
                print("âš ï¸ OCR æ¨¡çµ„æœªå®‰è£ï¼Œç„¡æ³•ä½¿ç”¨å¼·åˆ¶ OCR æ¨¡å¼")
                self.force_ocr_all = False
                self.enable_ocr = False
            else:
                print("âœ“ å¼·åˆ¶ OCR æ¨¡å¼ï¼šæ‰€æœ‰ PDF éƒ½å°‡ä½¿ç”¨ OCR è™•ç†")
                self.enable_ocr = True
        elif self.enable_ocr:
            if not OCR_AVAILABLE:
                print("âš ï¸ OCR åŠŸèƒ½å·²å•Ÿç”¨ä½†ç¼ºå°‘å¿…è¦æ¨¡çµ„ï¼ŒOCR å°‡è¢«åœç”¨")
                self.enable_ocr = False
            else:
                print("âœ“ OCR åŠŸèƒ½å·²å•Ÿç”¨ï¼Œå°‡è‡ªå‹•è™•ç†æƒæç‰ˆ PDF")

        print("åˆå§‹åŒ–å®Œæˆ")

    def get_english_collection_name(self, folder_name):
        return self.folder_mapping.get(folder_name, f"unknown_{folder_name.lower().replace(' ', '_')}")

    # === OCR è¼”åŠ©å‡½æ•¸ ===
    def _load_ocr_model(self):
        """å»¶é²åŠ è¼‰ OCR æ¨¡å‹ï¼ˆåªåœ¨éœ€è¦æ™‚åŠ è¼‰ï¼‰"""
        if self.ocr_model is None and self.enable_ocr:
            print("\næ­£åœ¨åŠ è¼‰ OCR æ¨¡å‹...")
            os.environ["CUDA_VISIBLE_DEVICES"] = '0'
            try:
                self.ocr_tokenizer = AutoTokenizer.from_pretrained(
                    self.ocr_model_path,
                    trust_remote_code=True
                )
                self.ocr_model = AutoModel.from_pretrained(
                    self.ocr_model_path,
                    trust_remote_code=True,
                    use_safetensors=True,
                    torch_dtype=torch.bfloat16
                ).eval().cuda()
                print("âœ“ OCR æ¨¡å‹åŠ è¼‰å®Œæˆ\n")
            except Exception as e:
                print(f"âŒ OCR æ¨¡å‹åŠ è¼‰å¤±æ•—: {e}")
                self.enable_ocr = False

    def _pdf_to_images(self, pdf_path, dpi=200, max_dimension=2000):
        """å°‡ PDF è½‰æ›ç‚ºåœ–ç‰‡"""
        doc = fitz.open(str(pdf_path))
        images = []
        mat = fitz.Matrix(dpi / 72.0, dpi / 72.0)

        for page in doc:
            pix = page.get_pixmap(matrix=mat, alpha=False)
            img = Image.open(io.BytesIO(pix.tobytes("png"))).convert("RGB")

            width, height = img.size
            max_size = max(width, height)

            if max_size > max_dimension:
                scale = max_dimension / max_size
                new_width = int(width * scale)
                new_height = int(height * scale)
                img = img.resize((new_width, new_height), Image.LANCZOS)

            images.append(img)

        doc.close()
        return images

    def _clean_ocr_output(self, text):
        """æ¸…ç† OCR è¼¸å‡º"""
        if not text:
            return ""

        lines = []
        for line in text.split('\n'):
            stripped = line.strip()
            if not stripped.startswith(('<|ref|>', '<|det|>', '<|grounding|>')):
                lines.append(line)

        result = '\n'.join(lines)
        result = re.sub(r'\n{3,}', '\n\n', result)
        return result.strip()

    def _ocr_pdf(self, pdf_path, folder_name):
        """ä½¿ç”¨ OCR è™•ç†æƒæç‰ˆ PDF"""
        if not self.enable_ocr:
            return []

        self._load_ocr_model()
        if self.ocr_model is None:
            return []

        print(f"  ğŸ” ä½¿ç”¨ OCR è™•ç†æƒæç‰ˆ PDF: {pdf_path.name}")
        start_time = time.time()
        all_docs = []

        try:
            images = self._pdf_to_images(pdf_path)
            print(f"     å…± {len(images)} é ")

            with tempfile.TemporaryDirectory() as tmp_dir:
                for i, img in enumerate(images):
                    page_num = i + 1
                    print(f"     ç¬¬ {page_num}/{len(images)} é ...", end=' ', flush=True)

                    try:
                        img_path = os.path.join(tmp_dir, f"page_{page_num}.png")
                        img.save(img_path, "PNG")

                        result_file = os.path.join(tmp_dir, "result.mmd")
                        if os.path.exists(result_file):
                            os.remove(result_file)

                        # OCR æ¨è«–
                        self.ocr_model.infer(
                            self.ocr_tokenizer,
                            prompt="<image>\n<|grounding|>Convert the document to markdown.",
                            image_file=img_path,
                            output_path=tmp_dir,
                            base_size=1024,
                            image_size=640,
                            crop_mode=True,
                            save_results=True,
                            test_compress=True
                        )

                        if os.path.exists(result_file):
                            with open(result_file, 'r', encoding='utf-8') as f:
                                content = f.read()
                                cleaned = self._clean_ocr_output(content)

                                if cleaned.strip() and len(cleaned.strip()) >= 10:
                                    prefixed_content = f"[ä¾†æºæ–‡ä»¶: {pdf_path.name}]\n{cleaned}"
                                    all_docs.append(Document(
                                        page_content=prefixed_content,
                                        metadata={
                                            'pdf_file': pdf_path.name,
                                            'folder': folder_name,
                                            'source_type': 'ocr_pdf',
                                            'page': page_num,
                                            'original_text': cleaned
                                        }
                                    ))
                                    print(f"âœ“ ({len(cleaned)} å­—å…ƒ)", flush=True)
                                else:
                                    print("âš  å…§å®¹ç‚ºç©º", flush=True)
                        else:
                            print("âš  æœªç”¢ç”Ÿçµæœ", flush=True)

                    except Exception as e:
                        print(f"âœ— éŒ¯èª¤: {str(e)[:50]}", flush=True)
                        continue

            elapsed = time.time() - start_time
            if all_docs:
                print(f"     âœ… OCR å®Œæˆï¼è€—æ™‚ {elapsed:.1f} ç§’ï¼Œè™•ç† {len(all_docs)} é ")
                self.ocr_processed_pdfs.append({
                    'file': pdf_path.name,
                    'folder': folder_name,
                    'pages': len(all_docs),
                    'time': elapsed
                })
            else:
                print(f"     âš ï¸ OCR å®Œæˆä½†ç„¡æœ‰æ•ˆå…§å®¹")

        except Exception as e:
            print(f"     âŒ OCR å¤±æ•—: {str(e)[:100]}")

        return all_docs

    # === PDF è™•ç† ===
    def load_pdf_documents(self, folder_path):
        all_docs = []
        pdf_files = list(folder_path.glob("*.pdf"))
        if not pdf_files:
            return all_docs

        print(f"æ‰¾åˆ° {len(pdf_files)} å€‹PDFæ–‡ä»¶")
        for pdf_file in pdf_files:
            try:
                # å¦‚æœå¼·åˆ¶ä½¿ç”¨ OCRï¼Œç›´æ¥è·³éæ–‡å­—æå–
                if self.force_ocr_all:
                    print(f"æ­£åœ¨åŠ è¼‰: {pdf_file.name} [å¼·åˆ¶ OCR æ¨¡å¼]")
                    ocr_docs = self._ocr_pdf(pdf_file, folder_path.name)
                    if ocr_docs:
                        all_docs.extend(ocr_docs)
                    else:
                        print(f"  âš ï¸ OCR è™•ç†å¤±æ•—æˆ–ç„¡å…§å®¹")
                        self.failed_pdfs.append({
                            'file': pdf_file.name,
                            'folder': folder_path.name,
                            'error': 'OCR è™•ç†å¤±æ•—æˆ–ç„¡æœ‰æ•ˆå…§å®¹'
                        })
                    continue

                # æ™ºèƒ½æ¨¡å¼ï¼šå…ˆå˜—è©¦æå–æ–‡å­—
                print(f"æ­£åœ¨åŠ è¼‰: {pdf_file.name}")
                loader = PyPDFLoader(str(pdf_file))
                docs = loader.load()

                # é©—è­‰æ˜¯å¦æœ‰æœ‰æ•ˆå…§å®¹
                valid_docs = []
                total_text_length = 0

                for doc in docs:
                    # æ¸…ç†ä¸¦æª¢æŸ¥å…§å®¹
                    cleaned_content = self._clean_text(doc.page_content).strip()
                    content_without_whitespace = ''.join(cleaned_content.split())

                    # æª¢æŸ¥æ˜¯å¦æœ‰å¯¦è³ªå…§å®¹ï¼ˆè‡³å°‘10å€‹éç©ºç™½å­—ç¬¦ï¼‰
                    if len(content_without_whitespace) >= 10:
                        total_text_length += len(content_without_whitespace)
                        prefixed_content = f"[ä¾†æºæ–‡ä»¶: {pdf_file.name}]\n{cleaned_content}"
                        valid_docs.append(Document(
                            page_content=prefixed_content,
                            metadata={
                                **doc.metadata,
                                'pdf_file': pdf_file.name,
                                'folder': folder_path.name,
                                'source_type': 'pdf'
                            }
                        ))

                # æª¢æŸ¥æ•´å€‹PDFæ˜¯å¦æœ‰æ•ˆ
                if not valid_docs or total_text_length < 50:
                    print(f"  âš ï¸ è­¦å‘Š: {pdf_file.name} å…§å®¹ç‚ºç©ºæˆ–éå°‘ï¼ˆå¯èƒ½æ˜¯æƒæç‰ˆPDFï¼‰")

                    # å¦‚æœå•Ÿç”¨ OCRï¼Œå˜—è©¦ä½¿ç”¨ OCR è™•ç†
                    if self.enable_ocr:
                        ocr_docs = self._ocr_pdf(pdf_file, folder_path.name)
                        if ocr_docs:
                            all_docs.extend(ocr_docs)
                            # å¾ empty_pdfs è¨˜éŒ„ä¸­ç§»é™¤ï¼Œå› ç‚ºå·²ç¶“æˆåŠŸç”¨ OCR è™•ç†
                            continue
                        else:
                            print(f"     âš ï¸ OCR è™•ç†å¾Œä»ç„¡æœ‰æ•ˆå…§å®¹")

                    # è¨˜éŒ„ç©ºç™½PDF
                    self.empty_pdfs.append({
                        'file': pdf_file.name,
                        'folder': folder_path.name,
                        'pages': len(docs),
                        'total_chars': total_text_length,
                        'reason': 'å…§å®¹ç‚ºç©ºæˆ–éå°‘' + ('ï¼ˆOCR è™•ç†å¤±æ•—ï¼‰' if self.enable_ocr else 'ï¼ˆå¯èƒ½éœ€è¦OCRè™•ç†ï¼‰')
                    })
                else:
                    all_docs.extend(valid_docs)
                    print(f"  âœ… æˆåŠŸåŠ è¼‰ {len(valid_docs)} é ï¼ˆç¸½å­—ç¬¦æ•¸: {total_text_length}ï¼‰")

            except Exception as e:
                error_msg = str(e)[:200]
                print(f"  âŒ è™•ç†å¤±æ•—: {pdf_file.name}")
                print(f"     éŒ¯èª¤: {error_msg}")
                self.failed_pdfs.append({
                    'file': pdf_file.name,
                    'folder': folder_path.name,
                    'error': error_msg
                })
                continue

        return all_docs

    # === JSONL è™•ç†ï¼ˆä¸åˆ‡å‰²ï¼‰===
    def load_jsonl_files(self):
        if not self.jsonl_folder_path or not self.jsonl_folder_path.exists():
            print("âš ï¸ JSONL è³‡æ–™å¤¾æœªè¨­å®šæˆ–ä¸å­˜åœ¨ï¼Œè·³é JSONL è¼‰å…¥")
            return []
        
        jsonl_files = list(self.jsonl_folder_path.glob("*.jsonl"))
        if not jsonl_files:
            print("âš ï¸ æ²’æœ‰æ‰¾åˆ° JSONL æª”æ¡ˆï¼Œè·³é JSONL è¼‰å…¥")
            return []
        
        documents = []
        seen_answer_contents = set()  # ğŸ‘ˆ æ–°å¢ï¼šç”¨ä¾†è¨˜éŒ„å·²è¦‹éçš„ç­”æ¡ˆå…§å®¹
        
        for file_path in jsonl_files:
            print(f"è¼‰å…¥ JSONL: {file_path.name}")
            with open(file_path, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        data = json.loads(line)

                        # è™•ç†å…©ç¨®æ ¼å¼:
                        # æ ¼å¼1: æœ‰ question å’Œ text åˆ†é–‹çš„æ¬„ä½
                        # æ ¼å¼2: text æ¬„ä½åŒ…å« "å•é¡Œ: xxx\nç­”æ¡ˆ: xxx"
                        question_text = data.get('question', '').strip()
                        answer_text = data.get('text', '').strip()

                        # ğŸ‘‰ ä¿å­˜åŸå§‹çš„å®Œæ•´æ–‡æœ¬ï¼ˆç”¨æ–¼å¾ŒçºŒè¿”å›åŸå§‹æ ¼å¼ï¼‰
                        original_full_text = data.get('text', '').strip()

                        # å¦‚æœæ²’æœ‰ question æ¬„ä½ï¼Œå˜—è©¦å¾ text æ¬„ä½è§£æ
                        if not question_text and answer_text:
                            # æª¢æŸ¥æ˜¯å¦ç‚º "å•é¡Œ: xxx\nç­”æ¡ˆ: xxx" æ ¼å¼
                            if 'å•é¡Œ:' in answer_text and 'ç­”æ¡ˆ:' in answer_text:
                                parts = answer_text.split('\n')
                                for part in parts:
                                    if part.strip().startswith('å•é¡Œ:'):
                                        question_text = part.replace('å•é¡Œ:', '').strip()
                                    elif part.strip().startswith('ç­”æ¡ˆ:'):
                                        answer_text = part.replace('ç­”æ¡ˆ:', '').strip()
                        elif question_text and not ('å•é¡Œ:' in original_full_text):
                            # æ ¼å¼1ï¼šå¦‚æœ question å’Œ text æ˜¯åˆ†é–‹çš„ï¼Œé‡å»ºåŸå§‹æ ¼å¼
                            original_full_text = f"å•é¡Œ: {question_text}\nç­”æ¡ˆ: {answer_text}"

                        # ===== æ–°å¢ï¼šå…§å®¹å»é‡é‚è¼¯ =====
                        if not question_text or not answer_text:
                            continue

                        # ä½¿ç”¨ç­”æ¡ˆå…§å®¹çš„é›œæ¹Šå€¼æˆ–å‰200å­—å…ƒä½œç‚ºå»é‡éµï¼ˆé¿å…è¨˜æ†¶é«”çˆ†ç‚¸ï¼‰
                        content_key = answer_text[:200]  # æˆ–ä½¿ç”¨ hash(answer_text)
                        # if content_key in seen_answer_contents:
                        #     print(f"  è·³éé‡è¤‡å…§å®¹ (ç¬¬ {line_num} è¡Œ)")
                        #     continue
                        seen_answer_contents.add(content_key)
                        # ==============================

                        # é—œéµï¼šåªåµŒå…¥å•é¡Œ (Q)ï¼Œç­”æ¡ˆ (A) æ”¾åœ¨ metadata["original_text"]
                        content_parts = [question_text]
                        metadata = data.get('metadata', {})
                        if metadata.get('keywords'):
                            content_parts.append(f"é—œéµå­—: {metadata['keywords']}")
                        if metadata.get('reference'):
                            content_parts.append(f"åƒè€ƒ: {metadata['reference']}")

                        page_content = "\n".join(content_parts)
                        doc = Document(
                            page_content=page_content,
                            metadata={
                                "id": data.get('id', f"{file_path.stem}_{line_num}"),
                                "source_file": metadata.get('source_file', file_path.name),
                                "sheet_name": metadata.get('sheet_name', ''),
                                "title": question_text,  # ğŸ‘ˆ æ¨™é¡Œä¹Ÿå­˜å•é¡Œ
                                "keywords": metadata.get('keywords', ''),
                                "reference": metadata.get('reference', ''),
                                "original_text": answer_text,  # ğŸ‘ˆ ç­”æ¡ˆå­˜åœ¨ original_text
                                "original_full_text": original_full_text,  # ğŸ‘ˆ å®Œæ•´çš„åŸå§‹æ–‡æœ¬ "å•é¡Œ: xxx\nç­”æ¡ˆ: xxx"
                                "source_type": "jsonl"
                            }
                        )
                        documents.append(doc)
                    except Exception as e:
                        print(f"  JSONL {file_path.name} ç¬¬ {line_num} è¡Œè§£æéŒ¯èª¤: {e}")
                        continue
        print(f"âœ… ç¸½å…±è¼‰å…¥ {len(documents)} ç­† JSONL è³‡æ–™ï¼ˆå·²å»é‡ï¼‰")
        return documents

    # === åªè¼‰å…¥ JSONL ===
    def load_jsonl_only(self, batch_size=50):
        """åªè¼‰å…¥ JSONL è³‡æ–™åˆ° medical_knowledge_baseï¼ˆæœƒå…ˆåˆªé™¤èˆŠè³‡æ–™ï¼‰"""
        print(f"\n{'='*80}")
        print("ğŸ”„ åªè¼‰å…¥ JSONL è³‡æ–™")
        print(f"{'='*80}")

        jsonl_docs = self.load_jsonl_files()
        if not jsonl_docs:
            print("âš ï¸ æ²’æœ‰ JSONL è³‡æ–™å¯è¼‰å…¥")
            return 0

        cleaned_jsonl = []
        for doc in jsonl_docs:
            cleaned = self._validate_and_clean_document(doc)
            if cleaned:
                cleaned_jsonl.append(cleaned)

        if not cleaned_jsonl:
            print("âš ï¸ JSONL ç„¡æœ‰æ•ˆè³‡æ–™")
            return 0

        saved = self._batch_save_to_vectordb(
            cleaned_jsonl,
            "medical_knowledge_base",
            batch_size
        )
        print(f"\n{'='*80}")
        print(f"âœ… JSONL è¼‰å…¥å®Œæˆï¼å„²å­˜ {saved} ç­†è‡³ 'medical_knowledge_base'")
        print(f"{'='*80}")
        return saved

    # === åªè¼‰å…¥ PDF ===
    def load_pdf_only(self, batch_size=20):
        """åªè¼‰å…¥ PDF è³‡æ–™ï¼ˆæœƒå…ˆåˆªé™¤å„è³‡æ–™å¤¾å°æ‡‰çš„èˆŠ collectionï¼‰"""
        self.empty_pdfs = []
        self.failed_pdfs = []
        self.ocr_processed_pdfs = []

        print(f"\n{'='*80}")
        print("ğŸ”„ åªè¼‰å…¥ PDF è³‡æ–™")
        print(f"{'='*80}")

        pdf_folders = [d for d in self.pdf_base_path.iterdir() if d.is_dir()]
        if not pdf_folders:
            print("âš ï¸ ç„¡ PDF è³‡æ–™å¤¾")
            return 0

        total_collections = 0
        print(f"ğŸ“ æ‰¾åˆ° {len(pdf_folders)} å€‹ PDF è³‡æ–™å¤¾")

        for i, folder in enumerate(sorted(pdf_folders), 1):
            print(f"\nè™•ç†è³‡æ–™å¤¾ {i}/{len(pdf_folders)}: {folder.name}")
            pdf_docs = self.load_pdf_documents(folder)
            if not pdf_docs:
                print(f"  è·³éï¼ˆç„¡æœ‰æ•ˆ PDF å…§å®¹ï¼‰")
                continue

            collection_name = self.get_english_collection_name(folder.name)
            final_docs = []
            for doc in pdf_docs:
                splits = self.pdf_text_splitter.split_documents([doc])
                for split in splits:
                    split.page_content = self._clean_chunk_start(split.page_content)
                final_docs.extend(splits)

            saved = self._batch_save_to_vectordb(final_docs, collection_name, batch_size)
            if saved > 0:
                total_collections += 1
                print(f"  âœ… å„²å­˜ {saved} ç­†è‡³ '{collection_name}'")

        print(f"\n{'='*80}")
        print(f"âœ… PDF è¼‰å…¥å®Œæˆï¼ç¸½å…±å»ºç«‹ {total_collections} å€‹ collections")
        print(f"{'='*80}")

        self.print_processing_report()
        return total_collections

    # === çµ±ä¸€è¼‰å…¥ï¼šPDF + JSONL ===
    def load_all_data_unified(self, pdf_batch_size=20, jsonl_batch_size=50):
        # é‡ç½®è¨˜éŒ„
        self.empty_pdfs = []
        self.failed_pdfs = []

        print(f"\n{'='*80}")
        print("ğŸ”„ çµ±ä¸€è¼‰å…¥æ‰€æœ‰è³‡æ–™ï¼ˆPDF + JSONLï¼‰")
        print(f"{'='*80}")
        total_collections = 0

        # === 1. è¼‰å…¥ JSONL åˆ° medical_knowledge_base ===
        jsonl_docs = self.load_jsonl_files()
        if jsonl_docs:
            cleaned_jsonl = []
            for doc in jsonl_docs:
                cleaned = self._validate_and_clean_document(doc)
                if cleaned:
                    cleaned_jsonl.append(cleaned)

            if cleaned_jsonl:
                saved = self._batch_save_to_vectordb(
                    cleaned_jsonl,
                    "medical_knowledge_base",
                    jsonl_batch_size
                )
                if saved > 0:
                    total_collections += 1
                    print(f"âœ… JSONL è³‡æ–™å·²å„²å­˜è‡³ 'medical_knowledge_base' ({saved} ç­†)")
            else:
                print("âš ï¸ JSONL ç„¡æœ‰æ•ˆè³‡æ–™")
        else:
            print("â­ï¸ è·³é JSONL è¼‰å…¥")

        # === 2. è¼‰å…¥ PDFï¼ˆæŒ‰è³‡æ–™å¤¾ï¼‰===
        pdf_folders = [d for d in self.pdf_base_path.iterdir() if d.is_dir()]
        if pdf_folders:
            print(f"\nğŸ“ æ‰¾åˆ° {len(pdf_folders)} å€‹ PDF è³‡æ–™å¤¾")
            for i, folder in enumerate(sorted(pdf_folders), 1):
                print(f"\nè™•ç†è³‡æ–™å¤¾ {i}/{len(pdf_folders)}: {folder.name}")
                pdf_docs = self.load_pdf_documents(folder)
                if not pdf_docs:
                    print(f"  è·³éï¼ˆç„¡æœ‰æ•ˆ PDF å…§å®¹ï¼‰")
                    continue

                collection_name = self.get_english_collection_name(folder.name)
                final_docs = []
                for doc in pdf_docs:
                    splits = self.pdf_text_splitter.split_documents([doc])
                    # æ¸…ç†åˆ†å‰²å¾Œçš„æ–‡æœ¬ï¼Œç§»é™¤é–‹é ­çš„æ¨™é»ç¬¦è™Ÿå’Œç©ºæ ¼
                    for split in splits:
                        split.page_content = self._clean_chunk_start(split.page_content)
                    final_docs.extend(splits)

                saved = self._batch_save_to_vectordb(final_docs, collection_name, pdf_batch_size)
                if saved > 0:
                    total_collections += 1
                    print(f"  âœ… å„²å­˜ {saved} ç­†è‡³ '{collection_name}'")
        else:
            print("â­ï¸ ç„¡ PDF è³‡æ–™å¤¾")

        print(f"\n{'='*80}")
        print(f"ğŸ‰ çµ±ä¸€è¼‰å…¥å®Œæˆï¼ç¸½å…±å»ºç«‹ {total_collections} å€‹ collections")
        print(f"{'='*80}")

        # é¡¯ç¤ºè™•ç†å ±å‘Š
        self.print_processing_report()

    # === ç”Ÿæˆè™•ç†å ±å‘Š ===
    def print_processing_report(self):
        """é¡¯ç¤ºPDFè™•ç†å ±å‘Šï¼Œåˆ—å‡ºç©ºç™½å’Œå¤±æ•—çš„æ–‡ä»¶"""
        print(f"\n{'='*80}")
        print("ğŸ“Š PDF è™•ç†å ±å‘Š")
        print(f"{'='*80}")

        # OCR è™•ç†çµ±è¨ˆ
        if self.ocr_processed_pdfs:
            total_time = sum(item['time'] for item in self.ocr_processed_pdfs)
            total_pages = sum(item['pages'] for item in self.ocr_processed_pdfs)
            print(f"\nğŸ” OCR è™•ç†çµ±è¨ˆ ({len(self.ocr_processed_pdfs)} å€‹ PDF):")
            print("-" * 80)
            for idx, item in enumerate(self.ocr_processed_pdfs, 1):
                print(f"{idx}. æª”æ¡ˆ: {item['folder']}/{item['file']}")
                print(f"   é æ•¸: {item['pages']}, è€—æ™‚: {item['time']:.1f} ç§’")
            print(f"\n   ç¸½è¨ˆ: {total_pages} é , {total_time:.1f} ç§’")

        if self.empty_pdfs:
            print(f"\nâš ï¸ ç©ºç™½æˆ–å…§å®¹éå°‘çš„ PDF ({len(self.empty_pdfs)} å€‹):")
            if not self.enable_ocr:
                print("æç¤ºï¼šå•Ÿç”¨ OCR åŠŸèƒ½å¯ä»¥è™•ç†æƒæç‰ˆ PDF")
            print("-" * 80)
            for idx, item in enumerate(self.empty_pdfs, 1):
                print(f"{idx}. æª”æ¡ˆ: {item['folder']}/{item['file']}")
                print(f"   é æ•¸: {item['pages']}, å­—ç¬¦æ•¸: {item['total_chars']}")
                print(f"   åŸå› : {item['reason']}")
        else:
            print("\nâœ… æ‰€æœ‰ PDF éƒ½æœ‰æœ‰æ•ˆå…§å®¹")

        if self.failed_pdfs:
            print(f"\nâŒ è™•ç†å¤±æ•—çš„ PDF ({len(self.failed_pdfs)} å€‹):")
            print("-" * 80)
            for idx, item in enumerate(self.failed_pdfs, 1):
                print(f"{idx}. æª”æ¡ˆ: {item['folder']}/{item['file']}")
                print(f"   éŒ¯èª¤: {item['error']}")
        else:
            print("\nâœ… æ‰€æœ‰ PDF è™•ç†æˆåŠŸ")

        # ä¿å­˜å ±å‘Šåˆ°æ–‡ä»¶
        if self.empty_pdfs or self.failed_pdfs or self.ocr_processed_pdfs:
            # ä½¿ç”¨ç›¸å°è·¯å¾‘
            report_path = Path("./pdf_processing_report.txt")
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write("PDF è™•ç†å ±å‘Š\n")
                f.write("=" * 80 + "\n\n")

                if self.ocr_processed_pdfs:
                    total_time = sum(item['time'] for item in self.ocr_processed_pdfs)
                    total_pages = sum(item['pages'] for item in self.ocr_processed_pdfs)
                    f.write(f"OCR è™•ç†çµ±è¨ˆ ({len(self.ocr_processed_pdfs)} å€‹ PDF):\n")
                    f.write("-" * 80 + "\n")
                    for item in self.ocr_processed_pdfs:
                        f.write(f"æª”æ¡ˆ: {item['folder']}/{item['file']}\n")
                        f.write(f"é æ•¸: {item['pages']}, è€—æ™‚: {item['time']:.1f} ç§’\n\n")
                    f.write(f"ç¸½è¨ˆ: {total_pages} é , {total_time:.1f} ç§’\n\n")

                if self.empty_pdfs:
                    f.write(f"ç©ºç™½æˆ–å…§å®¹éå°‘çš„ PDF ({len(self.empty_pdfs)} å€‹):\n")
                    f.write("-" * 80 + "\n")
                    for item in self.empty_pdfs:
                        f.write(f"æª”æ¡ˆ: {item['folder']}/{item['file']}\n")
                        f.write(f"é æ•¸: {item['pages']}, å­—ç¬¦æ•¸: {item['total_chars']}\n")
                        f.write(f"åŸå› : {item['reason']}\n\n")

                if self.failed_pdfs:
                    f.write(f"\nè™•ç†å¤±æ•—çš„ PDF ({len(self.failed_pdfs)} å€‹):\n")
                    f.write("-" * 80 + "\n")
                    for item in self.failed_pdfs:
                        f.write(f"æª”æ¡ˆ: {item['folder']}/{item['file']}\n")
                        f.write(f"éŒ¯èª¤: {item['error']}\n\n")

            print(f"\nğŸ“„ è©³ç´°å ±å‘Šå·²ä¿å­˜è‡³: {report_path}")

        print(f"{'='*80}\n")

    # === å…¶ä»–å¿…è¦æ–¹æ³•ï¼ˆä¿æŒä¸è®Šï¼‰===
    def list_collections(self):
        try:
            conn = psycopg2.connect(self.db_connection_string)
            cursor = conn.cursor()
            cursor.execute("SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name='langchain_pg_collection');")
            if not cursor.fetchone()[0]:
                print("langchain_pg_collection è¡¨ä¸å­˜åœ¨")
                return
            cursor.execute("SELECT name FROM langchain_pg_collection ORDER BY name;")
            collections = [row[0] for row in cursor.fetchall()]
            print(f"\nå·²å»ºç«‹çš„ Collections ({len(collections)} å€‹):")
            for i, name in enumerate(collections, 1):
                print(f"{i:2d}. {name}")
            cursor.close()
            conn.close()
        except Exception as e:
            print(f"åˆ—å‡º collections å¤±æ•—: {e}")

    def search_test(self, query, collection_name=None):
        try:
            if not collection_name:
                conn = psycopg2.connect(self.db_connection_string)
                cursor = conn.cursor()
                cursor.execute("SELECT name FROM langchain_pg_collection LIMIT 1;")
                result = cursor.fetchone()
                cursor.close()
                conn.close()
                if not result:
                    print("ç„¡å¯ç”¨ collection")
                    return
                collection_name = result[0]
            
            print(f"\nåœ¨ collection '{collection_name}' ä¸­æœå°‹: '{query}'")
            vectorstore = PGVector(
                embeddings=self.embeddings,
                connection=self.db_connection_string,
                collection_name=collection_name,
            )
            results = vectorstore.similarity_search_with_score(query, k=5)
            print(f"æ‰¾åˆ° {len(results)} å€‹çµæœ:")
            for i, (doc, score) in enumerate(results, 1):
                source_type = doc.metadata.get('source_type', 'unknown')
                print(f"\nçµæœ {i} (ç›¸ä¼¼åº¦: {score:.4f}, ä¾†æº: {source_type})")

                if source_type == 'pdf':
                    preview = doc.page_content[:].replace('\n', ' ')
                    print(f"  å…§å®¹: {preview}...")
                    print(f"  PDF: {doc.metadata.get('pdf_file', 'N/A')}")
                elif source_type == 'jsonl':
                    # é¡¯ç¤ºå®Œæ•´çš„å•é¡Œå’Œç­”æ¡ˆ
                    question = doc.metadata.get('title', 'N/A')
                    answer = doc.metadata.get('original_text', 'N/A')
                    print(f"  å•é¡Œ: {question}")
                    print(f"  ç­”æ¡ˆ: {answer[:]}{'...' if len(answer) > 300 else ''}")
                    print(f"  ä¾†æºæª”æ¡ˆ: {doc.metadata.get('source_file', 'N/A')}")
                    print(f"  é—œéµå­—: {doc.metadata.get('keywords', 'N/A')}")
                else:
                    preview = doc.page_content[:].replace('\n', ' ')
                    print(f"  å…§å®¹: {preview}...")
        except Exception as e:
            print(f"æœå°‹å¤±æ•—: {e}")

    def reset_database(self):
        """âš ï¸ å±éšªï¼šåˆªé™¤æ‰€æœ‰ collectionsï¼ˆåŒ…æ‹¬ mem0ã€educational_images ç­‰ï¼‰"""
        try:
            conn = psycopg2.connect(self.db_connection_string)
            conn.set_isolation_level(ISOLATION_LEVEL_AUTOCOMMIT)
            cursor = conn.cursor()
            print("\nâš ï¸  è­¦å‘Šï¼šé€™æœƒåˆªé™¤æ‰€æœ‰è³‡æ–™ï¼ŒåŒ…æ‹¬ mem0ã€educational_images ç­‰ï¼")
            print("    å»ºè­°ä½¿ç”¨é¸é … 5/6/7 ä¾†å–®ç¨åˆªé™¤ JSONL æˆ– PDF è³‡æ–™ã€‚")
            confirm = input("\nè¼¸å…¥ 'DELETE ALL' ç¢ºèªåˆªé™¤æ‰€æœ‰è³‡æ–™: ").strip()
            if confirm == 'DELETE ALL':
                for table in ['langchain_pg_embedding', 'langchain_pg_collection']:
                    cursor.execute(f"DROP TABLE IF EXISTS {table} CASCADE;")
                    print(f"å·²åˆªé™¤ {table}")
                print("è³‡æ–™åº«é‡ç½®å®Œæˆ")
            else:
                print("å·²å–æ¶ˆ")
            cursor.close()
            conn.close()
        except Exception as e:
            print(f"é‡ç½®å¤±æ•—: {e}")

    def delete_collection(self, collection_name):
        """åˆªé™¤æŒ‡å®šçš„ collection"""
        try:
            conn = psycopg2.connect(self.db_connection_string)
            conn.set_isolation_level(ISOLATION_LEVEL_AUTOCOMMIT)
            cursor = conn.cursor()

            # å…ˆå–å¾— collection çš„ uuid
            cursor.execute(
                "SELECT uuid FROM langchain_pg_collection WHERE name = %s;",
                (collection_name,)
            )
            result = cursor.fetchone()

            if not result:
                print(f"âŒ Collection '{collection_name}' ä¸å­˜åœ¨")
                cursor.close()
                conn.close()
                return False

            collection_uuid = result[0]

            # åˆªé™¤è©² collection çš„æ‰€æœ‰ embeddings
            cursor.execute(
                "DELETE FROM langchain_pg_embedding WHERE collection_id = %s;",
                (collection_uuid,)
            )
            deleted_embeddings = cursor.rowcount

            # åˆªé™¤ collection è¨˜éŒ„
            cursor.execute(
                "DELETE FROM langchain_pg_collection WHERE uuid = %s;",
                (collection_uuid,)
            )

            print(f"âœ… å·²åˆªé™¤ collection '{collection_name}' ({deleted_embeddings} ç­†è³‡æ–™)")
            cursor.close()
            conn.close()
            return True

        except Exception as e:
            print(f"âŒ åˆªé™¤å¤±æ•—: {e}")
            return False

    def delete_jsonl_collection(self):
        """åˆªé™¤ JSONL è³‡æ–™ï¼ˆmedical_knowledge_baseï¼‰"""
        print("\nğŸ—‘ï¸  åˆªé™¤ JSONL è³‡æ–™ (medical_knowledge_base)")
        confirm = input("ç¢ºå®šåˆªé™¤ medical_knowledge_base? (y/N): ").strip().lower()
        if confirm == 'y':
            return self.delete_collection("medical_knowledge_base")
        else:
            print("å·²å–æ¶ˆ")
            return False

    def delete_pdf_collections(self):
        """åˆªé™¤æ‰€æœ‰ PDF collectionsï¼ˆæ ¹æ“š folder_mappingï¼‰"""
        print("\nğŸ—‘ï¸  åˆªé™¤ PDF è³‡æ–™")
        print("å°‡åˆªé™¤ä»¥ä¸‹ collections:")

        # åˆ—å‡ºæ‰€æœ‰ PDF collections
        pdf_collections = list(self.folder_mapping.values())
        for i, name in enumerate(pdf_collections, 1):
            print(f"  {i}. {name}")

        confirm = input(f"\nç¢ºå®šåˆªé™¤ä»¥ä¸Š {len(pdf_collections)} å€‹ PDF collections? (y/N): ").strip().lower()
        if confirm == 'y':
            success_count = 0
            for coll_name in pdf_collections:
                if self.delete_collection(coll_name):
                    success_count += 1
            print(f"\nâœ… æˆåŠŸåˆªé™¤ {success_count}/{len(pdf_collections)} å€‹ PDF collections")
            return True
        else:
            print("å·²å–æ¶ˆ")
            return False

    def delete_specific_collection_interactive(self):
        """äº’å‹•å¼åˆªé™¤æŒ‡å®š collection"""
        self.list_collections()
        coll_name = input("\nè«‹è¼¸å…¥è¦åˆªé™¤çš„ collection åç¨±: ").strip()
        if not coll_name:
            print("å·²å–æ¶ˆ")
            return False

        # å®‰å…¨æª¢æŸ¥ï¼šè­¦å‘Šåˆªé™¤é‡è¦çš„ collections
        protected_collections = ['memory_agent_chinese_v2', 'educational_images']
        if coll_name in protected_collections:
            print(f"\nâš ï¸  è­¦å‘Šï¼š'{coll_name}' æ˜¯é‡è¦çš„ç³»çµ±è³‡æ–™ï¼")
            confirm = input(f"ç¢ºå®šè¦åˆªé™¤ '{coll_name}'? è¼¸å…¥ 'YES' ç¢ºèª: ").strip()
            if confirm != 'YES':
                print("å·²å–æ¶ˆ")
                return False
        else:
            confirm = input(f"ç¢ºå®šåˆªé™¤ '{coll_name}'? (y/N): ").strip().lower()
            if confirm != 'y':
                print("å·²å–æ¶ˆ")
                return False

        return self.delete_collection(coll_name)

    def _clean_text(self, text):
        if not text: return ""
        cleaned = ""
        for char in text:
            cp = ord(char)
            if 0xD800 <= cp <= 0xDFFF: continue
            if cp < 32 and char not in '\n\r\t': continue
            cleaned += char
        return cleaned

    def _clean_chunk_start(self, text):
        """æ¸…ç†æ–‡æœ¬é–‹é ­çš„æ¨™é»ç¬¦è™Ÿå’Œç©ºæ ¼ï¼Œä¿æŒå¥å­å®Œæ•´æ€§"""
        if not text:
            return text

        # ç§»é™¤é–‹é ­çš„æ¨™é»ç¬¦è™Ÿï¼ˆå¥è™Ÿã€é©šå˜†è™Ÿã€å•è™Ÿï¼‰å’Œç©ºæ ¼
        cleaned = text.lstrip()
        while cleaned and cleaned[0] in 'ã€‚ï¼ï¼Ÿ.,!? \t\n':
            cleaned = cleaned[1:].lstrip()

        return cleaned

    def _validate_and_clean_document(self, doc):
        try:
            if not isinstance(doc.page_content, str):
                doc.page_content = str(doc.page_content) if doc.page_content else ""
            doc.page_content = self._clean_text(doc.page_content).strip()
            if not doc.page_content or len(doc.page_content) < 10:
                return None
            try:
                doc.page_content.encode('utf-8', errors='strict')
            except UnicodeEncodeError:
                doc.page_content = doc.page_content.encode('utf-8', errors='replace').decode('utf-8')
            if doc.metadata:
                cleaned_meta = {}
                for k, v in doc.metadata.items():
                    if v is None: cleaned_meta[k] = ""
                    elif isinstance(v, (str, int, float, bool)):
                        cleaned_meta[k] = self._clean_text(v) if isinstance(v, str) else v
                    else:
                        cleaned_meta[k] = self._clean_text(str(v))
                doc.metadata = cleaned_meta
            return doc
        except Exception as e:
            print(f"æ¸…ç†æ–‡æª”éŒ¯èª¤: {e}")
            return None

    def _batch_save_to_vectordb(self, documents, collection_name, batch_size):
        print(f"\nå„²å­˜ {len(documents)} ç­†è‡³ '{collection_name}'...")
        cleaned = [self._validate_and_clean_document(d) for d in documents]
        cleaned = [d for d in cleaned if d is not None]
        if not cleaned:
            print("ç„¡æœ‰æ•ˆæ–‡æª”")
            return 0
        
        total_saved = 0
        for i in range(0, len(cleaned), batch_size):
            batch = cleaned[i:i+batch_size]
            try:
                if i == 0:
                    PGVector.from_documents(
                        documents=batch,
                        embedding=self.embeddings,
                        connection=self.db_connection_string,
                        collection_name=collection_name,
                        pre_delete_collection=True,
                    )
                else:
                    vs = PGVector(
                        embeddings=self.embeddings,
                        connection=self.db_connection_string,
                        collection_name=collection_name,
                    )
                    vs.add_documents(batch)
                total_saved += len(batch)
            except Exception as e:
                print(f"æ‰¹æ¬¡ {i//batch_size + 1} éŒ¯èª¤: {str(e)[:100]}")
                continue
        return total_saved


# ===== ä¸»ç¨‹å¼ =====
def main():
    os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

    db_connection_string = f"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}"
    PDF_DATA_PATH = "/home/danny/AI-agent/DataSet"
    JSONL_DATA_PATH = "/home/danny/AI-agent/RAG_JSONL"  # è«‹ç¢ºèªè·¯å¾‘æ­£ç¢º

    print("å‘é‡è³‡æ–™åº«è¼‰å…¥å·¥å…·ï¼ˆçµ±ä¸€è¼‰å…¥ PDF + JSONLï¼‰")
    print("="*60)

    # è©¢å•æ˜¯å¦å•Ÿç”¨ OCR
    enable_ocr = False
    force_ocr_all = False

    if OCR_AVAILABLE:
        print("\nOCR è™•ç†æ¨¡å¼:")
        print("1. æ™ºèƒ½æ¨¡å¼ï¼ˆæ¨è–¦ï¼‰- åªå°æƒæç‰ˆ PDF ä½¿ç”¨ OCR")
        print("2. å¼·åˆ¶æ¨¡å¼ - æ‰€æœ‰ PDF éƒ½ä½¿ç”¨ OCRï¼ˆæ›´å®Œæ•´ä½†è¼ƒæ…¢ï¼‰")
        print("3. ä¸ä½¿ç”¨ OCR")

        ocr_choice = input("\nè«‹é¸æ“‡ (1-3, é è¨­ 1): ").strip() or "1"

        if ocr_choice == "1":
            enable_ocr = True
            ocr_mode_text = "æ™ºèƒ½ OCR"
        elif ocr_choice == "2":
            force_ocr_all = True
            ocr_mode_text = "å¼·åˆ¶ OCR (æ‰€æœ‰PDF)"
        else:
            ocr_mode_text = "ä¸ä½¿ç”¨ OCR"
    else:
        ocr_mode_text = "OCR æœªå®‰è£"
        print("\nâš ï¸ OCR æ¨¡çµ„æœªå®‰è£ï¼Œå°‡è·³éæƒæç‰ˆ PDF è™•ç†")

    loader = UnifiedVectorDBLoader(
        db_connection_string,
        PDF_DATA_PATH,
        JSONL_DATA_PATH,
        enable_ocr=enable_ocr,
        force_ocr_all=force_ocr_all
    )

    while True:
        print("\n" + "="*60)
        print("é¸é …:")
        print("="*60)
        print("ã€è¼‰å…¥è³‡æ–™ã€‘")
        print(f"1. ğŸš€ çµ±ä¸€è¼‰å…¥æ‰€æœ‰è³‡æ–™ï¼ˆPDF + JSONLï¼‰ [{ocr_mode_text}]")
        print("2. ğŸ“„ åªè¼‰å…¥ JSONL è³‡æ–™ (é‡å¯« medical_knowledge_base)")
        print(f"3. ğŸ“ åªè¼‰å…¥ PDF è³‡æ–™ (é‡å¯«å„ PDF collections) [{ocr_mode_text}]")
        print("-" * 60)
        print("ã€æŸ¥è©¢ã€‘")
        print("4. ğŸ“‹ åˆ—å‡ºæ‰€æœ‰ Collections")
        print("5. ğŸ” æ¸¬è©¦æœå°‹åŠŸèƒ½")
        print("-" * 60)
        print("ã€åˆªé™¤è³‡æ–™ã€‘")
        print("6. ğŸ—‘ï¸  åªåˆªé™¤ JSONL è³‡æ–™ (medical_knowledge_base)")
        print("7. ğŸ—‘ï¸  åªåˆªé™¤ PDF è³‡æ–™ (æ‰€æœ‰ PDF collections)")
        print("8. ğŸ—‘ï¸  åˆªé™¤æŒ‡å®š Collection")
        print("-" * 60)
        print("9. âš ï¸  é‡ç½®æ•´å€‹è³‡æ–™åº«ï¼ˆå±éšªï¼åˆªé™¤æ‰€æœ‰è³‡æ–™ï¼‰")
        print("0. â›” é€€å‡º")

        choice = input("\nè«‹é¸æ“‡ (0-9): ").strip()

        if choice == '1':
            pdf_batch = int(input("PDF æ‰¹æ¬¡å¤§å° (é è¨­ 20): ") or "20")
            jsonl_batch = int(input("JSONL æ‰¹æ¬¡å¤§å° (é è¨­ 50): ") or "50")
            confirm = input("âš ï¸ ç¢ºå®šçµ±ä¸€è¼‰å…¥æ‰€æœ‰è³‡æ–™ï¼Ÿ(y/N): ").strip().lower()
            if confirm == 'y':
                loader.load_all_data_unified(pdf_batch, jsonl_batch)

        elif choice == '2':
            jsonl_batch = int(input("JSONL æ‰¹æ¬¡å¤§å° (é è¨­ 50): ") or "50")
            confirm = input("âš ï¸ ç¢ºå®šé‡å¯« JSONL è³‡æ–™ï¼Ÿ(æœƒè¦†è“‹ medical_knowledge_base) (y/N): ").strip().lower()
            if confirm == 'y':
                loader.load_jsonl_only(jsonl_batch)

        elif choice == '3':
            pdf_batch = int(input("PDF æ‰¹æ¬¡å¤§å° (é è¨­ 20): ") or "20")
            confirm = input("âš ï¸ ç¢ºå®šé‡å¯« PDF è³‡æ–™ï¼Ÿ(æœƒè¦†è“‹å„ PDF collections) (y/N): ").strip().lower()
            if confirm == 'y':
                loader.load_pdf_only(pdf_batch)

        elif choice == '4':
            loader.list_collections()

        elif choice == '5':
            query = input("æœå°‹é—œéµå­—: ").strip() or "ç‹‚çŠ¬ç—…å€‹äººé˜²è­·è£å‚™"
            coll = input("Collection (ç•™ç©ºè‡ªå‹•): ").strip() or None
            loader.search_test(query, coll)

        elif choice == '6':
            loader.delete_jsonl_collection()

        elif choice == '7':
            loader.delete_pdf_collections()

        elif choice == '8':
            loader.delete_specific_collection_interactive()

        elif choice == '9':
            loader.reset_database()

        elif choice == '0':
            print("çµæŸ")
            break
        else:
            print("ç„¡æ•ˆé¸é …")

if __name__ == "__main__":
    main()