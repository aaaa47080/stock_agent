"""
rag_jsonl.py - JSONL æª”æ¡ˆè™•ç†èˆ‡æ ¼å¼åŒ–
Markdown ç‰ˆæœ¬ - ç‚º LLM å„ªåŒ–
å®Œæ•´ä¿®æ­£ç‰ˆ - æ”¯æ´ Excel å’Œ PDF çš„ä¾†æºæ ¼å¼åŒ–
âœ… v2.0: ä½¿ç”¨çµ±ä¸€ Metadata æ¨™æº–
"""

import os
import sys
import json
import threading
from langchain_core.documents import Document
from langchain_postgres import PGVector
from typing import List, Dict, Any, Tuple
from dotenv import load_dotenv

# æ·»åŠ çˆ¶ç›®éŒ„åˆ° Python è·¯å¾‘
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
# æ·»åŠ ç•¶å‰ç›®éŒ„ä»¥ä¾¿å°å…¥åŒç´šæ¨¡çµ„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
from core.config import embeddings

# ğŸ†• å°å…¥çµ±ä¸€ Metadata æ¨™æº–
from unified_metadata import (
    normalize_metadata,
    format_unified_citation,
    create_unified_doc_data
)

load_dotenv()

# ğŸ”’ å…¨å±€é–ï¼šä¿è­· PGVector åˆå§‹åŒ–ï¼ˆé¿å…ä¸¦è¡Œæ™‚çš„ SQLAlchemy è¡¨å®šç¾©è¡çªï¼‰
_vectorstore_init_lock = threading.Lock()

# ğŸ—ƒï¸ å‘é‡è³‡æ–™åº«ç·©å­˜ï¼šé¿å…é‡è¤‡åˆå§‹åŒ–åŒä¸€å€‹ collection
_vectorstore_cache: Dict[str, PGVector] = {}


# ========================================
# æª”åæ­£è¦åŒ–å‡½æ•¸ï¼ˆå·²ç§»è‡³ unified_metadata.pyï¼‰
# ========================================
# normalize_source_filename() å·²å¾ unified_metadata å°å…¥

# ========================================
# çµ±ä¸€çš„ä¾†æºæ¨™è¨»å‡½æ•¸ï¼ˆå·²ç§»è‡³ unified_metadata.pyï¼‰
# ========================================

def format_source_citation(doc_type: str, metadata: dict) -> str:
    """
    çµ±ä¸€çš„ä¾†æºæ¨™è¨»æ ¼å¼ï¼ˆå‘å¾Œå…¼å®¹ç‰ˆæœ¬ï¼‰

    âš ï¸ å·²å»¢æ£„ï¼šå»ºè­°ä½¿ç”¨ unified_metadata.format_unified_citation()
    æ­¤å‡½æ•¸ä¿ç•™ä»¥å‘å¾Œå…¼å®¹ï¼Œå…§éƒ¨èª¿ç”¨æ–°çš„çµ±ä¸€æ¨™æº–

    Args:
        doc_type: 'pdf', 'jsonl', æˆ– 'auto'ï¼ˆè‡ªå‹•æª¢æ¸¬ï¼‰
        metadata: æ–‡æª”çš„ metadata å­—å…¸

    Returns:
        çµ±ä¸€æ ¼å¼çš„ä¾†æºæ¨™è¨»
    """
    # ä½¿ç”¨æ–°çš„çµ±ä¸€æ¨™æº–è™•ç†
    normalized = normalize_metadata(metadata)
    return format_unified_citation(normalized)


# ========================================
# æ–‡ä»¶è¼‰å…¥èˆ‡è½‰æ›
# ========================================

def load_json_files_from_folder(folder_path: str) -> List[Dict[Any, Any]]:
    """å¾è³‡æ–™å¤¾è¼‰å…¥æ‰€æœ‰JSONLæª”æ¡ˆ"""
    json_data = []
    
    if not os.path.exists(folder_path):
        print(f"è³‡æ–™å¤¾ä¸å­˜åœ¨: {folder_path}")
        return json_data
    
    for filename in os.listdir(folder_path):
        if filename.endswith('.jsonl'):
            file_path = os.path.join(folder_path, filename)
            line_count = 0
            try:
                with open(file_path, 'r', encoding='utf-8') as file:
                    for line in file:
                        line = line.strip()
                        if line:
                            data = json.loads(line)
                            json_data.append(data)
                            line_count += 1
                print(f"æˆåŠŸè¼‰å…¥: {filename}, åŒ…å« {line_count} ç­†è³‡æ–™")
            except Exception as e:
                print(f"è¼‰å…¥æª”æ¡ˆ {filename} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    return json_data


def json_to_documents(json_data_list: List[Dict[Any, Any]]) -> List[Document]:
    """å°‡JSONè³‡æ–™è½‰æ›ç‚ºDocumentå°è±¡"""
    documents = []
    
    for json_data in json_data_list:
        content_parts = []
        
        if 'text' in json_data:
            content_parts.append(json_data['text'])
        
        metadata = json_data.get('metadata', {})
        
        if metadata.get('title'):
            content_parts.append(f"æ¨™é¡Œ: {metadata['title']}")
        
        if metadata.get('keywords'):
            content_parts.append(f"é—œéµå­—: {metadata['keywords']}")
            
        if metadata.get('reference'):
            content_parts.append(f"åƒè€ƒè³‡æ–™: {metadata['reference']}")
            
        if metadata.get('source_file'):
            content_parts.append(f"æª”æ¡ˆåç¨±: {metadata['source_file']}")
        
        page_content = "\n".join(content_parts)
        
        document = Document(
            page_content=page_content,
            metadata={
                "id": json_data.get('id', ''),
                "source_file": metadata.get('source_file', ''),
                "sheet_name": metadata.get('sheet_name', ''),
                "title": metadata.get('title', ''),
                "keywords": metadata.get('keywords', ''),
                "reference": metadata.get('reference', ''),
                "original_text": json_data.get('text', '')
            }
        )
        documents.append(document)
    
    return documents


# ========================================
# å‘é‡è³‡æ–™åº«æ“ä½œ
# ========================================

def load_existing_vectordb(db_connection_string: str, collection_name: str = "medical_knowledge") -> PGVector:
    """
    è¼‰å…¥ç¾æœ‰çš„PostgreSQLå‘é‡è³‡æ–™åº«ï¼ˆç·šç¨‹å®‰å…¨ç‰ˆæœ¬ï¼‰

    ä½¿ç”¨ç·©å­˜å’Œé–æ©Ÿåˆ¶é¿å…ä¸¦è¡Œåˆå§‹åŒ–æ™‚çš„ SQLAlchemy è¡¨å®šç¾©è¡çª
    """
    # æª¢æŸ¥ç·©å­˜
    cache_key = f"{db_connection_string}::{collection_name}"

    if cache_key in _vectorstore_cache:
        return _vectorstore_cache[cache_key]

    # ä½¿ç”¨é–ä¿è­·åˆå§‹åŒ–éç¨‹
    with _vectorstore_init_lock:
        # é›™é‡æª¢æŸ¥ï¼šå¯èƒ½åœ¨ç­‰å¾…é–çš„éç¨‹ä¸­ï¼Œå…¶ä»–ç·šç¨‹å·²ç¶“åˆå§‹åŒ–äº†
        if cache_key in _vectorstore_cache:
            return _vectorstore_cache[cache_key]

        # åˆå§‹åŒ–å‘é‡è³‡æ–™åº«
        vectorstore = PGVector(
            embeddings=embeddings,
            connection=db_connection_string,
            collection_name=collection_name,
        )

        # å­˜å…¥ç·©å­˜
        _vectorstore_cache[cache_key] = vectorstore

        return vectorstore


def search_vectordb_with_scores(vectorstore: PGVector, query: str, k: int = 5) -> List[Tuple[Document, float]]:
    """æœç´¢PostgreSQLå‘é‡è³‡æ–™åº«ä¸¦è¿”å›ç›¸ä¼¼åº¦åˆ†æ•¸"""
    try:
        if vectorstore is None:
            print("å‘é‡è³‡æ–™åº«ç‚ºç©º")
            return []

        results = vectorstore.similarity_search_with_score(query, k=k)
        return results
    except Exception as e:
        print(f"å‘é‡è³‡æ–™åº«æœå°‹å¤±æ•—: {e}")
        return []


def clear_vectorstore_cache():
    """
    æ¸…é™¤å‘é‡è³‡æ–™åº«ç·©å­˜

    åœ¨ä»¥ä¸‹æƒ…æ³ä½¿ç”¨ï¼š
    - è³‡æ–™åº«é€£æ¥åƒæ•¸æ”¹è®Š
    - éœ€è¦å¼·åˆ¶é‡æ–°åˆå§‹åŒ–
    - è¨˜æ†¶é«”æ¸…ç†
    """
    global _vectorstore_cache
    with _vectorstore_init_lock:
        _vectorstore_cache.clear()
        print("âœ… å‘é‡è³‡æ–™åº«ç·©å­˜å·²æ¸…é™¤")


# ========================================
# çµæœæ ¼å¼åŒ– - Markdown ç‰ˆæœ¬ï¼ˆçµ±ä¸€ç‰ˆï¼‰
# ========================================

def get_structured_search_results(
    results: List[Tuple],
    query: str,
    top_n: int = 5,
    show_scores: bool = False,
    format_type: str = "markdown"
) -> Tuple[Dict[str, Any], str]:
    """
    å°‡æª¢ç´¢çµæœæ ¼å¼åŒ–ç‚ºçµæ§‹åŒ–æ•¸æ“š + Markdown æ–‡æœ¬ï¼ˆçµ±ä¸€ç‰ˆï¼‰
    âœ… v2.0: ä½¿ç”¨çµ±ä¸€ Metadata æ¨™æº–
    âœ… è‡ªå‹•æ”¯æ´æ‰€æœ‰è³‡æ–™æºæ ¼å¼ï¼ˆExcel, PDF, OCR PDFï¼‰

    Args:
        results: LangChain æª¢ç´¢çµæœ [(Document, score), ...]
        query: ç”¨æˆ¶å•é¡Œ
        top_n: è¿”å›å‰ N ç­†çµæœ
        show_scores: æ˜¯å¦é¡¯ç¤ºç›¸ä¼¼åº¦åˆ†æ•¸
        format_type: "markdown" æˆ– "json"

    Returns:
        Tuple[Dict, str]: (çµæ§‹åŒ–æ•¸æ“šå­—å…¸, æ ¼å¼åŒ–é¡¯ç¤ºå­—ç¬¦ä¸²)
    """
    # åªå–å‰ top_n ç­†
    limited_results = results[:top_n] if len(results) > top_n else results

    if not limited_results:
        empty_dict = {
            'query': query,
            'total_results': 0,
            'documents': []
        }
        return empty_dict, "æœªæ‰¾åˆ°ç›¸é—œè³‡æ–™ã€‚"

    # æŒ‰ç›¸ä¼¼åº¦åˆ†æ•¸æ’åºï¼ˆåˆ†æ•¸è¶Šå°è¶Šç›¸ä¼¼ï¼‰
    sorted_results = sorted(limited_results, key=lambda x: x[1])

    # 1ï¸âƒ£ æ§‹å»ºçµæ§‹åŒ–æ•¸æ“šå­—å…¸ï¼ˆä½¿ç”¨çµ±ä¸€æ¨™æº–ï¼‰
    structured_data = {
        'query': query,
        'total_results': len(sorted_results),
        'documents': []
    }

    for i, (doc, score) in enumerate(sorted_results, 1):
        # ğŸ†• ä½¿ç”¨çµ±ä¸€æ¨™æº–è™•ç†æ‰€æœ‰æ ¼å¼
        doc_data = create_unified_doc_data(doc, score, i)
        structured_data['documents'].append(doc_data)

    # 2ï¸âƒ£ ç”Ÿæˆæ ¼å¼åŒ–æ–‡æœ¬
    if format_type == "json":
        import json
        display_text = json.dumps(structured_data, ensure_ascii=False, indent=2)
    else:
        display_text = _format_jsonl_markdown(sorted_results, query, show_scores)

    return structured_data, display_text


def _format_jsonl_markdown(
    results: List[Tuple],
    query: str,
    show_scores: bool = False
) -> str:
    """
    å°‡æª¢ç´¢çµæœæ ¼å¼åŒ–ç‚º Markdown æ ¼å¼ï¼ˆçµ±ä¸€ç‰ˆï¼‰
    âœ… è‡ªå‹•æ”¯æ´ Excel å’Œ PDF æ ¼å¼
    âœ… è‡ªå‹•å°‡ xlsx æª”åé‚„åŸç‚º pdf æª”å
    
    Args:
        results: [(Document, score), ...]
        query: ç”¨æˆ¶æŸ¥è©¢
        show_scores: æ˜¯å¦é¡¯ç¤ºåˆ†æ•¸
    
    Returns:
        Markdown æ ¼å¼çš„æ–‡æœ¬
    """
    output = []
    output.append(f"# æª¢ç´¢çµæœ\n")
    output.append(f"**æŸ¥è©¢**ï¼š{query}\n")
    output.append(f"**çµæœæ•¸é‡**ï¼š{len(results)}\n")
    output.append("---\n")
    
    for i, (doc, score) in enumerate(results, 1):
        metadata = doc.metadata
        title = metadata.get('title', '')
        keywords = metadata.get('keywords', '')
        # å„ªå…ˆè®€å– original_full_textï¼ˆåŒ…å«å•é¡Œ+ç­”æ¡ˆï¼‰ï¼Œå¦å‰‡ä½¿ç”¨ original_text
        original_text = metadata.get('original_full_text', metadata.get('original_text', doc.page_content))
        
        # æ–‡æª”æ¨™é¡Œ
        output.append(f"## è³‡æ–™ {i}")
        
        # åªé¡¯ç¤ºæœ‰æ•ˆçš„æ¨™é¡Œï¼ˆéæ¿¾é è¨­å€¼ï¼‰
        if title and title != 'CGMH(All Right Reserved)':
            output.append(f"\n**å•é¡Œ**ï¼š{title}")
        
        # å…§å®¹
        if original_text and original_text.strip():
            output.append(f"\n**å…§å®¹**ï¼š\n\n{original_text.strip()}")
        
        # åªé¡¯ç¤ºæœ‰æ•ˆçš„é—œéµå­—ï¼ˆéæ¿¾é è¨­å€¼ï¼‰
        if keywords and keywords != 'CGMH(All Right Reserved)':
            output.append(f"\n**é—œéµå­—**ï¼š{keywords}")
        
        # âœ… ä½¿ç”¨ 'auto' è‡ªå‹•æª¢æ¸¬ä¸¦æ ¼å¼åŒ–ä¾†æº (å·²åŒ…å«æª”åæ­£è¦åŒ–)
        citation = format_source_citation('auto', metadata)
        output.append(f"\n**ä¾†æº**ï¼š{citation}")
        
        # åˆ†æ•¸ï¼ˆå¯é¸ï¼‰
        if show_scores:
            output.append(f"\n**ç›¸ä¼¼åº¦åˆ†æ•¸**ï¼š{score:.4f}")
        
        output.append("\n---\n")
    
    return "\n".join(output)


# ========================================
# æ¸¬è©¦ä»£ç¢¼
# ========================================

# ========================================
# ç°¡çŸ­æ¸¬è©¦ search_vectordb_with_scores
# ========================================

if __name__ == "__main__":
    DB_HOST = "172.23.37.2"
    DB_PORT = "5432"
    DB_NAME = "infection_rag"
    #DB_USER = "a1031737"
    DB_USER = "langchain"
    DB_PASSWORD = "langchain"
    # DB_PASSWORD = "a156277323"
    DB_CONNECTION = f"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}"
    if not DB_CONNECTION:
        print("éŒ¯èª¤ï¼šæœªè¨­å®š DATABASE_URL ç’°å¢ƒè®Šæ•¸")
        exit(1)

    try:
        # è¼‰å…¥ç¾æœ‰å‘é‡è³‡æ–™åº« - ä½¿ç”¨æ­£ç¢ºçš„ collection åç¨±
        # "medical_knowledge" æ˜¯ç©ºçš„ï¼Œæ‡‰è©²ä½¿ç”¨ "medical_knowledge_base"
        vectorstore = load_existing_vectordb(DB_CONNECTION, collection_name="medical_knowledge_base")

        # åŸ·è¡Œæœå°‹
        query = "éæ¥è§¸æ€§çš®è†šç‚æˆ–æ¥è§¸æ€§çš®è†šç‚ç™¼ç™¢æ€§çš®ç–¹"
        results = search_vectordb_with_scores(vectorstore, query, k=5)

        print(f"\næœå°‹æŸ¥è©¢: ã€Œ{query}ã€")
        print(f"æ‰¾åˆ° {len(results)} ç­†çµæœ\n")

        if results:
            for i, (doc, score) in enumerate(results, 1):
                print(f"--- çµæœ {i} (åˆ†æ•¸: {score:.4f}) ---")

                # é¡¯ç¤ºå•é¡Œ
                title = doc.metadata.get('title', '')
                if title:
                    print(f"å•é¡Œ: {title}")

                # é¡¯ç¤ºç­”æ¡ˆï¼ˆé€™æ˜¯é—œéµï¼ï¼‰
                answer = doc.metadata.get('original_text', '')
                if answer:
                    print(f"ç­”æ¡ˆ: {answer}")

                # é¡¯ç¤ºé—œéµå­—
                keywords = doc.metadata.get('keywords', '')
                if keywords:
                    print(f"é—œéµå­—: {keywords}")

                # é¡¯ç¤ºä¾†æº
                citation = format_source_citation('auto', doc.metadata)
                print(f"ä¾†æº: {citation}")
                print()
        else:
            print("âŒ æ²’æœ‰æ‰¾åˆ°ä»»ä½•çµæœ")
            print("\næç¤ºï¼šè«‹æª¢æŸ¥ï¼š")
            print("1. collection_name æ˜¯å¦æ­£ç¢º")
            print("2. embedding æ¨¡å‹æ˜¯å¦èˆ‡å»ºç«‹å‘é‡åº«æ™‚ä½¿ç”¨çš„ç›¸åŒ")
            print("3. è³‡æ–™åº«ä¸­æ˜¯å¦æœ‰è³‡æ–™")

    except Exception as e:
        print(f"æ¸¬è©¦å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()