"""
ä¸»ç¨‹å¼ - ç°¡åŒ–ç‰ˆï¼ˆæ¨¡çµ„åŒ–å¾Œï¼‰
ä½¿ç”¨çµ±ä¸€è¨˜æ†¶ç®¡ç†å™¨æ•´åˆä¸‰å±¤è¨˜æ†¶æ¶æ§‹
æ•´åˆ Langfuse è¿½è¹¤åŠŸèƒ½
"""

import asyncio
import json
import opencc
from langchain_core.messages import HumanMessage, AIMessage
from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import InMemorySaver
from graph.graph_nodes import memory_client
from core.prompt_config import MEMORY_DEDUCTION_PROMPT
# å°å…¥é…ç½®
from core.config import (
    llm, WORKFLOW_LIMITS, SHORT_TERM_MEMORY_CONFIG,
    MESSAGE_SUMMARIZATION_CONFIG, RETRIEVAL_CONFIG, TOOLS_CONFIG,
    LONG_TERM_MEMORY_CONFIG, MEMORY_MANAGEMENT_CONFIG
)

# åˆå§‹åŒ– OpenCC è½‰æ›å™¨ (ç°¡é«” -> ç¹é«”)
converter = opencc.OpenCC('s2tw.json')

# å°å…¥ Langfuse è¿½è¹¤
from monitoring.langfuse_integration import get_langfuse_config, update_trace_io

# ğŸ†• å°å…¥å¿«å–å’Œè¨˜æ†¶é«”ç®¡ç†
from utils.cache_manager import get_cache_manager, MemoryManager

# å°å…¥ langmem çš„ SummarizationNode
from langmem.short_term import SummarizationNode
from langchain_core.messages.utils import count_tokens_approximately

# å°å…¥è¨˜æ†¶ç³»çµ±
# from mem0 import Memory
# from long_term_memory import create_long_term_memory
# from memory_utils import create_unified_memory_manager

# å°å…¥æ¨¡çµ„åŒ–çš„çµ„ä»¶
from graph.graph_routing import (
    FullState,
    route_after_memory,
    route_after_classification,
    route_after_generate,
    route_after_check_question_type,
    route_after_validation
)

# å°å…¥ç¯€é»å‡½æ•¸
from graph.graph_nodes import (
    extract_current_query,
    retrieve_memory,
    answer_from_memory,
    classify_query_type,
    planning_node,  # æ–°å¢ï¼šPlanning ç¯€é»
    retrieve_knowledge,
    supplement_with_realtime_info,  # å³æ™‚æœå°‹è£œå……ç¯€é»ï¼ˆç”±ä½¿ç”¨è€…é¸æ“‡æ˜¯å¦å•Ÿç”¨ï¼‰
    generate_response,
    validate_answer,
    refine_answer,
    check_question_type,
    set_out_of_scope_message
)



# ===== åˆå§‹åŒ–å¿«å–å’Œè¨˜æ†¶é«”ç®¡ç†ç³»çµ± =====
# print("\n" + "="*60)
# print("ğŸ”§ åˆå§‹åŒ–å¿«å–å’Œè¨˜æ†¶é«”ç®¡ç†ç³»çµ±...")
# print("="*60)

# ç²å–å–®ä¾‹å¿«å–ç®¡ç†å™¨
cache_manager = get_cache_manager()
if not cache_manager:
    # print("â­ï¸ å¿«å–ç³»çµ±å·²åœç”¨")
    pass

# åˆå§‹åŒ–è¨˜æ†¶é«”ç®¡ç†å™¨
memory_manager = MemoryManager(MEMORY_MANAGEMENT_CONFIG)

# print("="*60 + "\n")

# ===== åˆå§‹åŒ–å·¥å…·ç³»çµ± =====
from tools_init import initialize_all_tools, get_active_tools

# æ ¹æ“šé…ç½®åˆå§‹åŒ–å·¥å…·ç³»çµ±
active_tools = []
if TOOLS_CONFIG.get('enabled', True):
    try:
        # print("\n" + "="*60)
        # print("ğŸ”§ åˆå§‹åŒ–å·¥å…·ç³»çµ±...")
        # print("="*60)
        initialize_all_tools()
        active_tools = get_active_tools()
        # if active_tools:
        #     print(f"\nâœ… å·¥å…·ç³»çµ±å·²å•Ÿç”¨ï¼Œè¼‰å…¥ {len(active_tools)} å€‹å·¥å…·")
        # else:
        #     print(f"\nâš ï¸ å·¥å…·ç³»çµ±å·²å•Ÿç”¨ï¼Œä½†æœªè¼‰å…¥ä»»ä½•å·¥å…·")
        # print("="*60 + "\n")
    except Exception as e:
        print(f"\nâš ï¸ å·¥å…·ç³»çµ±åˆå§‹åŒ–å¤±æ•—: {e}")
        print("ç³»çµ±å°‡ç¹¼çºŒé‹è¡Œï¼Œä½†å·¥å…·åŠŸèƒ½å°‡ä¸å¯ç”¨\n")
        active_tools = []
else:
    # print("\nâ­ï¸ å·¥å…·ç³»çµ±å·²åœç”¨ï¼ˆå¯åœ¨ config.py çš„ TOOLS_CONFIG ä¸­å•Ÿç”¨ï¼‰\n")
    pass

# ===== æ§‹å»º Graph =====
# print("ğŸ”§ æ§‹å»º LangGraph...")

# çŸ­æœŸè¨˜æ†¶ï¼ˆç•¶å‰æœƒè©±ï¼‰
checkpointer = InMemorySaver()

# ===== é…ç½® SummarizationNodeï¼ˆå¦‚æœå•Ÿç”¨ï¼‰===== 
summarization_node = None
if MESSAGE_SUMMARIZATION_CONFIG.get('enabled', True):
    # print("âœ… å•Ÿç”¨ Message Summarization")
    # å‰µå»ºç”¨æ–¼æ‘˜è¦çš„ LLMï¼ˆç¶å®š max_tokens ä»¥æ§åˆ¶æ‘˜è¦é•·åº¦ï¼‰
    summarization_model = llm.bind(max_tokens=MESSAGE_SUMMARIZATION_CONFIG['max_summary_tokens'])

    # å‰µå»º SummarizationNode
    summarization_node = SummarizationNode(
        token_counter=count_tokens_approximately,
        model=summarization_model,
        max_tokens=MESSAGE_SUMMARIZATION_CONFIG['max_tokens'],
        max_tokens_before_summary=MESSAGE_SUMMARIZATION_CONFIG['max_tokens_before_summary'],
        max_summary_tokens=MESSAGE_SUMMARIZATION_CONFIG['max_summary_tokens'],
        input_messages_key="messages",
        output_messages_key="messages",  # ğŸ”‘ é—œéµï¼šè¨­ç‚º "messages" ä»¥æ›¿æ›åŸæ¶ˆæ¯åˆ—è¡¨
    )
    # print(f"ğŸ“Š æ‘˜è¦é…ç½®: max_tokens={MESSAGE_SUMMARIZATION_CONFIG['max_tokens']}, "
    #       f"è§¸ç™¼é–¾å€¼={MESSAGE_SUMMARIZATION_CONFIG['max_tokens_before_summary']}")
else:
    # print("â­ï¸ Message Summarization å·²åœç”¨")
    pass

# å‰µå»º Graph
graph = StateGraph(FullState)

# æ·»åŠ ç¯€é»
graph.add_node("extract_current_query", extract_current_query)
graph.add_node("retrieve_memory", retrieve_memory)
graph.add_node("answer_from_memory", answer_from_memory)
graph.add_node("classify_query_type", classify_query_type)
graph.add_node("planning", planning_node)  # ğŸ†• Planning ç¯€é»
graph.add_node("retrieve_knowledge", retrieve_knowledge)
graph.add_node("supplement_realtime", supplement_with_realtime_info)  # å³æ™‚æœå°‹è£œå……ï¼ˆç”±ä½¿ç”¨è€…é€é enabled_tool_ids æ§åˆ¶ï¼‰
graph.add_node("generate_response", generate_response)
graph.add_node("validate_answer", validate_answer)
graph.add_node("refine_answer", refine_answer)
graph.add_node("check_question_type", check_question_type)
graph.add_node("set_out_of_scope_message", set_out_of_scope_message)

# å¦‚æœå•Ÿç”¨ summarizationï¼Œæ·»åŠ  summarization ç¯€é»
if summarization_node:
    graph.add_node("summarize", summarization_node)
    # print("âœ… å·²æ·»åŠ  'summarize' ç¯€é»åˆ° Graph")

# å®šç¾©é‚Š
# å¦‚æœå•Ÿç”¨ summarizationï¼Œåœ¨é€²å…¥ä¸»æµç¨‹å‰å…ˆç¶“éæ‘˜è¦ç¯€é»
if summarization_node:
    graph.add_edge(START, "summarize")
    graph.add_edge("summarize", "extract_current_query")
    # print("âœ… æµç¨‹: START â†’ summarize â†’ extract_current_query")
else:
    graph.add_edge(START, "extract_current_query")
    # print("âœ… æµç¨‹: START â†’ extract_current_query")
graph.add_edge("extract_current_query", "retrieve_memory")
graph.add_edge("retrieve_memory", "answer_from_memory")

# æ¢ä»¶è·¯ç”±
graph.add_conditional_edges(
    "answer_from_memory",
    route_after_memory,
    {
        "classify_query_type": "classify_query_type",
        "set_out_of_scope_message": "set_out_of_scope_message"
    }
)
graph.add_conditional_edges(
    "classify_query_type",
    route_after_classification,
    {
        "retrieve_knowledge": "planning",  # ğŸ†• æ”¹ç‚ºå…ˆé€²å…¥ Planning
        "generate_response": "generate_response",
        "set_out_of_scope_message": "set_out_of_scope_message"
    }
)
# ğŸ†• Planning å¾Œç›´æ¥é€²å…¥æª¢ç´¢
graph.add_edge("planning", "retrieve_knowledge")
# å·¥ä½œæµç¨‹ï¼šretrieve_knowledge â†’ supplement_realtime â†’ generate_response
# supplement_realtime ç¯€é»æœƒæ ¹æ“šä½¿ç”¨è€…é¸æ“‡çš„ enabled_tool_ids æ±ºå®šæ˜¯å¦ä½¿ç”¨å¤–éƒ¨æœå°‹
graph.add_edge("retrieve_knowledge", "supplement_realtime")
graph.add_edge("supplement_realtime", "generate_response")
graph.add_conditional_edges(
    "generate_response",
    route_after_generate,
    {
        "check_question_type": "check_question_type",
        "set_out_of_scope_message": "set_out_of_scope_message",
        "END": END
    }
)
graph.add_conditional_edges(
    "check_question_type",
    route_after_check_question_type,
    {
        "validate_answer": "validate_answer",
        "set_out_of_scope_message": "set_out_of_scope_message",
        "END": END
    }
)
graph.add_conditional_edges(
    "validate_answer",
    route_after_validation,
    {
        "refine_answer": "refine_answer",
        "retrieve_knowledge": "retrieve_knowledge",
        "set_out_of_scope_message": "set_out_of_scope_message",
        "END": END
    }
)
graph.add_edge("refine_answer", "validate_answer")
graph.add_edge("set_out_of_scope_message", END)

# ç·¨è­¯
app = graph.compile(checkpointer=checkpointer)

# print("âœ… LangGraph æ§‹å»ºå®Œæˆ\n")


# ===== å°è©±æ­·å²ç®¡ç† =====
conversation_history = []

def display_conversation_history():
    """é¡¯ç¤ºå°è©±æ­·å²"""
    if not conversation_history:
        print("\nğŸ“‹ ç›®å‰æ²’æœ‰å°è©±æ­·å²\n")
        return

    print("\n" + "=" * 80)
    print("ğŸ“‹ å°è©±æ­·å²")
    print("=" * 80)
    for i, (query, answer) in enumerate(conversation_history, 1):
        print(f"\n[{i}] ğŸ‘¤ ç”¨æˆ¶: {query[:50]}..." if len(query) > 50 else f"\n[{i}] ğŸ‘¤ ç”¨æˆ¶: {query}")
        print(f"    ğŸ¤– ç³»çµ±: {answer[:50]}..." if len(answer) > 50 else f"    ğŸ¤– ç³»çµ±: {answer}")
    print("=" * 80 + "\n")


def display_help():
    """é¡¯ç¤ºå¹«åŠ©ä¿¡æ¯"""
    print("\n" + "=" * 80)
    print("ğŸ“– å¯ç”¨æŒ‡ä»¤")
    print("=" * 80)
    print("  /help     - é¡¯ç¤ºæ­¤å¹«åŠ©ä¿¡æ¯")
    print("  /history  - é¡¯ç¤ºå°è©±æ­·å²")
    print("  /clear    - æ¸…é™¤å°è©±æ­·å²å’Œå¿«å–")
    print("  /stats    - ğŸ†• é¡¯ç¤ºå¿«å–èˆ‡è¨˜æ†¶é«”çµ±è¨ˆ")
    print("  /quit     - é€€å‡ºç³»çµ±")
    print("  /exit     - é€€å‡ºç³»çµ±")
    print("=" * 80 + "\n")


# ===== ä¸»ç¨‹å¼å…¥å£ =====
async def main():
    """ä¸»ç¨‹å¼ - é€£çºŒå°è©±ç‰ˆæœ¬"""
    global conversation_history  # ğŸ”§ ä¿®æ­£ï¼šå®£å‘Šä½¿ç”¨å…¨åŸŸè®Šæ•¸

    print("\n" + "=" * 80)
    print("ğŸ¥ é†«ç™‚è«®è©¢ç³»çµ±å·²å•Ÿå‹•ï¼ˆé€£çºŒå°è©±ç‰ˆæœ¬ + ä¸‰å±¤è¨˜æ†¶ï¼‰")
    print("=" * 80)
    print("ğŸ’¡ è¼¸å…¥ /help æŸ¥çœ‹å¯ç”¨æŒ‡ä»¤")
    print("=" * 80 + "\n")

    user_id = "test_user_00100100"
    session_id = "conversation_006"
    # ä½¿ç”¨ user_id + session_id çµ„åˆç¢ºä¿ä¸åŒç”¨æˆ¶çš„æœƒè©±å®Œå…¨éš”é›¢
    thread_id = f"{user_id}_{session_id}"
    config = {"configurable": {"thread_id": thread_id}}

    # é€£çºŒå°è©±å¾ªç’°
    while True:
        try:
            # ç²å–ç”¨æˆ¶è¼¸å…¥
            query = input("\nğŸ‘¤ è«‹è¼¸å…¥æ‚¨çš„å•é¡Œï¼ˆæˆ–è¼¸å…¥æŒ‡ä»¤ï¼‰: ").strip()

            # è™•ç†ç©ºè¼¸å…¥
            if not query:
                print("âš ï¸  è«‹è¼¸å…¥æœ‰æ•ˆçš„å•é¡Œæˆ–æŒ‡ä»¤")
                continue

            # è™•ç†ç³»çµ±æŒ‡ä»¤
            if query.startswith("/"):
                if query in ["/quit", "/exit"]:
                    print("\nğŸ‘‹ æ„Ÿè¬ä½¿ç”¨é†«ç™‚è«®è©¢ç³»çµ±ï¼Œå†è¦‹ï¼\n")
                    break
                elif query == "/help":
                    display_help()
                    continue
                elif query == "/history":
                    display_conversation_history()
                    continue
                elif query == "/clear":
                    conversation_history.clear()

                    # ğŸ†• æ¸…é™¤å¿«å–
                    if cache_manager:
                        cache_manager.clear_all()

                    # æ ¹æ“šé…ç½®æ±ºå®šæ˜¯å¦æ¸…é™¤é•·æœŸè¨˜æ†¶
                    if LONG_TERM_MEMORY_CONFIG.get('enabled', True):
                        memory_client.delete_all(user_id=user_id)
                        print("\nâœ… å°è©±æ­·å²ã€å¿«å–å’Œé•·æœŸè¨˜æ†¶å·²æ¸…é™¤\n")
                    else:
                        print("\nâœ… å°è©±æ­·å²å’Œå¿«å–å·²æ¸…é™¤ï¼ˆé•·æœŸè¨˜æ†¶å·²åœç”¨ï¼‰\n")
                    continue
                elif query == "/stats":
                    # ğŸ†• é¡¯ç¤ºå¿«å–å’Œè¨˜æ†¶é«”çµ±è¨ˆ
                    if cache_manager:
                        cache_manager.print_stats()
                    memory_manager.print_memory_info(conversation_history)
                    continue
                else:
                    print(f"âŒ æœªçŸ¥æŒ‡ä»¤: {query}")
                    print("ğŸ’¡ è¼¸å…¥ /help æŸ¥çœ‹å¯ç”¨æŒ‡ä»¤\n")
                    continue

            # è™•ç†æ­£å¸¸å•é¡Œ
            # print(f"\n{ '='*80}")
            # print(f"ğŸ” æ­£åœ¨è™•ç†æ‚¨çš„å•é¡Œ...")
            # print(f"{ '='*80}\n")

            # ç²å– Langfuse é…ç½®
            # session_id (thread_id) ç”¨æ–¼åœ¨ Langfuse ä¸­åˆ†çµ„åŒä¸€æœƒè©±çš„æ‰€æœ‰å°è©±
            # æ¯æ¬¡èª¿ç”¨ CallbackHandler æœƒè‡ªå‹•å‰µå»ºæ–°çš„ trace
            langfuse_cfg, langfuse_handler = get_langfuse_config(
                user_id=user_id,
                session_id=thread_id,  # ä½¿ç”¨ thread_id ä½œç‚º session_id,è€Œä¸æ˜¯æ¯æ¬¡ç”Ÿæˆæ–°çš„
                tags=["cli", "medical", "rag"],
                metadata={
                    "query": query,
                    "source": "cli",
                    "short_term_memory_enabled": SHORT_TERM_MEMORY_CONFIG.get('enabled', False),
                    "long_term_memory_enabled": LONG_TERM_MEMORY_CONFIG.get('enabled', False),
                    "datasource_ids": RETRIEVAL_CONFIG.get('default_datasource_ids')
                }
            )

            # æ§‹å»º messages åˆ—è¡¨
            messages = []
            if SHORT_TERM_MEMORY_CONFIG.get('enabled', True):
                for user_q, asst_a in conversation_history:
                    messages.append(HumanMessage(content=user_q))
                    messages.append(AIMessage(content=asst_a))

            # æ·»åŠ ç•¶å‰ç”¨æˆ¶å•é¡Œ
            messages.append(HumanMessage(content=query))

            # æ§‹å»º state
            initial_state = {
                "messages": messages,
                "user_id": user_id,
                "query_type": "",
                "knowledge": "",
                "current_query": "",
                "memory_summary": "",
                "memory_response": "",
                "memory_source": "",
                "final_answer": "",
                "validation_result": "",
                "validation_feedback": "",
                "retry_count": 0,
                "query_type_history": [],
                "knowledge_retrieval_count": 0,
                "validation_need_supplement_info": "",
                "question_category": "",
                "iteration_count": 0,
                "used_sources_dict": {},
                "planning_result": None,  # ğŸ†• Planning çµæœ
                "retrieval_steps": [],  # ğŸ†• æª¢ç´¢æ­¥é©Ÿ
                "current_step": 0,  # ğŸ†• ç•¶å‰æ­¥é©Ÿ
                "enable_short_term_memory": SHORT_TERM_MEMORY_CONFIG.get('enabled', False),
                "datasource_ids": RETRIEVAL_CONFIG.get('default_datasource_ids'),
                "enabled_tool_ids": TOOLS_CONFIG.get('default_tools')  # ğŸ†• ç¢ºä¿ CDC æœå°‹å·¥å…·è¢«å•Ÿç”¨
            }

            # é‹è¡Œ graph
            config_with_limit = {
                **config,
                "recursion_limit": WORKFLOW_LIMITS["langgraph_recursion_limit"],
                **langfuse_cfg
            }
            result = await app.ainvoke(initial_state, config_with_limit)

            # ç²å–å›ç­”
            answer = result.get("final_answer", "æŠ±æ­‰ï¼Œç„¡æ³•ç”Ÿæˆå›ç­”")
            
            # ä½¿ç”¨ OpenCC å°‡å›ç­”è½‰æ›ç‚ºç¹é«”ä¸­æ–‡ (é‡å° OCR å¯èƒ½ç”¢ç”Ÿçš„ç°¡é«”ä¸­æ–‡)
            if answer:
                answer = converter.convert(answer)
                
            query_type = result.get("query_type", "")

            # æ›´æ–° trace
            if langfuse_handler:
                update_trace_io(
                    handler=langfuse_handler,
                    user_query=query,
                    final_answer=answer,
                    additional_metadata={"query_type": query_type}
                )


            # ä¿å­˜åˆ°è¨˜æ†¶
            should_save_to_memory = (
                query_type not in ["greet", "out_of_scope", "conversation_history"] and
                answer not in [
                    "æŠ±æ­‰ï¼Œé€™å€‹å•é¡Œè¶…å‡ºäº†æˆ‘çš„å°ˆæ¥­ç¯„åœã€‚",
                    "æŠ±æ­‰ï¼Œé€™å€‹å•é¡Œè¶…å‡ºäº†æˆ‘çš„å°ˆæ¥­ç¯„åœã€‚æˆ‘å°ˆæ³¨æ–¼æä¾›é†«ç™‚æ„ŸæŸ“ç®¡åˆ¶ç›¸é—œçš„è«®è©¢æœå‹™ã€‚",
                    "æŠ±æ­‰ï¼Œæ‚¨çš„å•é¡Œè¶…å‡ºäº†æˆ‘çš„èªçŸ¥ç¯„åœï¼Œè«‹æ‚¨æ›å€‹æ–¹å¼å†æå•ä¸€æ¬¡ï¼Œæˆ–è€…è¯çµ¡ç›¸é—œå°ˆæ¥­äººå“¡ç‚ºæ‚¨æœå‹™ã€‚",
                    "æŠ±æ­‰ï¼Œè³‡æ–™åº«ä¸­æœªæ‰¾åˆ°å¯å›ç­”æ­¤å•é¡Œçš„é†«ç™‚è³‡è¨Šã€‚è«‹è«®è©¢å°ˆæ¥­é†«ç™‚äººå“¡æˆ–ç›¸é—œç§‘å®¤ã€‚",
                    "æŠ±æ­‰ï¼Œç„¡æ³•ç”Ÿæˆå›ç­”",
                    "æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ™‚ç™¼ç”ŸéŒ¯èª¤ã€‚"
                ]
            )

            if should_save_to_memory:
                if LONG_TERM_MEMORY_CONFIG.get('enabled', True):
                    try:
                        # print("\n" + "="*80)
                        # print("ğŸ’¾ ã€é•·æœŸè¨˜æ†¶å„²å­˜ã€‘é–‹å§‹")
                        # print("="*80)

                        clean_answer_for_memory = answer
                        if "**åƒè€ƒä¾æ“š**" in clean_answer_for_memory:
                            clean_answer_for_memory = clean_answer_for_memory[:clean_answer_for_memory.find("**åƒè€ƒä¾æ“š**")].rstrip()
                        if "**ç›¸é—œè¡¨æ ¼**" in clean_answer_for_memory:
                            clean_answer_for_memory = clean_answer_for_memory[:clean_answer_for_memory.find("**ç›¸é—œè¡¨æ ¼**")].rstrip()

                        qa_pair = f"ç”¨æˆ¶å•é¡Œï¼š{query}\nåŠ©æ‰‹å›ç­”ï¼š{clean_answer_for_memory}"

                        # print(f"ğŸ‘¤ ç”¨æˆ¶ ID: {user_id}")
                        # print(f"ğŸ“ å„²å­˜å…§å®¹é•·åº¦: {len(qa_pair)} å­—å…ƒ")
                        # print("-"*80)
                        # print("ğŸ“„ å„²å­˜å…§å®¹é è¦½:")
                        # print(qa_pair[:200] + "..." if len(qa_pair) > 200 else qa_pair)
                        # print("-"*80)

                        add_result = memory_client.add(
                            qa_pair,
                            user_id=user_id,
                            metadata={"category": "medical_chat"},
                            prompt=MEMORY_DEDUCTION_PROMPT
                        )

                        # print("\nğŸ“¦ å„²å­˜çµæœ:")
                        # print(json.dumps(add_result, ensure_ascii=False, indent=2))
                        # print("="*80)
                        # print("âœ… å·²æˆåŠŸå„²å­˜åˆ°é•·æœŸè¨˜æ†¶ï¼ˆmem0ï¼‰")
                        # print("="*80 + "\n")
                    except Exception as e:
                        print(f"\nâŒ é•·æœŸè¨˜æ†¶å„²å­˜å¤±æ•—: {e}")
                        import traceback
                        traceback.print_exc()
                        print("="*80 + "\n")
                else:
                    # print("â­ï¸ è·³éé•·æœŸè¨˜æ†¶å„²å­˜ï¼ˆå·²åœç”¨ï¼‰")
                    pass

            if should_save_to_memory:
                if SHORT_TERM_MEMORY_CONFIG.get('enabled', True):
                    clean_answer = answer
                    if "**åƒè€ƒä¾æ“š**" in clean_answer:
                        clean_answer = clean_answer[:clean_answer.find("**åƒè€ƒä¾æ“š**")].rstrip()
                    if "**ç›¸é—œè¡¨æ ¼**" in clean_answer:
                        clean_answer = clean_answer[:clean_answer.find("**ç›¸é—œè¡¨æ ¼**")].rstrip()
                    conversation_history.append((query, clean_answer))
                    # print(f"ğŸ’¾ å·²å„²å­˜åˆ°çŸ­æœŸè¨˜æ†¶ï¼ˆå°è©±æ­·å²ï¼‰- ç§»é™¤åƒè€ƒæ–‡ç»å¾Œ {len(clean_answer)} å­—å…ƒ")

                    # ğŸ†• è‡ªå‹•æ¸…ç†å°è©±æ­·å²
                    conversation_history, removed = memory_manager.cleanup(conversation_history)
                    if removed > 0:
                        # print(f"   âš ï¸ è¨˜æ†¶é«”ç®¡ç†ï¼šè‡ªå‹•æ¸…ç†äº† {removed} è¼ªèˆŠå°è©±")
                        pass
                else:
                    conversation_history.clear()
            else:
                # print(f"â­ï¸ è·³éè¨˜æ†¶å„²å­˜ï¼ˆé¡å‹: {query_type or 'æœªçŸ¥'}ï¼‰")
                if not SHORT_TERM_MEMORY_CONFIG.get('enabled', True):
                    conversation_history.clear()

            # é¡¯ç¤ºçµæœ
            print(f"\n{ '='*80}")
            print("ğŸ¤– ç³»çµ±å›ç­”:")
            print(f"{ '='*80}")
            print(answer)
            print(f"{ '='*80}\n")

        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ æª¢æ¸¬åˆ°ä¸­æ–·ä¿¡è™Ÿï¼Œæ­£åœ¨é€€å‡º...\n")
            break
        except Exception as e:
            print(f"\nâŒ ç™¼ç”ŸéŒ¯èª¤: {e}")
            import traceback
            traceback.print_exc()
            print("\nğŸ’¡ æ‚¨å¯ä»¥ç¹¼çºŒæå•æˆ–è¼¸å…¥ /quit é€€å‡º\n")


if __name__ == "__main__":
    asyncio.run(main())