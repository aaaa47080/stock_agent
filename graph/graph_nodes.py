"""
Graph ç¯€é»å‡½æ•¸æ¨¡çµ„ï¼ˆå„ªåŒ–ç‰ˆï¼‰
åŒ…å«æ‰€æœ‰ LangGraph ç¯€é»çš„å¯¦ç¾
"""

from typing import Dict, List, Tuple, Optional
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.output_parsers import PydanticOutputParser
import opencc
import asyncio
import os
import re
import json
import traceback

# å°å…¥é…ç½®
from core.config import llm, mem0_config, remove_think_tags, WORKFLOW_LIMITS
from core.prompt_config import (
    MEMORY_SEARCH_LIMIT, QUESTION_TYPE_CHECK_PROMPT,
    ANSWER_VALIDATION_PROMPT, ANSWER_REGENERATE_PROMPT, GREET_PROMPTTEMPLATE,
    SHORT_TERM_MEMORY_CHECK_PROMPT, QUERY_PLANNING_PROMPT
)
from utils.prompt_function import (
    get_query_classification_prompt, get_rag_response_prompt,
    get_rag_response_prompt_with_history
)
from core.dataclass import ValidationResult, QueryPlanningResult
from mem0 import Memory
from utils.response_utils import (
    parse_memory_results, post_process_response,
    smart_truncate_knowledge
)
from retrieval.retrieval_utils import (
    generate_sub_questions, retrieve_single_query,
    merge_documents_intelligently, is_reference_citation, is_figure_description,
    get_paragraph_signature
)
from graph.graph_routing import FullState
from langgraph.store.memory import InMemoryStore
from core.config import RETRIEVAL_CONFIG, TOOLS_CONFIG
from tools_config import get_tool
from retrieval.image_retriever import retrieve_relevant_images, get_image_mapper

# ğŸ†• Langfuse 3.0 åŒ¯å…¥ (å¯é¸ä¾è³´)
try:
    from langfuse import observe, get_client as get_langfuse_client
    LANGFUSE_AVAILABLE = True
except ImportError:
    LANGFUSE_AVAILABLE = False
    get_langfuse_client = None
    # å‰µå»ºç©ºçš„è£é£¾å™¨ä»¥é¿å…éŒ¯èª¤
    def observe(*args, **kwargs):
        def decorator(func):
            return func
        return decorator if not args else decorator(args[0])
    print("â„¹ï¸ Langfuse æœªå®‰è£ï¼Œå­å•é¡Œè¿½è¹¤åŠŸèƒ½å°‡è¢«åœç”¨")

# ===== åˆå§‹åŒ– =====
converter = opencc.OpenCC('s2tw.json')
memory_client = Memory.from_config(mem0_config)

# ğŸ†• åˆå§‹åŒ–å¿«å–ç®¡ç†å™¨ï¼ˆå…¨å±€å–®ä¾‹ï¼‰
from utils.cache_manager import get_cache_manager

# åˆå§‹åŒ– InMemoryStore
clue_store = None
if RETRIEVAL_CONFIG.get('clue_cache_enabled', True):
    clue_store = InMemoryStore()
    print("âœ… InMemoryStore å·²åˆå§‹åŒ–ï¼ˆç”¨æ–¼ç·šç´¢å¿«å–ï¼‰")
else:
    print("âš ï¸ InMemoryStore æœªå•Ÿç”¨ï¼ˆclue_cache_enabled=Falseï¼‰")


# ===== è¼”åŠ©å‡½æ•¸ =====

class LogHelper:
    """çµ±ä¸€çš„æ—¥èªŒè¼¸å‡ºè¼”åŠ©é¡"""
    
    @staticmethod
    def section(title: str, char: str = "=", width: int = 80):
        print(f"\n{char * width}")
        print(f"{title}")
        print(f"{char * width}")
    
    @staticmethod
    def subsection(title: str, char: str = "-", width: int = 80):
        print(f"{title}")
        print(f"{char * width}")
    
    @staticmethod
    def iteration_info(current: int, max_count: int, action: str):
        print(f"ğŸ” {action} - ç•¶å‰è¿­ä»£æ¬¡æ•¸: {current}/{max_count}")


def is_short_term_memory_enabled(state: FullState) -> bool:
    """æª¢æŸ¥çŸ­æœŸè¨˜æ†¶æ˜¯å¦å•Ÿç”¨"""
    from core.config import SHORT_TERM_MEMORY_CONFIG
    if hasattr(state, 'enable_short_term_memory') and state.enable_short_term_memory is not None:
        return state.enable_short_term_memory
    return SHORT_TERM_MEMORY_CONFIG.get('enabled', True)


def build_conversation_context(messages: List, include_current: bool = False) -> str:
    """æ§‹å»ºå°è©±ä¸Šä¸‹æ–‡å­—ä¸²"""
    context_parts = []
    msg_list = messages if include_current else messages[:-1] if messages else []
    
    for msg in msg_list:
        if isinstance(msg, HumanMessage):
            context_parts.append(f"ç”¨æˆ¶: {msg.content}")
        elif isinstance(msg, AIMessage):
            context_parts.append(f"åŠ©æ‰‹: {msg.content}")
    
    return "\n".join(context_parts)


def merge_document_content_smart(existing_content: str, new_content: str) -> str:
    """æ™ºèƒ½åˆä½µæ–‡æª”å…§å®¹ï¼Œé¿å…é‡è¤‡çš„QAå°"""
    if new_content in existing_content:
        return existing_content

    # æå–å•é¡Œ+ç­”æ¡ˆçµ„åˆä½œç‚ºå»é‡æ¨™è­˜
    existing_qa_pairs = set()
    for match in re.finditer(r'å•é¡Œ:(.*?)ç­”æ¡ˆ:(.*?)(?=é—œéµå­—:|åƒè€ƒ:|å•é¡Œ:|$)', existing_content, re.DOTALL):
        existing_qa_pairs.add((match.group(1).strip(), match.group(2).strip()))

    if not existing_qa_pairs:
        return existing_content + "\n\n" + new_content

    # éæ¿¾æ–°å…§å®¹ä¸­çš„é‡è¤‡QAå°
    new_paragraphs = []
    for para in new_content.split('\n'):
        para = para.strip()
        if not para:
            continue

        qa_match = re.search(r'å•é¡Œ:(.*?)ç­”æ¡ˆ:(.*?)(?=é—œéµå­—:|åƒè€ƒ:|å•é¡Œ:|$)', para, re.DOTALL)
        if qa_match:
            qa_key = (qa_match.group(1).strip(), qa_match.group(2).strip())
            if qa_key not in existing_qa_pairs:
                existing_qa_pairs.add(qa_key)
                new_paragraphs.append(para)
        else:
            new_paragraphs.append(para)

    return existing_content + "\n\n" + "\n".join(new_paragraphs) if new_paragraphs else existing_content


def merge_used_sources(combined: dict, new_dict: dict) -> dict:
    """åˆä½µä½¿ç”¨çš„åƒè€ƒæ–‡ç»å­—å…¸"""
    for source, doc_info in new_dict.items():
        content = doc_info.get('content', '') if isinstance(doc_info, dict) else doc_info
        score = doc_info.get('score', 999.0) if isinstance(doc_info, dict) else 999.0
        
        if source not in combined:
            combined[source] = {'content': content, 'score': score}
        else:
            existing = combined[source]
            existing_content = existing.get('content', '') if isinstance(existing, dict) else existing
            existing_score = existing.get('score', 999.0) if isinstance(existing, dict) else 999.0
            
            if content:
                combined[source] = {
                    'content': merge_document_content_smart(existing_content, content),
                    'score': min(existing_score, score)
                }
    return combined


async def stream_llm_response(message_stream) -> str:
    """æµå¼æ¥æ”¶ LLM å›æ‡‰"""
    full_response = ""
    async for chunk in message_stream:
        if hasattr(chunk, 'content'):
            full_response += chunk.content
    return full_response


def extract_disease_name(user_message: str) -> str:
    """æå–ç–¾ç—…åç¨±"""
    from core.prompt_config import DISEASE_EXTRACTION_PROMPT
    try:
        response = llm.invoke(DISEASE_EXTRACTION_PROMPT.format(user_message=user_message))
        disease_name = response.content.strip()
        return disease_name if disease_name != "æœªçŸ¥ç–¾ç—…" else ""
    except:
        return ""


# ===== æª¢ç´¢ç›¸é—œè¼”åŠ©å‡½æ•¸ =====

@observe(name="retrieve_multiple_queries")
async def retrieve_multiple_queries(
    queries: List[str],
    main_question: str,
    user_id: str,
    datasource_ids: Optional[List[str]] = None,
    existing_sources: Optional[dict] = None
) -> Tuple[str, dict, List]:
    """ä¸¦è¡Œæª¢ç´¢å¤šå€‹æŸ¥è©¢ä¸¦åˆä½µçµæœ"""

    # ğŸ†• ä½¿ç”¨ @observe è£é£¾å™¨å¾Œï¼Œæœƒè‡ªå‹•å‰µå»º span
    # æ‰‹å‹•æ›´æ–° metadata ä»¥æä¾›æ›´å¤šä¸Šä¸‹æ–‡
    if LANGFUSE_AVAILABLE and get_langfuse_client:
        try:
            get_langfuse_client().update_current_trace(
                metadata={
                    "total_sub_questions": len(queries),
                    "main_question": main_question,
                    "sub_questions": queries
                }
            )
        except Exception as e:
            print(f"âš ï¸ æ›´æ–° Langfuse trace metadata å¤±æ•—: {e}")

    tasks = [
        retrieve_single_query(
            q, main_question=main_question, store=clue_store,
            user_id=user_id, datasource_ids=datasource_ids,
            sub_question_index=idx + 1,  # ğŸ†• å‚³éå­å•é¡Œç·¨è™Ÿ
            is_main_question=(q == main_question)  # ğŸ”’ æ¨™è¨˜ä¸»å•é¡Œï¼ˆä¸»å•é¡Œä¸å¿«å–ï¼‰
        )
        for idx, q in enumerate(queries)
    ]
    results = await asyncio.gather(*tasks)

    all_knowledge_parts = []
    combined_sources = existing_sources.copy() if existing_sources else {}
    combined_tables = []

    # ğŸ†• çµ±è¨ˆè³‡è¨Š
    total_sources = 0
    successful_retrievals = 0

    for idx, (knowledge, sources, used_dict, matched_tables) in enumerate(results):
        if knowledge:
            all_knowledge_parts.append(knowledge)
            combined_sources = merge_used_sources(combined_sources, used_dict)
            if matched_tables:
                combined_tables.extend(matched_tables)
            successful_retrievals += 1
            total_sources += len(sources)
        else:
            print(f"   âš ï¸ å­å•é¡Œ {idx+1} ç„¡ç·šç´¢: {queries[idx]}")
            all_knowledge_parts.append(f"[å­å•é¡Œï¼š{queries[idx]}]\nç„¡ç·šç´¢")

    # ğŸ†• è¨˜éŒ„æ•´é«”æª¢ç´¢çµ±è¨ˆï¼ˆ@observe æœƒè‡ªå‹•è¨˜éŒ„ returnï¼Œé€™è£¡é¡å¤–è¨˜éŒ„çµ±è¨ˆï¼‰
    if LANGFUSE_AVAILABLE and get_langfuse_client:
        try:
            get_langfuse_client().update_current_span(
                metadata={
                    "successful_retrievals": successful_retrievals,
                    "total_sub_questions": len(queries),
                    "total_sources_retrieved": total_sources,
                    "total_unique_sources": len(combined_sources)
                }
            )
        except Exception as e:
            print(f"âš ï¸ æ›´æ–° Langfuse observation metadata å¤±æ•—: {e}")

    combined_knowledge = "\n\n---\n\n".join(all_knowledge_parts) if all_knowledge_parts else ""
    return combined_knowledge, combined_sources, combined_tables


def ensure_main_query_first(sub_questions: List[str], main_query: str) -> List[str]:
    """ç¢ºä¿ä¸»å•é¡Œåœ¨å­å•é¡Œåˆ—è¡¨çš„ç¬¬ä¸€ä½"""
    if main_query not in sub_questions:
        sub_questions.insert(0, main_query)
        print(f"   âœ… å·²å°‡åŸå§‹ä¸»å•é¡ŒåŠ å…¥å­å•é¡Œåˆ—è¡¨ç¬¬ä¸€ä½")
    return sub_questions


# ===== åƒè€ƒæ–‡ç»è™•ç† =====




def rebuild_references_from_used_sources(used_sources_dict: dict) -> str:
    """é‡å»ºåƒè€ƒæ–‡ç» (åƒ…ä¿ç•™æœ‰æ•ˆçš„ PDF æˆ–ç¶²é ä¾†æº)"""
    if not used_sources_dict:
        return ""

    # æ¨™æº–åŒ–æ–‡ä»¶åä¸¦æ”¶é›†å…§å®¹
    normalized_docs = {}

    for doc_name, doc_info in used_sources_dict.items():
        clean_name = doc_name.strip() if doc_name else ""
        
        # ğŸ†• éæ¿¾ç„¡æ•ˆæˆ–å…§éƒ¨çš„ä¾†æºåç¨±
        if not clean_name or clean_name.lower() in ['unknown', 'educational_vlm_analysis', 'none', 'async_pipeline_v2']:
            continue
            
        # ğŸ†• ç¢ºä¿åªä¿ç•™ PDF æˆ– URL (æˆ–æ‚¨å®šç¾©çš„å…¶ä»–æœ‰æ•ˆä¾†æº)
        is_url = clean_name.startswith(('http://', 'https://'))
        is_pdf = clean_name.lower().endswith('.pdf')
        
        if not (is_url or is_pdf):
            # å¦‚æœä¸æ˜¯ PDF ä¹Ÿä¸æ˜¯ URLï¼Œæª¢æŸ¥æ˜¯å¦ç‚ºæˆ‘å€‘å·²çŸ¥çš„ PDF ä¾†æºæ ¼å¼ (ç„¡å‰¯æª”åä½†ç¬¦åˆ PDF å‘½åè¦å¾‹)
            if not any(char.isdigit() for char in clean_name) and len(clean_name) < 5:
                continue

        clean_name = clean_name.replace('ã€Š', '').replace('ã€‹', '').strip()
        if not (is_url or is_pdf):
            clean_name = os.path.basename(clean_name)

        content = doc_info.get('content', '') if isinstance(doc_info, dict) else doc_info
        score = doc_info.get('score', 999.0) if isinstance(doc_info, dict) else 999.0

        if not content:
            continue

        if clean_name in normalized_docs:
            existing = normalized_docs[clean_name]
            merged_content = merge_documents_intelligently(clean_name, existing['content'], content)
            normalized_docs[clean_name] = {'content': merged_content, 'score': min(existing['score'], score)}
        else:
            normalized_docs[clean_name] = {
                'content': merge_documents_intelligently(clean_name, "", content),
                'score': score
            }

    if not normalized_docs:
        return ""

    # æŒ‰åˆ†æ•¸æ’åºä¸¦æ ¼å¼åŒ–è¼¸å‡º
    sorted_docs = sorted(normalized_docs.items(), key=lambda x: x[1]['score'])
    output = ["**åƒè€ƒä¾æ“š**\n"]

    # ğŸ†• å…¨å±€æ®µè½å»é‡ï¼ˆè·¨æ–‡æª”ï¼‰
    global_seen_signatures = set()
    has_valid_content = False  # æ¨™è¨˜æ˜¯å¦æœ‰æœ‰æ•ˆå…§å®¹

    for doc_name, doc_data in sorted_docs:
        content = doc_data['content']
        if not content.strip():
            continue
        
        # æš«å­˜ç•¶å‰æ–‡æª”çš„é¡¯ç¤ºå…§å®¹
        doc_display_output = []
        
        # åˆ†é›¢ QA å’Œ PDF éƒ¨åˆ†
        if 'ã€åŸå§‹PDFæ®µè½ã€‘' in content:
            parts = content.split('ã€åŸå§‹PDFæ®µè½ã€‘', 1)
            qa_part, pdf_part = parts[0].strip(), parts[1].strip() if len(parts) > 1 else ""
        else:
            qa_part = content if 'å•é¡Œ:' in content and 'ç­”æ¡ˆ:' in content else ""
            pdf_part = "" if qa_part else content
        
        # è™•ç† QA éƒ¨åˆ†
        if qa_part:
            idx = 1
            for line in qa_part.split('\n'):
                line = line.strip()
                if line.startswith('å•é¡Œ:'):
                    doc_display_output.append(f"{idx}. {line}")
                    idx += 1
                elif line.startswith('ç­”æ¡ˆ:'):
                    doc_display_output.append(f"   {line}\n")
        
        # è™•ç† PDF éƒ¨åˆ†
        if pdf_part:
            # åˆ¤æ–·æ˜¯å¦ç‚º CDC å…§å®¹
            is_cdc = 'æ¨™é¡Œï¼š' in pdf_part or 'ç™¼å¸ƒæ—¥æœŸï¼š' in pdf_part or 'CDC å®˜ç¶²å³æ™‚æœå°‹' in pdf_part

            # åªå°é CDC çš„ PDF å…§å®¹ç§»é™¤ <table> æ¨™ç±¤(å› ç‚ºå·²ç¶“é¡¯ç¤ºåœ–ç‰‡)
            if not is_cdc:
                pdf_part = re.sub(r'<table>.*?</table>', '', pdf_part, flags=re.DOTALL | re.IGNORECASE)

            pdf_part = pdf_part.strip()

            if pdf_part:  # åªæœ‰åœ¨ç§»é™¤ table å¾Œé‚„æœ‰å…§å®¹æ™‚æ‰è¼¸å‡º
                pdf_paragraphs_to_output = []

                if is_cdc:
                    pdf_paragraphs_to_output.append(pdf_part.strip())
                else:
                    # å…ˆæŒ‰æ®µè½åˆ†å‰²ï¼Œå†é€²ä¸€æ­¥è™•ç†å–®è¡Œå…§å®¹
                    paragraphs = pdf_part.split('\n\n')
                    for para in paragraphs:
                        para = para.strip()

                        # å°æ–¼å¯èƒ½è·¨è¡Œçš„å…§å®¹ï¼Œå…ˆåˆä½µæˆå–®è¡Œè™•ç†
                        para = para.replace('\n', ' ').strip()

                        # ğŸ†• é è™•ç†ï¼šç§»é™¤ [ä¾†æºæ–‡ä»¶:...] æ¨™ç±¤
                        para = re.sub(r'\[ä¾†æºæ–‡ä»¶:.*?\]', '', para).strip()

                        # ğŸ†• å¥é¦–æ¸…ç†ï¼šç§»é™¤é–‹é ­çš„æ¨™é»ç¬¦è™Ÿï¼ˆå¦‚å› åˆ‡å‰²å°è‡´çš„é€—è™Ÿé–‹é ­ï¼‰
                        para = re.sub(r'^[ï¼Œã€ï¼šï¼›,:]+', '', para).strip()

                        # ğŸ†• éæ¿¾ï¼šç§»é™¤æ˜é¡¯çš„é çœ‰/é è…³/ç‰ˆæ¬Šè²æ˜/ç›®éŒ„
                        # å¢åŠ å°ç›®éŒ„ç‰¹å¾µçš„è­˜åˆ¥ (ç›®éŒ„, ...., å£¹ã€è²³ã€åƒã€)
                        is_toc = "ç›®éŒ„" in para or ".........." in para or (re.search(r'[å£¹è²³åƒè‚†ä¼é™¸æŸ’æŒ]ã€', para) and re.search(r'\.\.\.', para))
                        
                        if any(kw in para for kw in ["æœ¬è‘—ä½œéç¶“è‘—ä½œæ¬ŠäººåŒæ„", "é•·åºšé†«ç™‚è²¡åœ˜æ³•äºº", "http://", "ç·¨å°", "è‘—ä½œæ¬Šäºº"]) or is_toc:
                            # é€²ä¸€æ­¥æª¢æŸ¥æ˜¯å¦ç‚ºçŸ­é›œè¨Šè¡Œ
                            if len(para) < 100 or "http" in para or "cm" in para or is_toc:
                                continue

                        if len(para) < 10 or ('å•é¡Œ:' in para and 'ç­”æ¡ˆ:' in para):
                            continue

                        # ğŸ†• è·³éå¼•ç”¨æ–‡ç»å’Œåœ–ç‰‡æè¿°
                        if is_reference_citation(para) or is_figure_description(para):
                            continue

                        # ğŸ†• å…¨å±€å»é‡æª¢æŸ¥
                        sig = get_paragraph_signature(para)
                        if sig in global_seen_signatures:
                            continue
                        global_seen_signatures.add(sig)

                        # ğŸ†• æå–æ‰€æœ‰å®Œæ•´å¥å­ (ä»¥æ¨™é»çµå°¾ï¼ŒåŒ…å«å¾ŒçºŒæ‹¬è™Ÿ)
                        # è¦å‰‡ï¼šåŒ¹é…éæ¨™é»å­—å…ƒ + æ¨™é»ç¬¦è™Ÿ + é¸ç”¨çš„é–‰åˆç¬¦è™Ÿ
                        sentence_pattern = r'[^ã€‚ï¼ï¼Ÿ!?\.]+[ã€‚ï¼ï¼Ÿ!?\.]+[ã€ã€ã€‹ï¼‰\)]*'
                        complete_sentences = re.findall(sentence_pattern, para)
                        
                        if not complete_sentences:
                            continue
                        
                        # éæ¿¾æ‰å¯èƒ½æ˜¯å°æ•¸é»èª¤åˆ¤æˆ–éçŸ­çš„å¥å­
                        valid_sentences = []
                        for s in complete_sentences:
                            s = s.strip()
                            # å†æ¬¡æ’é™¤è‹±æ–‡å¥è™Ÿä½œç‚ºå°æ•¸é»çš„æ®˜ç•™
                            if s.endswith('.') and re.search(r'\d\.$', s):
                                continue
                            if len(s) > 5:
                                valid_sentences.append(s)

                        if not valid_sentences:
                            continue

                        # çµ„åˆæ‰€æœ‰å®Œæ•´å¥å­
                        para = " ".join(valid_sentences)

                        # å†æ¬¡æª¢æŸ¥é•·åº¦ï¼Œç¢ºä¿å…§å®¹å…·æœ‰å¯¦è³ªæ„ç¾©
                        if len(para) > 15:
                            pdf_paragraphs_to_output.append(para)

                # åªæœ‰ç•¶æœ‰æ®µè½è¦è¼¸å‡ºæ™‚æ‰æ·»åŠ æ¨™é¡Œ
                if pdf_paragraphs_to_output:
                    doc_display_output.extend(["â”€" * 80, "ã€åŸå§‹PDFæ®µè½ã€‘\n"])
                    for para in pdf_paragraphs_to_output:
                        doc_display_output.extend([para, ""])

        # é‡è¦ï¼šåªæœ‰ç•¶ doc_display_output æœ‰å…§å®¹æ™‚ï¼Œæ‰è¼¸å‡ºæ–‡ä»¶æ¨™é¡Œ
        if doc_display_output:
            has_valid_content = True
            output.append(f"ã€Š{doc_name}ã€‹")
            output.extend(doc_display_output)
            output.append("â”€" * 80)
            output.append("")

    # å¦‚æœæ²’æœ‰ä»»ä½•æœ‰æ•ˆå…§å®¹ï¼Œç›´æ¥è¿”å›ç©ºå­—ä¸²
    if not has_valid_content:
        return ""

    # ğŸ†• åœ¨åƒè€ƒä¾æ“šæœ€å¾ŒåŠ ä¸Šè¯çµ¡æç¤ºï¼ˆä¸é¡å¤–åŠ åˆ†éš”ç·šï¼Œä½¿ç”¨ä¸Šä¸€å€‹æ–‡ä»¶çš„åˆ†éš”ç·šï¼‰
    if output:  # ç¢ºä¿æœ‰å…§å®¹æ‰åŠ å…¥è¯çµ¡æç¤º
        output.extend(["ğŸ’¡ å¦‚æœ‰ç›¸é—œå•é¡Œï¼Œè«‹æ´½é™¢å€æ„Ÿç®¡è­·å¸«", ""])

    return "\n".join(output)


# ===== è¡¨æ ¼è™•ç† =====

def extract_tables_from_metadata(original_docs_dict: dict) -> List[dict]:
    """å¾ metadata ä¸­æå–è¡¨æ ¼åœ–ç‰‡"""
    from core.config import EXTRACTED_TABLES_DIR
    tables = []

    print(f"\nğŸ” [DEBUG] extract_tables_from_metadata: æª¢æŸ¥ {len(original_docs_dict)} å€‹æ–‡æª”")

    for doc_name, doc_info in original_docs_dict.items():
        print(f"   ğŸ“„ doc_name: {doc_name}")
        print(f"      é¡å‹: {type(doc_info)}")

        if isinstance(doc_info, dict):
            has_table = doc_info.get('has_table', False)
            table_images = doc_info.get('table_images', [])
            print(f"      has_table: {has_table}")
            print(f"      table_images: {table_images}")

            if doc_info.get('has_table') and doc_info.get('table_images'):
                # å¾ doc_name æå– PDF åç¨±ï¼ˆå»æ‰ .pdf å‰¯æª”åå’Œé ç¢¼ï¼‰
                # ä¾‹å¦‚ï¼šã€Œxxx.pdfã€æˆ–ã€Œxxx_pXXã€-> ã€Œxxxã€
                if '.pdf' in doc_name:
                    pdf_name = doc_name.split('.pdf')[0]
                else:
                    # è™•ç†å¯èƒ½çš„é ç¢¼æ ¼å¼ xxx_pXX
                    pdf_name = re.sub(r'_p\d+.*$', '', doc_name)

                print(f"      âœ… æå–çš„ pdf_name: {pdf_name}")

                # çµ„åˆå®Œæ•´è·¯å¾‘ï¼šEXTRACTED_TABLES_DIR / images / æª”æ¡ˆåç¨±
                for img_filename in doc_info['table_images']:
                    # å…¼å®¹æ–°èˆŠæ ¼å¼ï¼šå¦‚æœæ˜¯å®Œæ•´è·¯å¾‘ï¼Œå‰‡æå–æª”æ¡ˆåç¨±
                    if os.path.sep in img_filename or '/' in img_filename:
                        img_filename = os.path.basename(img_filename)

                    full_path = os.path.join(EXTRACTED_TABLES_DIR, "images", img_filename)
                    print(f"         ğŸ” æª¢æŸ¥è·¯å¾‘: {full_path}")
                    print(f"         å­˜åœ¨: {os.path.exists(full_path)}")

                    if os.path.exists(full_path):
                        tables.append({
                            'image_path': full_path,
                            'similarity': 1.0,
                            'source': 'metadata'
                        })
                        print(f"         âœ… æˆåŠŸæ·»åŠ è¡¨æ ¼åœ–ç‰‡")
                    else:
                        print(f"         âŒ è·¯å¾‘ä¸å­˜åœ¨")

    print(f"\nâœ… [DEBUG] extract_tables_from_metadata: å…±æå– {len(tables)} å€‹è¡¨æ ¼åœ–ç‰‡\n")
    return tables


def extract_table_numbers_from_answer(answer: str) -> set:
    """å¾å›ç­”ä¸­æå–è¡¨æ ¼ç·¨è™Ÿ"""
    table_numbers = set()
    chinese_to_arabic = {
        'ä¸€': '1', 'äºŒ': '2', 'ä¸‰': '3', 'å››': '4', 'äº”': '5',
        'å…­': '6', 'ä¸ƒ': '7', 'å…«': '8', 'ä¹': '9', 'å': '10'
    }
    
    patterns = [
        r'è¡¨æ ¼\s*([0-9ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)',
        r'è¡¨\s*([0-9ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)',
        r'è¡¨\s*([0-9]+[-\s]*[0-9]+)',
        r'é™„è¡¨\s*([0-9ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)',
    ]
    
    for pattern in patterns:
        for match in re.finditer(pattern, answer):
            num = match.group(1)
            table_numbers.add(num)
            if num in chinese_to_arabic:
                table_numbers.add(chinese_to_arabic[num])
            arabic_to_chinese = {v: k for k, v in chinese_to_arabic.items()}
            if num in arabic_to_chinese:
                table_numbers.add(arabic_to_chinese[num])
    
    # å¾ HTML è¡¨æ ¼æå–
    for html_match in re.finditer(r'<table[^>]*>(.*?)</table>', answer, re.DOTALL | re.IGNORECASE):
        table_content = html_match.group(1)
        caption_match = re.search(r'<caption[^>]*>(.*?)</caption>', table_content, re.IGNORECASE)
        if caption_match:
            table_numbers.add(re.sub(r'<.*?>', '', caption_match.group(1)).strip())
        for th_match in re.findall(r'<th[^>]*>(.*?)</th>', table_content, re.IGNORECASE):
            table_numbers.add(re.sub(r'<.*?>', '', th_match).strip())
    
    return table_numbers


async def match_tables_for_answer(answer: str, docs_dict: dict) -> List[dict]:
    """ç‚ºå›ç­”åŒ¹é…è¡¨æ ¼åœ–ç‰‡"""
    import sys
    
    has_table_ref = "<table>" in answer or "<TABLE>" in answer or "|" in answer or "è¡¨" in answer
    if not has_table_ref:
        return []
    
    try:
        ocr_model_path = os.path.join(os.path.dirname(__file__), 'ocr_model')
        if ocr_model_path not in sys.path:
            sys.path.insert(0, ocr_model_path)
        from retrieval.table_matcher import has_table_format, process_search_result
        
        if not has_table_format(answer):
            return []
        
        print(f"\nğŸ–¼ï¸  å›ç­”ä¸­åµæ¸¬åˆ°è¡¨æ ¼ï¼Œå˜—è©¦åŒ¹é…åœ–ç‰‡...")
        matched_tables = []
        
        for doc_name, doc_info in docs_dict.items():
            doc_content = doc_info.get('content', '') if isinstance(doc_info, dict) else str(doc_info)
            if has_table_format(doc_content):
                page_match = re.match(r'.*_p(\d+)_', doc_name)
                page_num = int(page_match.group(1)) if page_match else None
                match_result = process_search_result(content=doc_content, source_file=doc_name, page_num=page_num)
                if match_result.get('matched_tables'):
                    matched_tables.extend(match_result['matched_tables'])
        
        # å»é‡å’Œéæ¿¾
        seen = set()
        unique_tables = []
        for t in matched_tables:
            img_path = t.get('image_path', '')
            if img_path and img_path not in seen:
                seen.add(img_path)
                unique_tables.append(t)
        
        filtered = [t for t in unique_tables if t.get('similarity', 0) > 0.3]
        filtered.sort(key=lambda x: x.get('similarity', 0), reverse=True)
        
        if filtered:
            return filtered[:5]
        
        # å¾Œå‚™æ–¹æ¡ˆï¼šå¾ extracted_tables ç›®éŒ„æœå°‹
        return await fallback_table_search(answer, docs_dict)
        
    except Exception as e:
        print(f"   âš ï¸ è¡¨æ ¼åŒ¹é…å¤±æ•—: {e}")
        return []


async def fallback_table_search(answer: str, docs_dict: dict) -> List[dict]:
    """å¾Œå‚™æ–¹æ¡ˆï¼šå¾ extracted_tables ç›®éŒ„æœå°‹è¡¨æ ¼"""
    import glob
    from core.config import EXTRACTED_TABLES_DIR

    table_numbers = extract_table_numbers_from_answer(answer)
    
    source_files = set()
    for doc_name in docs_dict.keys():
        if '.pdf' in doc_name:
            pdf_name = doc_name.split('.pdf')[0].replace('ã€Š', '').replace('ã€‹', '')
            source_files.add(pdf_name)
    
    chinese_to_arabic = {
        'ä¸€': '1', 'äºŒ': '2', 'ä¸‰': '3', 'å››': '4', 'äº”': '5',
        'å…­': '6', 'ä¸ƒ': '7', 'å…«': '8', 'ä¹': '9', 'å': '10'
    }
    
    fallback_tables = []
    for source_file in source_files:
        subdir = os.path.join(EXTRACTED_TABLES_DIR, source_file)
        if not os.path.exists(subdir):
            continue
        
        for jpg_file in glob.glob(os.path.join(subdir, "*_t*.jpg")):
            html_file = jpg_file.replace('.jpg', '.html')
            if not os.path.exists(html_file):
                continue
            
            try:
                filename_match = re.search(r'_t(\d+)\.jpg$', jpg_file)
                filename_table_num = filename_match.group(1) if filename_match else None
                
                with open(html_file, 'r', encoding='utf-8') as f:
                    html_content = f.read()
                
                h2_match = re.search(r'<h2>(.*?)</h2>', html_content)
                header_content = h2_match.group(1) if h2_match else html_content.split('<table>')[0] if '<table>' in html_content else html_content[:200]
                
                # æå–æ¨™é¡Œä¸­çš„è¡¨æ ¼ç·¨è™Ÿ
                title_table_nums = set()
                for pattern in [r'è¡¨æ ¼\s*([0-9ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)', r'è¡¨\s*([0-9ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)']:
                    for match in re.finditer(pattern, header_content):
                        num = match.group(1)
                        title_table_nums.add(num)
                        if num in chinese_to_arabic:
                            title_table_nums.add(chinese_to_arabic[num])
                
                if filename_table_num:
                    title_table_nums.add(filename_table_num)
                
                # åˆ¤æ–·æ˜¯å¦åŒ¹é…
                is_matched = bool(table_numbers & title_table_nums) if table_numbers else True
                
                if is_matched:
                    fallback_tables.append({
                        'table_title': header_content.strip() or os.path.basename(jpg_file),
                        'table_content': '',
                        'image_path': jpg_file,
                        'similarity': 0.9 if (table_numbers & title_table_nums) else 0.5,
                        'source': 'matching'
                    })
            except Exception as e:
                print(f"         âš ï¸ è®€å– {jpg_file} å¤±æ•—: {e}")
    
    if fallback_tables:
        fallback_tables.sort(key=lambda x: x.get('similarity', 0), reverse=True)
        return fallback_tables[:5]
    
    return []


def format_table_section(matched_tables: List[dict]) -> str:
    """æ ¼å¼åŒ–è¡¨æ ¼åœ–ç‰‡å€å¡Š"""
    if not matched_tables:
        return ""
    
    section = "\n\n**ğŸ“Š ç›¸é—œè¡¨æ ¼åœ–ç‰‡**\n"
    for idx, table_info in enumerate(matched_tables, 1):
        image_path = table_info.get('image_path', '')
        similarity = table_info.get('similarity', 0.0)
        source = table_info.get('source', 'matching')

        # å¾ image_path ç²å–æ–‡ä»¶å
        table_file = os.path.basename(image_path) if image_path else 'æœªçŸ¥æª”æ¡ˆ'

        # æå–è¡¨æ ¼æ¨™é¡Œ
        title_match = re.match(r'(.+?)_p(\d+)_t(\d+)', table_file.replace('.jpg', ''))
        if title_match:
            table_title = f"{title_match.group(1)} ç¬¬{title_match.group(2)}é  è¡¨æ ¼{title_match.group(3)}"
        else:
            table_title = table_file

        if source == 'metadata':
            section += f"\n{idx}. **{table_title}**"
        else:
            section += f"\n{idx}. **{table_title}** (ç›¸ä¼¼åº¦: {similarity:.2%})"

        if image_path:
            section += f"\n   - åœ–ç‰‡: `{table_file}`"
    
    return section


def format_educational_image_section(matched_images: List[dict]) -> str:
    """æ ¼å¼åŒ–è¡›æ•™åœ–ç‰‡å€å¡Š"""
    if not matched_images:
        return ""
    
    section = "\n\n**ğŸ–¼ï¸ ç›¸é—œè¡›æ•™åœ–ç‰‡**\n"
    for idx, img_info in enumerate(matched_images, 1):
        filename = img_info.get('filename', 'æœªçŸ¥æª”æ¡ˆ')
        topic = img_info.get('health_topic', 'æœªçŸ¥ä¸»é¡Œ')
        score = img_info.get('score', 0.0)
        
        section += f"\n{idx}. **{topic}**"
        section += f"\n   - åœ–ç‰‡: `{filename}`"
    
    return section


# ===== æœå°‹å·¥å…·ç›¸é—œ =====

def should_use_cdc_realtime(query: str, query_type: str = "") -> bool:
    """åˆ¤æ–·æ˜¯å¦éœ€è¦ä½¿ç”¨ CDC å³æ™‚æœå°‹"""
    time_keywords = ["æœ€æ–°", "è¿‘æœŸ", "ä»Šå¹´", "æœ¬æœˆ", "æœ¬é€±", "ç¾åœ¨", "ç›®å‰", "ç•¶å‰"]
    realtime_keywords = ["ç–«æƒ…", "çµ±è¨ˆ", "æ•¸æ“š", "è¶¨å‹¢", "é€šå ±", "æ¡ˆä¾‹æ•¸", "ç—…ä¾‹æ•¸"]
    
    has_time = any(kw in query for kw in time_keywords)
    has_realtime = any(kw in query for kw in realtime_keywords)
    is_medical = query_type in ["medical", "disease", "symptom", ""]
    
    return (has_time and is_medical) or has_realtime


def is_regulatory_content(content: str, url: str = "") -> bool:
    """
    åˆ¤æ–·å…§å®¹æ˜¯å¦ç‚ºæ³•è¦æ¢æ–‡é¡æ–‡ä»¶ï¼ˆä¸ç›¸é—œçš„è¡Œæ”¿æ–‡ä»¶ï¼‰

    Args:
        content: æ–‡ä»¶å…§å®¹
        url: æ–‡ä»¶ URLï¼ˆå¯é¸ï¼‰

    Returns:
        True è¡¨ç¤ºæ˜¯æ³•è¦é¡æ–‡ä»¶ï¼Œæ‡‰è©²è¢«éæ¿¾æ‰
    """
    # æª¢æŸ¥ URL æ˜¯å¦ç‚ºæ³•è¦ç¶²ç«™
    if 'law.moj.gov.tw' in url or 'lawbank.com.tw' in url:
        return True

    # æª¢æŸ¥å…§å®¹ä¸­æ˜¯å¦åŒ…å«å¤§é‡æ³•è¦ç‰¹å¾µè©å½™
    regulatory_keywords = [
        'ä¸»ç®¡æ©Ÿé—œï¼š',
        'ä¾æœ¬æ³•',
        'ç¬¬ä¸€æ¢',
        'ç¬¬äºŒæ¢',
        'ç¬¬ä¸‰æ¢',
        'æ‡‰é…åˆåŠå”åŠ©è¾¦ç†',
        'ä¸­è¯æ°‘åœ‹',
        'ç¸½çµ±ä»¤',
        'è¡Œæ”¿é™¢',
    ]

    # è¨ˆç®—æ³•è¦é—œéµè©å‡ºç¾æ¬¡æ•¸
    keyword_count = sum(1 for keyword in regulatory_keywords if keyword in content)

    # å¦‚æœå‡ºç¾ 3 å€‹ä»¥ä¸Šæ³•è¦é—œéµè©ï¼Œåˆ¤å®šç‚ºæ³•è¦æ–‡ä»¶
    if keyword_count >= 3:
        return True

    # æª¢æŸ¥æ˜¯å¦ç‚ºæ¢æ–‡åˆ—èˆ‰æ ¼å¼ï¼ˆä¸€ã€äºŒã€ä¸‰...ï¼‰
    # ä¸”å…§å®¹æ˜¯éƒ¨é–€è·è²¬è€Œéé†«ç™‚æŒ‡å¼•
    if 'ä¸€ã€' in content and 'äºŒã€' in content and 'ä¸‰ã€' in content:
        # æª¢æŸ¥æ˜¯å¦ç‚ºéƒ¨é–€è·è²¬åˆ—èˆ‰
        department_keywords = ['ä¸»ç®¡æ©Ÿé—œ', 'å”åŠ©è¾¦ç†', 'æ”¿ç­–å”èª¿', 'ç®¡åˆ¶ç­‰äº‹é …']
        if any(keyword in content for keyword in department_keywords):
            return True

    return False


def extract_search_sources_with_content(realtime_info: str, tool_name: str = "æœå°‹å·¥å…·") -> dict:
    """å¾æœå°‹å·¥å…·çµæœä¸­æå–ä¾†æº URL å’Œå…§å®¹ï¼Œä¸¦éæ¿¾æ³•è¦é¡æ–‡ä»¶"""
    sources_dict = {}
    # ä¿®æ”¹æ­£å‰‡:åŒ¹é…å¾ã€ŠURLã€‹åˆ°ä¸‹ä¸€å€‹ã€Šæˆ–å­—ç¬¦ä¸²çµå°¾çš„æ‰€æœ‰å…§å®¹
    pattern = r'ã€Š([^ã€‹]+)ã€‹(.*?)(?=\nã€Š|$)'

    for url, content in re.findall(pattern, realtime_info, re.DOTALL):
        url, content = url.strip(), content.strip()
        if url.startswith(('http://', 'https://')) and content:
            # æª¢æŸ¥æ˜¯å¦ç‚ºæ³•è¦é¡æ–‡ä»¶
            if is_regulatory_content(content, url):
                print(f"   [éæ¿¾] å·²éæ¿¾æ³•è¦é¡æ–‡ä»¶: {url[:80]}...")
                continue

            sources_dict[url] = content
            print(f"   [æå–] {tool_name} ä¾†æº: {url[:80]}... (å…§å®¹é•·åº¦: {len(content)} å­—å…ƒ)")
            # èª¿è©¦è¼¸å‡ºå‰100å­—å…ƒ
            if tool_name == "CDC":
                print(f"      å…§å®¹é è¦½: {content[:150]}...")

    return sources_dict


def extract_cdc_sources_with_content(realtime_info: str) -> dict:
    """å¾ CDC å³æ™‚æœå°‹çµæœä¸­æå–ä¾†æº"""
    return extract_search_sources_with_content(realtime_info, "CDC")


# ===== ç¯€é»å‡½æ•¸ =====

def extract_current_query(state: FullState):
    """æå–ç•¶å‰æŸ¥è©¢"""
    current_query = state.messages[-1].content if state.messages else ""
    LogHelper.section(f"â“ ç•¶å‰å•é¡Œ: {current_query}")
    
    return {
        "current_query": current_query,
        "knowledge": "",
        "query_type": "",
        "memory_summary": "",
        "memory_response": "",
        "memory_source": "",
        "knowledge_retrieval_count": 0,
        "validation_need_supplement_info": "",
        "suggested_sub_questions": [],
        "iteration_count": 0
    }


async def retrieve_memory(state: FullState):
    """å¾ mem0 æª¢ç´¢è¨˜æ†¶ï¼ˆå„ªåŒ–ç‰ˆï¼šä½¿ç”¨ç•¶å‰å•é¡Œä½œç‚ºæŸ¥è©¢ï¼‰"""
    from core.config import LONG_TERM_MEMORY_CONFIG

    use_memory = (
        state.enable_long_term_memory
        if hasattr(state, 'enable_long_term_memory') and state.enable_long_term_memory is not None
        else LONG_TERM_MEMORY_CONFIG.get('enabled', False)
    )

    if not use_memory:
        print("âš ï¸ é•·æœŸè¨˜æ†¶å·²åœç”¨")
        return {"memory_summary": ""}

    try:
        # ğŸ”§ å„ªåŒ–ï¼šä½¿ç”¨ç•¶å‰å•é¡Œä½œç‚ºæŸ¥è©¢ï¼Œæé«˜æª¢ç´¢ç²¾æº–åº¦
        query_text = state.current_query if state.current_query else "ç›®å‰é‚„æœ‰çš„å€‹äººç—…å²ã€å¥åº·ç‹€æ³ã€ç”Ÿæ´»ç¿’æ…£ç­‰ç›¸é—œè³‡è¨Š"

        print("\n" + "="*80)
        print("ğŸ” ã€é•·æœŸè¨˜æ†¶æª¢ç´¢ã€‘é–‹å§‹")
        print("="*80)
        print(f"ğŸ“ æŸ¥è©¢å…§å®¹: {query_text}")
        print(f"ğŸ‘¤ ç”¨æˆ¶ ID: {state.user_id}")
        print(f"ğŸ”¢ æª¢ç´¢æ•¸é‡é™åˆ¶: {MEMORY_SEARCH_LIMIT}")
        print("-"*80)

        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(
            None,
            lambda: memory_client.search(
                query=query_text,
                user_id=state.user_id,
                limit=MEMORY_SEARCH_LIMIT
            )
        )

        # ğŸ”§ èª¿è©¦ï¼šæ‰“å°åŸå§‹æª¢ç´¢çµæœ
        print("\nğŸ“¦ åŸå§‹æª¢ç´¢çµæœ:")
        print(json.dumps(result, ensure_ascii=False, indent=2))
        print("-"*80)

        memory_texts = parse_memory_results(result)
        memory_summary = "\n".join(f"- {m}" for m in memory_texts) if memory_texts else ""

        if memory_texts:
            print(f"\nâœ… æˆåŠŸæ‰¾åˆ° {len(memory_texts)} æ¢ç›¸é—œè¨˜æ†¶")
            print("="*80)
            print("ğŸ“‹ è§£æå¾Œçš„è¨˜æ†¶åˆ—è¡¨:")
            print("="*80)
            for i, mem in enumerate(memory_texts, 1):
                print(f"ã€è¨˜æ†¶ #{i}ã€‘ {mem}")
            print("="*80)

            print("\nğŸ“„ æœ€çµ‚ memory_summary:")
            print("-"*80)
            print(memory_summary)
            print("-"*80)
        else:
            print("\nâ„¹ï¸ æœªæ‰¾åˆ°ç›¸é—œçš„é•·æœŸè¨˜æ†¶")
            print("="*80)

        return {"memory_summary": memory_summary}
    except Exception as e:
        print(f"\nâŒ è¨˜æ†¶æª¢ç´¢å¤±æ•—: {e}")
        print("="*80)
        import traceback
        traceback.print_exc()
        print("="*80)
        return {"memory_summary": ""}


async def answer_from_memory(state: FullState):
    """æª¢æŸ¥ç•¶å‰å•é¡Œæ˜¯å¦éœ€è¦ç²¾ç¢ºåŒ–"""
    if not is_short_term_memory_enabled(state):
        print("â­ï¸ çŸ­æœŸè¨˜æ†¶å·²åœç”¨ï¼Œè·³éå•é¡Œç²¾ç¢ºåŒ–")
        return {"memory_response": "No_Answer", "memory_source": ""}

    print("ğŸ” æª¢æŸ¥æ˜¯å¦éœ€è¦ç²¾ç¢ºåŒ–å•é¡Œ...")

    # ğŸ”§ ä¿®æ­£ï¼šåªä¿ç•™æœ€è¿‘çš„ 3 è¼ªå°è©±ï¼ˆ6 æ¢æ¶ˆæ¯ï¼‰ï¼Œé¿å…éé•·çš„å°è©±æ­·å²å°è‡´èª¤åˆ¤
    # æ’é™¤ç•¶å‰å•é¡Œï¼ˆæœ€å¾Œä¸€æ¢æ¶ˆæ¯ï¼‰ï¼Œåªçœ‹ä¹‹å‰çš„å°è©±
    recent_messages = state.messages[-7:-1] if len(state.messages) > 1 else []
    conversation_history = build_conversation_context(recent_messages, include_current=False)

    if state.memory_summary:
        conversation_history += f"\nã€å€‹äººç—…å²ã€‘\n{state.memory_summary}\n"

    prompt = SHORT_TERM_MEMORY_CHECK_PROMPT.format(
        conversation_history=conversation_history or "ï¼ˆç›®å‰æ²’æœ‰å°è©±æ­·å²å’Œå€‹äººç—…å²ï¼‰",
        current_query=state.current_query
    )
    
    try:
        messages = [SystemMessage(content=prompt), HumanMessage(content=state.current_query)]
        response = await llm.ainvoke(messages)
        result = remove_think_tags(response.content.strip())

        # ä¸éœ€è¦ç²¾ç¢ºåŒ–
        if result == "No_Answer":
            return {"memory_response": "No_Answer", "memory_source": ""}

        # ç²¾ç¢ºåŒ–å¾Œçš„å•é¡Œ
        print(f"âœ… å•é¡Œå·²ç²¾ç¢ºåŒ–: {result}\n")
        return {"current_query": result, "memory_response": "No_Answer", "memory_source": ""}
        
    except Exception as e:
        print(f"âš ï¸ å•é¡Œç²¾ç¢ºåŒ–å¤±æ•—: {e}")
        return {"memory_response": "No_Answer", "memory_source": ""}




async def planning_node(state: FullState) -> Dict:
    """
    è¦åŠƒç¯€é»ï¼šåˆ†æå•é¡Œæ˜¯å¦éœ€è¦åˆ†æ­¥æª¢ç´¢

    ä¸»å‹•åˆ¤æ–·å•é¡Œè¤‡é›œåº¦,ç”Ÿæˆæµç¨‹æ€§çš„æª¢ç´¢æ­¥é©Ÿã€‚
    é€™æ¯”åœ¨é©—è­‰éšæ®µè¢«å‹•ç”Ÿæˆå­å•é¡Œæ›´é«˜æ•ˆã€‚
    """
    print("\n" + "="*80)
    print("ğŸ¯ Planning éšæ®µï¼šåˆ†æå•é¡Œè¤‡é›œåº¦")
    print("="*80)

    # æ§‹å»ºå°è©±ä¸Šä¸‹æ–‡
    conversation_context = ""
    if is_short_term_memory_enabled(state) and state.messages:
        conversation_context = build_conversation_context(state.messages, include_current=False)

    # ğŸ†• æª¢æŸ¥å¿«å–ï¼ˆğŸ”’ åŠ å…¥ user_id é˜²æ­¢éš±ç§æ´©æ¼ï¼‰
    cache_manager = get_cache_manager()
    if cache_manager:
        cached_result = cache_manager.get_planning_cache(
            state.current_query,
            conversation_context,
            user_id=state.user_id  # ğŸ”’ éš”é›¢ä¸åŒç”¨æˆ¶çš„å¿«å–
        )
        if cached_result:
            print("âœ… ä½¿ç”¨å¿«å–çš„ Planning çµæœ")
            return cached_result

    # èª¿ç”¨ LLM é€²è¡Œè¦åŠƒ
    prompt = QUERY_PLANNING_PROMPT.format(
        user_query=state.current_query,
        conversation_context=conversation_context or "ï¼ˆç„¡å°è©±æ­·å²ï¼‰"
    )

    try:
        parser = PydanticOutputParser(pydantic_object=QueryPlanningResult)
        chain = llm | parser
        result: QueryPlanningResult = await chain.ainvoke(prompt)

        if result.needs_planning:
            print(f"\nâœ… éœ€è¦åˆ†æ­¥æª¢ç´¢")
            print(f"ğŸ“ ç†ç”±: {result.reasoning}")
            print(f"\nğŸ“‹ æª¢ç´¢è¨ˆåŠƒï¼ˆå…± {len(result.retrieval_steps)} æ­¥ï¼‰:")
            for step in result.retrieval_steps:
                print(f"  æ­¥é©Ÿ {step.step}: {step.query}")
                print(f"    ç›®çš„: {step.purpose}")
            print()

            output = {
                "planning_result": result.model_dump(),
                "retrieval_steps": [step.model_dump() for step in result.retrieval_steps]
            }
        else:
            print(f"\nâ­ï¸  ä¸éœ€è¦åˆ†æ­¥æª¢ç´¢")
            print(f"ğŸ“ ç†ç”±: {result.reasoning}\n")
            output = {
                "planning_result": result.model_dump(),
                "retrieval_steps": []
            }

        # ğŸ†• å„²å­˜åˆ°å¿«å–ï¼ˆğŸ”’ åŠ å…¥ user_idï¼‰
        if cache_manager:
            cache_manager.set_planning_cache(
                state.current_query,
                output,
                conversation_context,
                user_id=state.user_id  # ğŸ”’ éš”é›¢ä¸åŒç”¨æˆ¶çš„å¿«å–
            )

        return output

    except Exception as e:
        print(f"âš ï¸ Planning å¤±æ•—ï¼Œå›é€€åˆ°ç›´æ¥æª¢ç´¢: {e}")
        traceback.print_exc()
        fallback_result = {
            "planning_result": {
                "needs_planning": False,
                "reasoning": f"Planning å¤±æ•—: {str(e)}",
                "retrieval_steps": []
            },
            "retrieval_steps": []
        }

        # ğŸ†• ä¹Ÿå¿«å–å¤±æ•—çµæœï¼ˆé¿å…é‡è¤‡å˜—è©¦ï¼ŒğŸ”’ åŠ å…¥ user_idï¼‰
        if cache_manager:
            cache_manager.set_planning_cache(
                state.current_query,
                fallback_result,
                conversation_context,
                user_id=state.user_id  # ğŸ”’ éš”é›¢ä¸åŒç”¨æˆ¶çš„å¿«å–
            )

        return fallback_result


async def classify_query_type(state: FullState):
    """åˆ†é¡æŸ¥è©¢é¡å‹"""
    if state.query_type:
        print(f"ğŸ¯ ä½¿ç”¨é è¨­çš„æŸ¥è©¢é¡å‹: {state.query_type}\n")
        return {"query_type": state.query_type}
    
    current_query = state.current_query
    context = ""
    
    if is_short_term_memory_enabled(state):
        if state.memory_summary:
            context += f"\nã€è¨˜æ†¶ã€‘\n{state.memory_summary}\n"
        conversation = build_conversation_context(state.messages[:-1] if len(state.messages) > 1 else [])
        if conversation:
            context += f"\nã€å°è©±æ­·å²ã€‘\n{conversation}"
        
        # ä¸Šä¸‹æ–‡è£œå…¨
        try:
            from core.prompt_config import CONTEXT_COMPLETION_PROMPT
            if context.strip():
                prompt = CONTEXT_COMPLETION_PROMPT.format(history_context=context, current_query=current_query)
                response = await llm.ainvoke([HumanMessage(content=prompt)])
                data = json.loads(remove_think_tags(response.content.strip()))
                if data.get("needs_completion"):
                    current_query = data.get("completed_query", current_query)
        except Exception as e:
            print(f"âš ï¸ ä¸Šä¸‹æ–‡è£œå…¨å¤±æ•—: {e}")
        
        # ç—‡ç‹€æª¢æ¸¬
        try:
            from core.prompt_config import SYMPTOM_EXTRACTION_PROMPT
            prompt = SYMPTOM_EXTRACTION_PROMPT.format(user_query=current_query, history_context=context or "ç„¡")
            response = await llm.ainvoke([HumanMessage(content=prompt)])
            data = json.loads(remove_think_tags(response.content.strip()))
            
            if data.get("has_new_symptom") and data.get("current_symptoms") and data.get("historical_symptoms"):
                return {
                    "query_type": "rag_needed",
                    "current_query": f"{current_query}\n[ç³»çµ±è¨»è¨˜ï¼šç”¨æˆ¶ä¹‹å‰é‚„æåˆ°éï¼š{', '.join(data['historical_symptoms'])}]"
                }
        except Exception as e:
            print(f"âš ï¸ ç—‡ç‹€æª¢æ¸¬å¤±æ•—: {e}")
    
    # æ­£å¸¸åˆ†é¡
    prompt = get_query_classification_prompt(query=current_query, history_summary=context or "ç„¡æ­·å²è¨˜éŒ„")
    
    try:
        response = await llm.ainvoke(prompt)
        query_type = remove_think_tags(response.content.strip()).lower()
        valid_types = ["out_of_scope", "short_term", "rag_needed", "greet"]
        query_type = query_type if query_type in valid_types else "rag_needed"
        return {"query_type": query_type, "current_query": current_query}
    except:
        return {"query_type": "rag_needed", "current_query": current_query}


async def retrieve_knowledge(state: FullState):
    """æª¢æª¢ç´¢çŸ¥è­˜ - æ”¯æ´ Planning å¤šæ­¥é©Ÿæª¢ç´¢"""
    LogHelper.section(f"ğŸ” é–‹å§‹çŸ¥è­˜æª¢ç´¢ - ç•¶å‰è¿­ä»£æ¬¡æ•¸: {state.iteration_count}/{WORKFLOW_LIMITS['max_iteration_count']}")

    # å®‰å…¨æª¢æŸ¥
    if state.iteration_count >= WORKFLOW_LIMITS['retrieval_iteration_threshold']:
        print(f"âš ï¸ è¿­ä»£æ¬¡æ•¸å·²é”é–¾å€¼ï¼Œè·³éæª¢ç´¢")
        return {
            "knowledge": "", "used_sources_dict": {}, "knowledge_retrieval_count": state.knowledge_retrieval_count,
            "iteration_count": state.iteration_count + 1
        }

    current_query = state.current_query
    user_id = state.user_id

    # ğŸ†• æª¢ç´¢ç›¸é—œè¡›æ•™åœ–ç‰‡ï¼ˆåƒ…ç•¶ä½¿ç”¨è€…é¸æ“‡è©²è³‡æ–™æºæ™‚åŸ·è¡Œï¼‰
    # é è¨­æƒ…æ³ä¸‹ï¼ˆdatasource_ids ç‚º Noneï¼‰ï¼Œæˆ‘å€‘ä¿æŒé–‹å•Ÿä»¥ç¶­æŒåŸæœ‰åŠŸèƒ½
    should_retrieve_images = True
    if state.datasource_ids is not None:
        should_retrieve_images = "educational_images" in state.datasource_ids

    educational_images = []
    if should_retrieve_images:
        # æ ¹æ“šä½¿ç”¨è€…é¸æ“‡çš„æ–‡å­—è³‡æ–™æºï¼Œè½‰æ›ç‚ºå°æ‡‰çš„åœ–ç‰‡è³‡æ–™æºéæ¿¾
        image_mapper = get_image_mapper()
        image_datasource_ids = image_mapper.convert_text_datasources_to_image_datasources(
            state.datasource_ids
        )
        if image_datasource_ids:
            print(f"ğŸ–¼ï¸  åœ–ç‰‡è³‡æ–™æºéæ¿¾: {image_datasource_ids}")

        educational_images = await retrieve_relevant_images(
            current_query,
            image_datasource_ids=image_datasource_ids
        )
        if educational_images:
            print(f"ğŸ–¼ï¸  æª¢ç´¢åˆ° {len(educational_images)} å¼µç›¸é—œè¡›æ•™åœ–ç‰‡")
        else:
            print("ğŸ–¼ï¸  æœªæª¢ç´¢åˆ°ç›¸é—œè¡›æ•™åœ–ç‰‡")
    else:
        print("ğŸ–¼ï¸  è¡›æ•™åœ–ç‰‡æª¢ç´¢å·²åœç”¨")

    # ===== å„ªå…ˆä½¿ç”¨ Planning æ­¥é©Ÿ =====
    if state.retrieval_steps and len(state.retrieval_steps) > 0:
        print(f"\nğŸ¯ ä½¿ç”¨ Planning è¦åŠƒçš„æª¢ç´¢æ­¥é©Ÿï¼ˆå…± {len(state.retrieval_steps)} æ­¥ï¼‰")

        # æå–æ‰€æœ‰æ­¥é©Ÿçš„æŸ¥è©¢
        planned_queries = [step['query'] for step in state.retrieval_steps]

        # ğŸ†• ç¢ºä¿åŸå§‹ä¸»å•é¡Œä¹Ÿè¢«åŒ…å«åœ¨æª¢ç´¢ä¸­
        planned_queries = ensure_main_query_first(planned_queries, current_query)

        # ä¾åºåŸ·è¡Œå¤šæ­¥é©Ÿæª¢ç´¢
        combined_knowledge, combined_sources, combined_tables = await retrieve_multiple_queries(
            planned_queries, current_query, user_id, state.datasource_ids, state.used_sources_dict
        )

        # å¦‚æœä¹‹å‰å·²æœ‰çŸ¥è­˜ï¼Œå‰‡åˆä½µ
        if state.knowledge:
            combined_knowledge = state.knowledge + "\n\n---\n\n" + combined_knowledge

        actual_queries = planned_queries

    # ===== å›é€€: ä½¿ç”¨èˆŠçš„å­å•é¡Œæ–¹å¼ï¼ˆé©—è­‰éšæ®µç”Ÿæˆï¼‰ =====
    elif state.suggested_sub_questions:
        sub_questions = ensure_main_query_first(state.suggested_sub_questions.copy(), current_query)
        print(f"ğŸ”„ ä½¿ç”¨é©—è­‰éšæ®µå»ºè­°çš„å­å•é¡Œï¼ˆå…± {len(sub_questions)} å€‹ï¼‰")

        combined_knowledge, combined_sources, combined_tables = await retrieve_multiple_queries(
            sub_questions, current_query, user_id, state.datasource_ids, state.used_sources_dict
        )

        if state.knowledge:
            combined_knowledge = state.knowledge + "\n\n---\n\n" + combined_knowledge

        actual_queries = sub_questions

    elif state.validation_need_supplement_info:
        sub_questions = await generate_sub_questions(
            current_query, state.validation_need_supplement_info, reasoning=state.question_type_reasoning
        )
        sub_questions = ensure_main_query_first(sub_questions, current_query)
        print(f"ğŸ”„ æ ¹æ“šé©—è­‰åé¥‹ç”Ÿæˆå­å•é¡Œï¼ˆå…± {len(sub_questions)} å€‹ï¼‰")

        combined_knowledge, combined_sources, combined_tables = await retrieve_multiple_queries(
            sub_questions, current_query, user_id, state.datasource_ids, state.used_sources_dict
        )

        if state.knowledge:
            combined_knowledge = state.knowledge + "\n\n---\n\n" + combined_knowledge

        actual_queries = sub_questions

    # ===== ç›´æ¥æª¢ç´¢ï¼ˆç°¡å–®å•é¡Œï¼‰ =====
    else:
        print("â© ç›´æ¥æª¢ç´¢ï¼ˆç„¡ Planning æ­¥é©Ÿï¼‰")
        combined_knowledge, _, combined_sources, combined_tables = await retrieve_single_query(
            current_query, store=clue_store, user_id=user_id, datasource_ids=state.datasource_ids,
            is_main_question=True  # ğŸ”’ ç›´æ¥æª¢ç´¢æ™‚å°±æ˜¯ä¸»å•é¡Œï¼Œä¸å¿«å–
        )
        actual_queries = [current_query]

    # æª¢æŸ¥æª¢ç´¢çµæœ (æ–‡å­— + åœ–ç‰‡)
    has_information = bool(combined_sources) or bool(educational_images)

    if not has_information:
        print("âš ï¸ æ‰€æœ‰æª¢ç´¢ï¼ˆåŒ…å«æ–‡å­—èˆ‡åœ–ç‰‡ï¼‰éƒ½æ²’æœ‰æ‰¾åˆ°æœ‰æ•ˆåƒè€ƒå…§å®¹")
        return {
            "knowledge": "", "used_sources_dict": {}, "original_docs_dict": {},
            "knowledge_retrieval_count": state.knowledge_retrieval_count + 1,
            "query_type": "out_of_scope", "actual_search_queries": [],
            "iteration_count": state.iteration_count + 1,
            "matched_educational_images": []
        }

    # ğŸ†• å¼·åˆ¶æ³¨å…¥åœ–ç‰‡åˆ†æå…§å®¹ï¼šç„¡è«–æ˜¯å¦æœ‰æ–‡å­—ä¾†æºï¼Œåªè¦æœ‰è¡›æ•™åœ–ç‰‡ï¼Œå°±å°‡å…¶æè¿°åŠ å…¥çŸ¥è­˜åº«
    if educational_images:
        print(f"ğŸ–¼ï¸  å°‡ {len(educational_images)} å¼µè¡›æ•™åœ–ç‰‡çš„åˆ†æå…§å®¹æ³¨å…¥ç‚ºçŸ¥è­˜ç·šç´¢")
        image_clues = []
        for img in educational_images:
            # æ ¼å¼åŒ–åœ–ç‰‡å…§å®¹ï¼Œæ˜ç¢ºå‘ŠçŸ¥ LLM é€™æ˜¯åœ–ç‰‡è³‡è¨Š
            topic = img.get('health_topic', 'æœªå‘½ååœ–ç‰‡')
            desc = img.get('detailed_description', 'ç„¡è©³ç´°æè¿°')
            filename = img.get('filename', '')
            
            # ğŸ†• å»é‡æª¢æŸ¥ï¼šé¿å…é‡è¤‡æ³¨å…¥ç›¸åŒçš„åœ–ç‰‡è³‡è¨Š
            clue_signature = f"ã€è¡›æ•™åœ–ç‰‡è³‡è¨Š - {topic}ã€‘\n(æª”å: {filename})"
            if clue_signature in combined_knowledge:
                continue

            image_clues.append(f"{clue_signature}\nåœ–ç‰‡å…§å®¹åˆ†æ: {desc}")
        
        # çµ„åˆåœ–ç‰‡è£œå……è³‡è¨Šï¼ˆå¦‚æœæœ‰æ–°å…§å®¹ï¼‰
        if image_clues:
            image_knowledge_block = (
                "ã€ç³»çµ±è¨»è¨˜ï¼šä»¥ä¸‹æ˜¯æª¢ç´¢åˆ°çš„è¡›æ•™åœ–ç‰‡è©³ç´°å…§å®¹åˆ†æã€‚è«‹å……åˆ†åˆ©ç”¨é€™äº›è³‡è¨Šä¾†å›ç­”ä½¿ç”¨è€…çš„å•é¡Œï¼Œ"
                "é€™é€šå¸¸åŒ…å«å…·é«”çš„ç…§è­·æ­¥é©Ÿæˆ–åœ–ç¤ºèªªæ˜ã€‚è‹¥å›ç­”å…§å®¹æºè‡ªæ­¤è™•ï¼Œè«‹è‡ªç„¶åœ°èå…¥å›ç­”ä¸­ã€‚ã€‘\n\n" 
                + "\n\n".join(image_clues)
            )

            # å°‡åœ–ç‰‡è³‡è¨Šé™„åŠ åˆ°ç¾æœ‰çŸ¥è­˜ä¸­ (å¦‚æœå·²æœ‰çŸ¥è­˜å‰‡ç”¨åˆ†éš”ç·šéš”é–‹)
            if combined_knowledge:
                combined_knowledge = combined_knowledge + "\n\n---\n\n" + image_knowledge_block
            else:
                combined_knowledge = image_knowledge_block

    return {
        "knowledge": combined_knowledge,
        "used_sources_dict": combined_sources,
        "original_docs_dict": combined_sources,
        "matched_table_images": combined_tables,
        "matched_educational_images": educational_images,
        "knowledge_retrieval_count": state.knowledge_retrieval_count + 1,
        "actual_search_queries": actual_queries,
        "validation_need_supplement_info": "",
        "suggested_sub_questions": [],
        "iteration_count": state.iteration_count + 1
    }


async def generate_response(state: FullState):
    """ç”Ÿæˆå›ç­”"""
    LogHelper.section(f"ğŸ’¬ é–‹å§‹ç”Ÿæˆå›ç­” - ç•¶å‰è¿­ä»£æ¬¡æ•¸: {state.iteration_count}/{WORKFLOW_LIMITS['max_iteration_count']}")
    
    # å¤„ç†ç‰¹æ®Šç±»å‹
    if state.query_type == "greet":
        try:
            response = await llm.ainvoke(f"{GREET_PROMPTTEMPLATE}\n\nç•¶å‰å•é¡Œï¼š{state.current_query}")
            greeting = response.content.strip()
        except:
            greeting = "æ‚¨å¥½ï¼Œæˆ‘æ˜¯é•·åºšé†«é™¢å°å¹«æ‰‹ï¼Œæˆ‘æœƒåœ¨æˆ‘æ‰€çŸ¥çš„ç¯„åœå…§å›ç­”æ‚¨çš„å•é¡Œã€‚"
        return {"final_answer": greeting, "messages": [AIMessage(content=greeting)]}
    
    if state.query_type == "out_of_scope":
        msg = "æŠ±æ­‰ï¼Œé€™å€‹å•é¡Œè¶…å‡ºäº†æˆ‘çš„å°ˆæ¥­ç¯„åœã€‚"
        return {"final_answer": msg, "messages": [AIMessage(content=msg)]}
    
    # æ£€æŸ¥çŸ¥è¯†æœ‰æ•ˆæ€§
    knowledge = state.knowledge
    if not knowledge or not knowledge.strip():
        return {"final_answer": "æŠ±æ­‰ï¼Œè³‡æ–™åº«ä¸­æœªæ‰¾åˆ°å¯å›ç­”æ­¤å•é¡Œçš„é†«ç™‚è³‡è¨Šã€‚è«‹è«®è©¢å°ˆæ¥­é†«ç™‚äººå“¡æˆ–ç›¸é—œç§‘å®¤ã€‚"}
    
    # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰å­é—®é¢˜éƒ½æ— çº¿ç´¢
    parts = knowledge.split("\n\n---\n\n")
    if all("ç„¡ç·šç´¢" in p and "[å­å•é¡Œï¼š" in p for p in parts):
        return {"final_answer": "æŠ±æ­‰ï¼Œè³‡æ–™åº«ä¸­æœªæ‰¾åˆ°å¯å›ç­”æ­¤å•é¡Œçš„é†«ç™‚è³‡è¨Šã€‚è«‹è«®è©¢å°ˆæ¥­é†«ç™‚äººå“¡æˆ–ç›¸é—œç§‘å®¤ã€‚"}
    
    # æ„å»ºå†å²å¯¹è¯
    history_messages = []
    if is_short_term_memory_enabled(state) and len(state.messages) > 1:
        history_messages = state.messages[-6:-1]
    
    # é€‰æ‹© prompt
    if history_messages:
        conversation_context = build_conversation_context(history_messages)
        prompt = get_rag_response_prompt_with_history(
            knowledge=knowledge, question=state.current_query,
            memory_summary=state.memory_summary, conversation_context=conversation_context,
            question_reasoning=state.question_type_reasoning
        )
    else:
        prompt = get_rag_response_prompt(
            knowledge=knowledge, question=state.current_query,
            question_reasoning=state.question_type_reasoning
        )
    
    try:
        message_stream = llm.astream(prompt)
        full_response = await stream_llm_response(message_stream)
        
        # åå¤„ç†
        processed_response = await post_process_response(full_response, list(state.used_sources_dict.keys()))
        final_answer = converter.convert(remove_think_tags(processed_response))

        # å¤„ç†è¡¨æ ¼å›¾ç‰‡
        matched_tables = state.matched_table_images or []
        
        # ä» metadata æå–
        if not matched_tables and state.original_docs_dict:
            matched_tables = extract_tables_from_metadata(state.original_docs_dict)
        
        # åŒ¹é…è¡¨æ ¼
        if not matched_tables:
            matched_tables = await match_tables_for_answer(final_answer, state.used_sources_dict or {})
        
        # âœ… å»é‡è¡¨æ ¼å›¾ç‰‡ï¼ˆåŸºäº image_pathï¼‰
        if matched_tables:
            seen_paths = set()
            unique_tables = []
            for table in matched_tables:
                img_path = table.get('image_path', '')
                if img_path and img_path not in seen_paths:
                    seen_paths.add(img_path)
                    unique_tables.append(table)

            # æŒ‰ç›¸ä¼¼åº¦æ’åºï¼ˆé«˜åˆ°ä½ï¼‰
            unique_tables.sort(key=lambda x: x.get('similarity', 0.0), reverse=True)
            matched_tables = unique_tables

            print(f"âœ… å»é‡å¾Œå‰©é¤˜ {len(matched_tables)} å€‹å”¯ä¸€è¡¨æ ¼åœ–ç‰‡")

        # âœ… ä¿®æ”¹ï¼šåŒæ™‚é¡¯ç¤ºæ–‡å­—åƒè€ƒè³‡æ–™ã€è¡¨æ ¼åœ–ç‰‡å’Œè¡›æ•™åœ–ç‰‡
        
        # å®šç¾©ç„¡æ•ˆå›ç­”çš„åˆ—è¡¨ï¼Œé¿å…åœ¨ç„¡æ•ˆå›ç­”å¾Œé™„åŠ åƒè€ƒè³‡æ–™
        NO_ANSWER_MESSAGES = [
            "æŠ±æ­‰ï¼Œé€™å€‹å•é¡Œè¶…å‡ºäº†æˆ‘çš„å°ˆæ¥­ç¯„åœã€‚",
            "æŠ±æ­‰ï¼Œè³‡æ–™åº«ä¸­æœªæ‰¾åˆ°å¯å›ç­”æ­¤å•é¡Œçš„é†«ç™‚è³‡è¨Šã€‚è«‹è«®è©¢å°ˆæ¥­é†«ç™‚äººå“¡æˆ–ç›¸é—œç§‘å®¤ã€‚",
            "æŠ±æ­‰ï¼Œç„¡æ³•ç”Ÿæˆå›ç­”",
            "æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ™‚ç™¼ç”ŸéŒ¯èª¤ã€‚"
        ]
        
        is_no_answer = any(msg in final_answer for msg in NO_ANSWER_MESSAGES)

        # åˆå§‹åŒ–è®Šæ•¸ï¼Œé¿å… UnboundLocalError
        matched_edu_images = []
        
        if is_no_answer:
            print("âš ï¸ æª¢æ¸¬åˆ°ç„¡æ•ˆå›ç­”ï¼Œè·³éé™„åŠ åƒè€ƒè³‡æ–™å’Œåœ–ç‰‡")
        else:
            # 1. æº–å‚™æ–‡å­—åƒè€ƒè³‡æ–™
            docs_dict = state.used_sources_dict
            new_refs = ""
            if docs_dict:
                # ç§»é™¤ LLM ç”Ÿæˆçš„åƒè€ƒä¾æ“šå€å¡Š
                if "**åƒè€ƒä¾æ“š**" in final_answer:
                    final_answer = final_answer[:final_answer.find("**åƒè€ƒä¾æ“š**")].rstrip()
                new_refs = rebuild_references_from_used_sources(docs_dict)

            # 2. æº–å‚™èˆ‡å»é‡è¡›æ•™åœ–ç‰‡
            matched_edu_images = state.matched_educational_images or []
            if matched_edu_images:
                # âœ… å»é‡è¡›æ•™åœ–ç‰‡ï¼ˆåŸºæ–¼ filename èˆ‡ ä¸»é¡Œï¼‰
                seen_filenames = set()
                seen_topics = set()
                unique_edu_images = []
                for img in matched_edu_images:
                    fname = img.get('filename', '')
                    topic = img.get('health_topic', '')
                    
                    if fname and fname not in seen_filenames and topic not in seen_topics:
                        seen_filenames.add(fname)
                        seen_topics.add(topic)
                        unique_edu_images.append(img)
                matched_edu_images = unique_edu_images

            # 3. æº–å‚™è¡¨æ ¼åœ–ç‰‡ (å·²åœ¨ä¸Šæ–¹å»é‡éï¼Œé€™è£¡ç›´æ¥ä½¿ç”¨)
            # matched_tables å·²åœ¨å‰é¢è™•ç†é

            # ğŸ†• åš´æ ¼æª¢æŸ¥ï¼šå¿…é ˆè¦æœ‰æœ‰æ•ˆçš„æ–‡å­—åƒè€ƒè³‡æ–™æ‰èƒ½ç”Ÿæˆå›ç­”
            # å³ä½¿æœ‰åœ–ç‰‡ï¼Œå¦‚æœæ²’æœ‰æ–‡å­—åƒè€ƒä¾æ“šï¼Œä¹Ÿä¸æ‡‰è©²çµ¦å‡ºç¶œåˆå»ºè­°ï¼ˆé¿å…å¹»è¦ºæˆ–ç„¡æ ¹æ“šçš„å»ºè­°ï¼‰
            has_valid_refs = bool(new_refs.strip())
            
            if not has_valid_refs:
                print("âš ï¸ ç¶“éåš´æ ¼éæ¿¾å¾Œç„¡æœ‰æ•ˆæ–‡å­—åƒè€ƒè³‡æ–™ï¼ˆå³ä½¿æœ‰åœ–ç‰‡ï¼‰ï¼Œè¦–ç‚ºç„¡æ•ˆï¼Œå›é€€è‡³ç½é ­è¨Šæ¯")
                return {"final_answer": "æŠ±æ­‰ï¼Œè³‡æ–™åº«ä¸­æœªæ‰¾åˆ°å¯å›ç­”æ­¤å•é¡Œçš„é†«ç™‚è³‡è¨Šã€‚è«‹è«®è©¢å°ˆæ¥­é†«ç™‚äººå“¡æˆ–ç›¸é—œç§‘å®¤ã€‚"}

            # 4. çµ„åˆæœ€çµ‚å›ç­”
            if new_refs:
                final_answer = final_answer + "\n\n" + new_refs
                print(f"âœ… å·²æ·»åŠ æ–‡å­—åƒè€ƒè³‡æ–™")

            if matched_edu_images:
                print(f"âœ… æª¢æ¸¬åˆ° {len(matched_edu_images)} å€‹è¡›æ•™åœ–ç‰‡ï¼ˆå·²å»é‡ï¼‰ï¼Œä¸€ä½µé¡¯ç¤º")
                final_answer += format_educational_image_section(matched_edu_images)

            if matched_tables:
                print(f"âœ… æª¢æ¸¬åˆ° {len(matched_tables)} å€‹è¡¨æ ¼åœ–ç‰‡ï¼Œä¸€ä½µé¡¯ç¤º")
                final_answer += format_table_section(matched_tables)
        
        return {
            "final_answer": final_answer,
            "iteration_count": state.iteration_count + 1,
            "matched_table_images": matched_tables,
            "matched_educational_images": matched_edu_images
        }
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆå¤±æ•—: {e}")
        return {"final_answer": "æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ™‚ç™¼ç”ŸéŒ¯èª¤ã€‚"}

async def validate_answer(state: FullState) -> Dict:
    """é©—è­‰å›ç­”å“è³ª"""
    LogHelper.section(f"ğŸ” é–‹å§‹é©—è­‰å›ç­” - ç•¶å‰è¿­ä»£æ¬¡æ•¸: {state.iteration_count}/{WORKFLOW_LIMITS['max_iteration_count']}")
    
    # æª¢æŸ¥çŸ¥è­˜ç‚ºç©ºä½†æœ‰åƒè€ƒå¼•ç”¨
    if (not state.knowledge or not state.knowledge.strip()) and \
       any(kw in state.final_answer for kw in ["æ ¹æ“šè©²æ–‡ä»¶", "ã€Š", "åƒè€ƒä¾æ“š"]):
        return {
            "validation_result": "need_more_knowledge",
            "validation_feedback": "å›ç­”åŒ…å«åƒè€ƒè³‡æ–™å¼•ç”¨ä½†æ²’æœ‰å¯¦éš›æª¢ç´¢çµæœ",
            "iteration_count": state.iteration_count + 1
        }
    
    # è¿­ä»£æ¬¡æ•¸æª¢æŸ¥
    if state.iteration_count >= WORKFLOW_LIMITS['validation_iteration_threshold']:
        return {"validation_result": "out_of_scope"}
    
    if state.knowledge_retrieval_count >= WORKFLOW_LIMITS['validation_retrieval_threshold']:
        return {"validation_result": "out_of_scope"}
    
    # æº–å‚™é©—è­‰
    validate_output = PydanticOutputParser(pydantic_object=ValidationResult)
    truncated_knowledge = smart_truncate_knowledge(state.knowledge, max_chars=8192)
    
    conversation_context = ""
    if is_short_term_memory_enabled(state) and state.messages:
        conversation_context = build_conversation_context(state.messages[:-1])
    
    full_prompt = ANSWER_VALIDATION_PROMPT.format(
        user_query=state.current_query,
        assistant_answer=state.final_answer,
        knowledge_used=truncated_knowledge or "ï¼ˆç„¡æª¢ç´¢çŸ¥è­˜ï¼‰",
        conversation_context=conversation_context or "ï¼ˆç„¡å°è©±æ­·å²ï¼‰",
        format_instructions=validate_output.get_format_instructions()
    )
    
    try:
        chain = llm | validate_output
        result: ValidationResult = await chain.ainvoke(full_prompt)
        
        output = {"validation_result": result.validation_type}
        
        if result.validation_type == "need_more_knowledge":
            output["iteration_count"] = state.iteration_count + 1
            if result.missing_info:
                output["validation_need_supplement_info"] = result.missing_info
            if result.suggested_sub_questions:
                output["suggested_sub_questions"] = result.suggested_sub_questions
        elif result.validation_type == "regenerate":
            output["iteration_count"] = state.iteration_count + 1
        
        return output
        
    except Exception as e:
        print(f"âŒ é©—è­‰å¤±æ•—: {e}")
        return {"validation_result": "pass"}


async def refine_answer(state: FullState):
    """ç²¾ç…‰å›ç­”ï¼ˆä¿ç•™åƒè€ƒæ–‡ç»å’Œåœ–ç‰‡ï¼‰"""
    conversation_context = ""
    if is_short_term_memory_enabled(state) and state.messages:
        conversation_context = build_conversation_context(state.messages[:-1])

    refine_prompt = ANSWER_REGENERATE_PROMPT.format(
        current_query=state.current_query,
        conversation_context=conversation_context or "ï¼ˆç„¡å°è©±æ­·å²ï¼‰",
        validation_feedback=state.validation_feedback or "éœ€è¦æ”¹é€²å›ç­”å“è³ª"
    )

    try:
        response = await llm.ainvoke(refine_prompt)
        refined = converter.convert(remove_think_tags(response.content))

        # ğŸ†• é‡æ–°æ·»åŠ åƒè€ƒæ–‡ç»ï¼ˆå¦‚æœåŸæœ¬æœ‰çš„è©±ï¼‰
        docs_dict = state.used_sources_dict
        if docs_dict:
            # ç§»é™¤ LLM å¯èƒ½ç”Ÿæˆçš„åƒè€ƒä¾æ“šå€å¡Š
            if "**åƒè€ƒä¾æ“š**" in refined:
                refined = refined[:refined.find("**åƒè€ƒä¾æ“š**")].rstrip()
            new_refs = rebuild_references_from_used_sources(docs_dict)
            if new_refs:
                refined = refined + "\n\n" + new_refs

        # ğŸ†• é‡æ–°æ·»åŠ è¡›æ•™åœ–ç‰‡ï¼ˆå¦‚æœåŸæœ¬æœ‰çš„è©±ï¼‰
        matched_edu_images = state.matched_educational_images or []
        if matched_edu_images:
            refined += format_educational_image_section(matched_edu_images)

        # ğŸ†• é‡æ–°æ·»åŠ è¡¨æ ¼åœ–ç‰‡ï¼ˆå¦‚æœåŸæœ¬æœ‰çš„è©±ï¼‰
        matched_tables = state.matched_table_images or []
        if matched_tables:
            refined += format_table_section(matched_tables)

        return {"final_answer": refined, "retry_count": state.retry_count + 1, "iteration_count": state.iteration_count + 1}
    except:
        return {"retry_count": state.retry_count + 1}


async def check_question_type(state: FullState):
    """æª¢æŸ¥å•é¡Œé¡åˆ¥ä¸¦æå– reasoning"""
    print(f"\nğŸ” æª¢æŸ¥å•é¡Œé¡åˆ¥ - ç•¶å‰è¿­ä»£æ¬¡æ•¸: {state.iteration_count}/{WORKFLOW_LIMITS['max_iteration_count']}")
    
    conversation_history = ""
    if is_short_term_memory_enabled(state) and state.messages:
        conversation_history = build_conversation_context(state.messages, include_current=True)
    
    prompt = QUESTION_TYPE_CHECK_PROMPT.format(
        current_query=state.current_query,
        conversation_history=conversation_history or "ï¼ˆç„¡å°è©±æ­·å²ï¼‰"
    )
    
    try:
        response = await llm.ainvoke(prompt)
        content = remove_think_tags(response.content.strip())
        
        # æ¸…ç† JSON
        for prefix in ["```json", "```"]:
            if content.startswith(prefix):
                content = content[len(prefix):]
        if content.endswith("```"):
            content = content[:-3]
        
        parsed = json.loads(content.strip())
        return {
            "question_category": parsed.get("question_type", "medical"),
            "question_type_reasoning": parsed.get("reasoning", "")
        }
    except:
        return {"question_category": "medical", "question_type_reasoning": ""}


async def supplement_with_realtime_info(state: FullState):
    """
    ç”¨å³æ™‚æœå°‹å·¥å…·è£œå……çŸ¥è­˜æª¢ç´¢çµæœ

    åªè¦ä½¿ç”¨è€…é€é enabled_tool_ids é¸æ“‡äº†å¤–éƒ¨æœå°‹å·¥å…·ï¼Œå°±æœƒåŸ·è¡Œæœå°‹
    ä¸åšé¡å¤–çš„æ™ºèƒ½åˆ¤æ–·ï¼ˆå·²ç°¡åŒ–ï¼‰
    """
    enabled_tool_ids = state.enabled_tool_ids if state.enabled_tool_ids is not None else TOOLS_CONFIG.get('default_tools', [])

    # å¦‚æœå·¥å…·ç³»çµ±åœç”¨ä¸”ä½¿ç”¨è€…æ²’æœ‰æŒ‡å®šå·¥å…·ï¼Œå‰‡è·³é
    if not TOOLS_CONFIG.get('enabled', True) and state.enabled_tool_ids is None:
        return {}

    # ç²å–æœç´¢å·¥å…·
    search_tools = []
    for tool_id in enabled_tool_ids:
        tool_config = get_tool(tool_id)
        if tool_config and tool_config.metadata.get('category') == 'external_search':
            search_tools.append((tool_id, tool_config))

    # å¦‚æœæ²’æœ‰æœç´¢å·¥å…·ï¼Œå‰‡è·³éï¼ˆä½¿ç”¨è€…æœªé¸æ“‡ä»»ä½•å¤–éƒ¨æœå°‹å·¥å…·ï¼‰
    if not search_tools:
        return {}

    # ç¢ºå®šæœç´¢æŸ¥è©¢
    search_queries = state.actual_search_queries if hasattr(state, 'actual_search_queries') and state.actual_search_queries else [state.current_query]
    
    all_realtime_info = []
    all_sources_dict = {}
    
    for query in search_queries:
        if not query or len(query.strip()) < 2:
            continue
        
        for tool_id, tool_config in search_tools:
            try:
                tool_func = tool_config.tool_func
                
                # æ ¹æ“šå·¥å…·é¡å‹èª¿ç”¨
                if tool_id == "cdc_realtime_search":
                    result = tool_func.invoke({"keyword": query, "num_results": 2})
                elif tool_id == "google_realtime_search":
                    result = await tool_func.ainvoke({"query": query, "max_results": 3})
                elif tool_id == "duckduckgo_realtime_search":
                    result = await tool_func.ainvoke({
                        "query": query, "max_results": 3, "region": "tw-tzh",
                        "safe_search": "moderate", "get_full_content": True
                    })
                else:
                    result = await tool_func.ainvoke({"query": query, "max_results": 3})
                
                if result and not str(result).startswith("âŒ"):
                    all_realtime_info.append(f"\nã€{tool_config.name}ã€‘\n{'='*80}\n{result}")
                    
                    sources = extract_cdc_sources_with_content(result) if tool_id == "cdc_realtime_search" else extract_search_sources_with_content(result, tool_config.name)
                    all_sources_dict.update(sources)
                    
            except Exception as e:
                print(f"âŒ {tool_config.name} éŒ¯èª¤: {e}")
    
    if all_realtime_info:
        combined_realtime = "\n".join(all_realtime_info)
        knowledge = state.knowledge or ""
        
        combined_knowledge = (
            f"{knowledge}\n{'*'*3}\nã€å³æ™‚æœå°‹è³‡è¨Šã€‘\n{'*'*3}\n{combined_realtime}"
            if knowledge else f"ã€å³æ™‚æœå°‹è³‡è¨Šã€‘\n{'*'*3}\n{combined_realtime}"
        )
        
        updated_sources = (state.used_sources_dict.copy() if state.used_sources_dict else {})
        updated_sources.update(all_sources_dict)
        
        return {
            "knowledge": combined_knowledge,
            "used_sources_dict": updated_sources,
            "original_docs_dict": updated_sources
        }
    
    return {}


async def set_out_of_scope_message(_state: FullState):
    """è¨­ç½®è¶…å‡ºç¯„åœè¨Šæ¯"""
    from core.prompt_config import ERROR_RESPONSES
    return {"final_answer": ERROR_RESPONSES["out_of_scope"]}