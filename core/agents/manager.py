"""
Agent V4 â€” Manager Agent (LangGraph Edition)

Orchestrates the agent loop via a LangGraph StateGraph:
  classify â†’ plan â†’ execute â†’ synthesize â†’ feedback

HITL points use interrupt() so web mode can pause/resume properly.
"""
from __future__ import annotations
import json
import re
from uuid import uuid4
from dataclasses import asdict, fields
from datetime import datetime
from typing import Optional, List, Dict
try:
    from typing import NotRequired
except ImportError:
    from typing_extensions import NotRequired
from typing_extensions import TypedDict

from langchain_core.messages import HumanMessage
from langgraph.graph import StateGraph, END
from langgraph.types import interrupt, Command
from langgraph.checkpoint.memory import MemorySaver
from concurrent.futures import ThreadPoolExecutor, as_completed

from api.utils import logger  # Added logger

from .models import (
    TaskComplexity, SubTask, ExecutionContext,
    AgentResult, CollaborationRequest,
)
from .agent_registry import AgentRegistry
from .tool_registry import ToolRegistry
from .router import AgentRouter
# from .hitl import HITLManager
from .codebook import Codebook
from .prompt_registry import PromptRegistry
from .watcher import WatcherAgent
from core.tools.universal_resolver import UniversalSymbolResolver

# æ¨¡çµ„ç´šå…±ç”¨ checkpointerï¼šè·¨ bootstrap() å‘¼å«æŒä¹…åŒ– session state
_checkpointer = MemorySaver()


AGENT_ICONS: Dict[str, str] = {
    "full_analysis": "ğŸ“Š",
    "technical":     "ğŸ“ˆ",
    "sentiment":     "ğŸ’¬",
    "fundamental":   "ğŸ¢",
    "news":          "ğŸ“°",
    "chat":          "ğŸ¤–",
}


from typing import Annotated
import operator

class ManagerState(TypedDict):
    """LangGraph ç¯€é»é–“å‚³éçš„ç‹€æ…‹åŒ…ã€‚"""
    # å¿…å¡«ï¼ˆåˆå§‹åŒ–æ™‚æä¾›ï¼‰
    # Use reducer to prevent "Can receive only one value per step" error during HITL resumes
    session_id: Annotated[str, lambda x, y: y]
    query: str
    # åŸ·è¡Œä¸­å¡«å…¥
    complexity: NotRequired[Optional[str]]          # "simple" | "complex" | "ambiguous"
    intent: NotRequired[Optional[str]]
    topics: NotRequired[Optional[List[str]]]
    ambiguity_question: NotRequired[Optional[str]]
    plan: NotRequired[Optional[List[dict]]]         # List[SubTask as dict]
    agent_results: NotRequired[Optional[List[dict]]]
    user_clarifications: NotRequired[Optional[List[str]]]
    retry_count: NotRequired[Optional[int]]
    codebook_entry_id: NotRequired[Optional[str]]
    final_response: NotRequired[Optional[str]]
    plan_confirmed: NotRequired[Optional[bool]]
    history: NotRequired[Optional[str]]             # å¾ DB è¼‰å…¥çš„å°è©±æ­·å²ï¼ˆç´”æ–‡å­—ï¼‰
    # è¨ˆç•«å”å•†ï¼ˆPlan Negotiation HITLï¼‰
    plan_negotiating: NotRequired[Optional[bool]]   # æ˜¯å¦é€²å…¥è¨ˆç•«å”å•†æ¨¡å¼
    negotiation_request: NotRequired[Optional[str]] # ç”¨æˆ¶çš„ä¿®æ”¹è«‹æ±‚æ–‡å­—
    negotiation_response: NotRequired[Optional[str]]# LLM çš„å¯è¡Œæ€§å›æ‡‰
    negotiate_count: NotRequired[Optional[int]]     # å”å•†æ¬¡æ•¸ï¼ˆé˜²ç„¡é™å¾ªç’°ï¼‰
    current_tool_result: NotRequired[Optional[str]] # ç•¶å‰å”å•†è¼ªæ¬¡çš„å·¥å…·åŸ·è¡Œçµæœ
    # Plan Reflection
    plan_reflection_count: NotRequired[Optional[int]]         # how many times reflect_plan has looped
    plan_reflection_suggestion: NotRequired[Optional[str]]    # improvement hint fed back to plan node
    plan_reflection_approved: NotRequired[Optional[bool]]     # True = plan passed quality gate
    # Pre-Research éšæ®µ
    research_data: NotRequired[Optional[dict]]           # tool åŸ·è¡Œçµæœ
    research_summary: NotRequired[Optional[str]]          # äººé¡å¯è®€æ‘˜è¦ï¼ˆMarkdownï¼‰
    research_clarifications: NotRequired[Optional[List[str]]]  # ç”¨æˆ¶åœ¨ pre_research è£œå……çš„æ–¹å‘
    current_step_index: NotRequired[int] # ç•¶å‰åŸ·è¡Œæ­¥é©Ÿç´¢å¼• (0-based)
    # èªè¨€åå¥½
    language: NotRequired[Optional[str]]                  # "zh-TW" | "en"
    # è¨ˆç•«è¨è«–ï¼ˆä½¿ç”¨è€…åœ¨ HITL æå•ï¼Œå–æ¶ˆè¨ˆç•«å¾Œç›´æ¥å›ç­”ï¼‰
    is_discussion: NotRequired[Optional[bool]]
    discussion_question: NotRequired[Optional[str]]
    # Discuss Mode: free-form Q&A after plan is shown, without re-entering planning
    discuss_mode: NotRequired[Optional[bool]]             # True = discussion mode active
    discuss_plan_snapshot: NotRequired[Optional[List[dict]]]  # frozen plan being discussed
    replan_request: NotRequired[Optional[bool]]           # True = user wants a new plan


class ManagerAgent:
    def __init__(
        self,
        llm_client,
        agent_registry: AgentRegistry,
        tool_registry: ToolRegistry,
        codebook,
        web_mode: bool = False,
    ):
        self.llm = llm_client
        self.agent_registry = agent_registry
        self.tool_registry = tool_registry
        # self.hitl = hitl # Removed
        self.codebook = codebook
        self.router = AgentRouter(agent_registry)
        self.web_mode = web_mode
        self.progress_callback = None
        self.watcher = WatcherAgent(llm_client)
        self.universal_resolver = UniversalSymbolResolver()

        # ç·¨è­¯ LangGraph
        self.graph = self._build_graph()

    # â”€â”€ Graph å»ºæ§‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _build_graph(self):
        workflow = StateGraph(ManagerState)

        workflow.add_node("classify",       self._classify_node)
        workflow.add_node("clarify",        self._clarify_node)
        workflow.add_node("pre_research",   self._pre_research_node)   # è¤‡é›œä»»å‹™é ç ”ç©¶
        workflow.add_node("plan",           self._plan_node)
        workflow.add_node("reflect_plan",   self._reflect_plan_node)  # Plan quality gate
        workflow.add_node("confirm_plan",   self._confirm_plan_node)
        workflow.add_node("negotiate_plan", self._negotiate_plan_node)  # è¨ˆç•«å”å•†
        workflow.add_node("discuss",        self._discuss_node)         # è¨ˆç•«è¨è«–å•é¡Œå›ç­”
        workflow.add_node("execute",        self._execute_node)
        workflow.add_node("watcher",        self._watcher_node)
        workflow.add_node("synthesize",     self._synthesize_node)
        workflow.add_node("save",           self._save_node)

        workflow.set_entry_point("classify")

        workflow.add_conditional_edges("classify", self._after_classify, {
            "clarify":      "clarify",
            "pre_research": "pre_research",   # complex â†’ å…ˆé ç ”ç©¶
            "plan":         "plan",           # simple  â†’ ç›´æ¥è¦åŠƒ
        })
        workflow.add_edge("clarify",      "classify")    # æ¾„æ¸…å¾Œé‡æ–°åˆ†é¡
        workflow.add_conditional_edges("pre_research", self._after_pre_research, {
            "plan": "plan",   # ç”¨æˆ¶ç¢ºèªæ–¹å‘ â†’ é€²å…¥è¦åŠƒ
            "save": "save",   # ç”¨æˆ¶æå• â†’ å›ç­”å¾ŒçµæŸ
        })

        workflow.add_conditional_edges("plan", self._after_plan, {
            "reflect_plan": "reflect_plan",   # complex â†’ quality gate
            "execute":      "execute",        # simple  â†’ direct execute
        })
        workflow.add_conditional_edges("reflect_plan", self._after_reflect_plan, {
            "confirm":  "confirm_plan",   # approved â†’ show to user
            "re_plan":  "plan",           # rejected â†’ re-plan with suggestion
        })
        workflow.add_conditional_edges("confirm_plan", self._after_confirm, {
            "execute":   "execute",
            "negotiate": "negotiate_plan",         # ç”¨æˆ¶æå‡ºä¿®æ”¹ â†’ å”å•†
            "discuss":   "discuss",               # ç”¨æˆ¶æå• â†’ è¨è«–å›ç­”å¾ŒçµæŸ
            "end":       END,
        })
        workflow.add_edge("negotiate_plan", "confirm_plan")  # å”å•†å¾Œå›åˆ°ç¢ºèª
        workflow.add_edge("discuss",        "save")          # è¨è«–å›ç­”å¾Œå„²å­˜çµæŸ

        # Execution Loop: execute -> check -> (execute | watcher)
        workflow.add_conditional_edges("execute", self._after_execute, {
            "continue": "execute",
            "done":     "watcher"
        })
        
        workflow.add_edge("watcher",   "synthesize")
        workflow.add_edge("synthesize", "save")
        workflow.add_edge("save",       END)

        return workflow.compile(checkpointer=_checkpointer)

    # â”€â”€ ç¯€é»å¯¦ä½œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def _classify_node(self, state: ManagerState) -> dict:
        query = state.get("query", "")

        # â”€â”€ LLM Classificationï¼ˆå« symbol èƒå–ï¼‰â”€â”€
        # LLM ä¸€æ¬¡å®Œæˆï¼šagent é¸æ“‡ + å„å¸‚å ´ symbol èƒå–ï¼ˆå«ä»£è©è§£æï¼‰
        agents_info = self.agent_registry.agents_info_for_prompt()
        tools_info  = ", ".join([t.name for t in self.tool_registry.list_all_tools()])
        history     = state.get("history") or "ï¼ˆç„¡å°è©±æ­·å²ï¼‰"
        prompt = PromptRegistry.render(
            "manager", "classify",
            query=query,
            agents_info=agents_info,
            tools_info=tools_info,
            history=history,
        )
        try:
            import asyncio
            loop = asyncio.get_running_loop()
            response = await loop.run_in_executor(None, lambda: self.llm.invoke([HumanMessage(content=prompt)]))
            data = self._parse_json(self._llm_content(response)) or {}
        except Exception as e:
            print(f"[Manager] classify error: {e}")
            data = {"complexity": "simple", "intent": "chat", "topics": []}

        # ç›¸å®¹èˆŠç‰ˆ prompt å›å‚³ "agent" æ¬„ä½
        intent     = data.get("intent") or data.get("agent", "chat")
        complexity = data.get("complexity", "simple")
        symbols    = data.get("symbols") or {}

        # â”€â”€ Symbol-based routingï¼ˆå¾ LLM èƒå–çš„ symbols é©…å‹•ï¼‰â”€â”€
        market_to_agent = {"crypto": "crypto", "tw": "tw_stock", "us": "us_stock"}
        matched = [(m, symbols[m]) for m in ("crypto", "tw", "us") if symbols.get(m)]

        if len(matched) > 1:
            # å¤šå¸‚å ´ï¼šå»ºç«‹ç¢ºå®šæ€§ planï¼Œè·³é HITL planning
            steps = [
                {
                    "step": i,
                    "description": f"åˆ†æ {sym}",
                    "agent": market_to_agent[mkt],
                    "tool_hint": None,
                }
                for i, (mkt, sym) in enumerate(matched, 1)
            ]
            return {
                "complexity":     "complex",
                "intent":         steps[0]["agent"],
                "topics":         [sym for _, sym in matched],
                "plan":           steps,
                "plan_confirmed": True,
                # Reset reflection state to prevent leakage
                "plan_reflection_count":      0,
                "plan_reflection_suggestion": None,
                "plan_reflection_approved":   None,
            }
        elif len(matched) == 1:
            # å–®ä¸€å¸‚å ´ï¼šè¦†è“‹ intentï¼ˆLLM èƒå–æ¯” agent é¸æ“‡æ›´å¯é ï¼‰
            mkt, sym = matched[0]
            intent = market_to_agent[mkt]
            if not data.get("topics"):
                data["topics"] = [sym]

        # è‹¥ LLM å›å‚³ä¸èªè­˜çš„ agent nameï¼Œfallback åˆ° chat
        known_agents = {m.name for m in self.agent_registry.list_all()}
        if intent not in known_agents:
            logger.warning(f"[Classify] Unknown agent '{intent}', falling back to chat")
            intent = "chat"

        replan_request = bool(data.get("replan_request", False))
        result = {
            "complexity":                 complexity,
            "intent":                     intent,
            "topics":                     data.get("topics", []),
            "ambiguity_question":         data.get("ambiguity_question"),
            "replan_request":             replan_request,
            # Reset reflection state for each new query to prevent leakage
            "plan_reflection_count":      0,
            "plan_reflection_suggestion": None,
            "plan_reflection_approved":   None,
        }
        # If user explicitly re-requests planning, exit discuss mode
        if replan_request:
            result["discuss_mode"] = False
            result["discuss_plan_snapshot"] = None
        return result

    async def _clarify_node(self, state: ManagerState) -> dict:
        """HITL Point 1ï¼šæ­§ç¾©æ¾„æ¸…ã€‚"""
        question = state.get("ambiguity_question") or "è«‹å•æ‚¨å…·é«”æƒ³äº†è§£ä»€éº¼ï¼Ÿ"
        answer = interrupt({
            "type":     "clarification",
            "question": question,
        })
        new_query = f"{state.get('query', '')}\nä½¿ç”¨è€…è£œå……ï¼š{answer}"
        clarifications = list(state.get("user_clarifications") or []) + [answer]
        # Reset topics so _classify_node re-derives them from the updated query,
        # preventing stale topics from carrying incorrect symbols into pre_research.
        return {"query": new_query, "user_clarifications": clarifications, "topics": []}

    async def _pre_research_node(self, state: ManagerState) -> dict:
        """Pre-Research ç¯€é»ï¼šè‡ªå‹•æ”¶é›†è³‡æ–™ï¼Œä¸€æ¬¡ HITL è®“ç”¨æˆ¶è£œå……æˆ–ç¢ºèªåˆ†ææ–¹å‘ã€‚"""
        import asyncio
        query  = state.get("query", "")
        topics = state.get("topics") or []
        loop = asyncio.get_running_loop()

        # 1. å¾ topics/query æå–ä¸»è¦å¹£ç¨®
        symbol = await self._extract_research_symbol(topics, query)

        # 2. ç™¼é€ progress äº‹ä»¶ï¼ˆè®“å‰ç«¯é¡¯ç¤ºã€Œæ­£åœ¨æ”¶é›†è³‡æ–™ã€ï¼‰
        if self.progress_callback:
            self.progress_callback({"type": "research_start", "symbol": symbol})

        # 3. è‡ªå‹•åŸ·è¡Œå·¥å…·ï¼ˆä¸ interruptï¼‰
        research_data: dict = {}

        news_meta  = self.tool_registry.get("google_news",      caller_agent="manager")
        price_meta = self.tool_registry.get("get_crypto_price", caller_agent="manager")

        if news_meta:
            try:
                # Wrap tool execution
                result = await loop.run_in_executor(None, lambda: news_meta.handler.invoke({"symbol": symbol, "limit": 5}))
                if result:
                    research_data["news"] = result
            except Exception as e:
                print(f"[PreResearch] google_news failed: {e}")

        if price_meta:
            try:
                result = await loop.run_in_executor(None, lambda: price_meta.handler.invoke({"symbol": symbol}))
                if result:
                    research_data["price"] = result
            except Exception as e:
                print(f"[PreResearch] get_crypto_price failed: {e}")

        # 4. æ ¼å¼åŒ– Markdown æ‘˜è¦
        research_summary = self._format_research_summary(research_data, symbol)

        CONFIRM_TOKENS = {"confirm", "é–‹å§‹è¦åŠƒ", "å¯ä»¥", "ok", "ç¹¼çºŒ", "åŸ·è¡Œ", ""}
        clarifications = list(state.get("research_clarifications") or [])

        # 5. HITL å¾ªç’°ï¼šç”¨æˆ¶å¯ä»¥å•å•é¡Œæˆ–çµ¦æ–¹å‘ï¼Œç›´åˆ°ç¢ºèªç‚ºæ­¢
        qa_question = None
        qa_answer   = None
        first_iteration = True

        while True:
            if first_iteration:
                msg = f"æˆ‘å·²æ•´ç† **{symbol}** çš„å³æ™‚è³‡æ–™ä¾›æ‚¨åƒè€ƒï¼š"
                summary_tosend = research_summary
                q_prompt = "æƒ³èšç„¦å“ªå€‹æ–¹å‘ï¼Ÿï¼ˆä¾‹å¦‚ï¼šåªçœ‹æŠ€è¡“é¢ / é‡é»çœ‹æ–°èï¼‰è‹¥æœ‰ç–‘å•ä¹Ÿå¯ç›´æ¥å•ï¼Œç•™ç©ºç¢ºèªé–‹å§‹åˆ†æã€‚"
            else:
                msg = "é‚„æœ‰å…¶ä»–å•é¡Œå—ï¼Ÿ"
                summary_tosend = None # Suppress summary
                q_prompt = "è‹¥ç„¡å…¶ä»–å•é¡Œï¼Œè«‹ç›´æ¥ç¢ºèªé–‹å§‹è¦åŠƒã€‚"

            payload = {
                "type":             "pre_research",
                "message":          msg,
                "research_summary": summary_tosend,
                "question":         q_prompt,
            }
            # è‹¥æœ‰ Q&A ç­”æ¡ˆï¼Œé™„åœ¨ payload è®“å‰ç«¯é¡¯ç¤ºç‚ºä¸»èŠå¤©è¨Šæ¯
            if qa_question and qa_answer:
                # Embed in message for guaranteed visibility
                # msg = f"ğŸ’¡ **é—œæ–¼ã€Œ{qa_question}ã€çš„å›ç­”**ï¼š\n{qa_answer}\n\n(å·²æ›´æ–° {symbol} è³‡æ–™å¦‚ä¸Š)"
                # Actually, prepend it to the message
                msg = f"ğŸ’¡ **å›ç­”**ï¼š{qa_question}\n\n{qa_answer}\n\n---\n{msg}"
                
                payload["qa_question"] = qa_question
                payload["qa_answer"]   = qa_answer
                payload["message"]     = msg # Update message
                qa_question = qa_answer = None  # åªå‚³ä¸€æ¬¡

            user_response = interrupt(payload)
            first_iteration = False # Mark as not first iteration after interrupt returns
            # user_response might be a dict (from chat.js wrapper) or string
            if isinstance(user_response, dict):
                action = user_response.get("action", "")
                resp   = user_response.get("text") or user_response.get("value") or ""
            else:
                action = ""
                resp   = str(user_response or "")

            resp = resp.strip()

            # discuss_question actionï¼šä½¿ç”¨è€…æå•ï¼Œå–æ¶ˆ pre_researchï¼Œç›´æ¥ä»¥èŠå¤©å›ç­”
            if action == "discuss_question" and resp:
                qa_answer = await self._answer_research_question(resp, research_summary, symbol)
                print(f"[PreResearch] discuss_question: '{resp}' â†’ '{qa_answer[:60]}...'")
                return {
                    "research_data":           research_data,
                    "research_summary":        research_summary,
                    "research_clarifications": clarifications,
                    "is_discussion":           True,
                    "discussion_question":     resp,
                    "final_response":          qa_answer,
                }

            # ç¢ºèªè© â†’ ç›´æ¥é€²å…¥ plan
            if resp.lower() in CONFIRM_TOKENS:
                break

            # åµæ¸¬æ˜¯å¦ç‚ºå•é¡Œï¼ˆå«å•è™Ÿï¼Œæˆ–ä»¥æå•è©é–‹é ­ï¼‰
            QUESTION_STARTERS = (
                "ä½ è¦ºå¾—", "ä½ èªç‚º", "ä½ å»ºè­°", "å“ªå€‹", "å“ªå‰‡", "å“ªä¸€", "å“ªäº›",
                "ç‚ºä»€éº¼", "ä»€éº¼", "æ€éº¼", "å¦‚ä½•", "å¤šå°‘", "å¹¾å€‹", "æ˜¯å¦",
                "What", "Which", "How", "Why", "Who", "When", "Where",
                "Is", "Are", "Do", "Does", "Can", "Could", "Would", "Should"
            )
            resp_stripped = resp.strip()
            is_question = (
                resp_stripped.endswith("?") or resp_stripped.endswith("ï¼Ÿ") or
                any(resp_stripped.lower().startswith(w.lower()) for w in QUESTION_STARTERS)
            )

            print(f"[PreResearch] HITL Input: '{resp}' | IsQuestion: {is_question}")

            if is_question:
                qa_question = resp
                qa_answer   = await self._answer_research_question(resp, research_summary, symbol)
                print(f"[PreResearch] QA Answer: {qa_answer[:50]}...")
            else:
                # æ–¹å‘æç¤º â†’ åŠ å…¥ clarificationsï¼Œé€²å…¥ plan
                clarifications.append(resp)
                break

        return {
            "research_data":           research_data,
            "research_summary":        research_summary,
            "research_clarifications": clarifications,
        }

    async def _answer_research_question(self, question: str, research_summary: str, symbol: str) -> str:
        """ç”¨ LLM æ ¹æ“šå·²æ”¶é›†çš„ç ”ç©¶è³‡æ–™å›ç­”ç”¨æˆ¶çš„å•é¡Œã€‚è‹¥è³‡æ–™ä¸è¶³ï¼Œè‡ªå‹•ç”¨ web_search è£œå……ã€‚"""
        import asyncio
        loop = asyncio.get_running_loop()

        # å…ˆå˜—è©¦å¾ç¾æœ‰è³‡æ–™å›ç­”ï¼Œè‹¥ LLM åˆ¤æ–·è³‡æ–™ä¸è¶³å‰‡è£œå…… web_search
        probe_prompt = (
            f"ä»¥ä¸‹æ˜¯é—œæ–¼ {symbol} çš„å³æ™‚å¸‚å ´è³‡æ–™ï¼š\n\n"
            f"{research_summary}\n\n"
            f"ç”¨æˆ¶å•é¡Œï¼š{question}\n\n"
            f"è‹¥ä¸Šè¿°è³‡æ–™å·²è¶³å¤ å›ç­”å•é¡Œï¼Œè«‹ç›´æ¥å›ç­”ï¼ˆç¹é«”ä¸­æ–‡ï¼‰ã€‚"
            f"è‹¥è³‡æ–™ä¸è¶³ï¼ˆä¾‹å¦‚å•åŠç¸½ç™¼è¡Œé‡ã€å¸‚å€¼ã€ç™½çš®æ›¸ã€é–‹ç™¼åœ˜éšŠç­‰æ¨™æº–å·¥å…·æœªæ¶µè“‹çš„è³‡è¨Šï¼‰ï¼Œ"
            f"è«‹åªå›è¦† JSONï¼š{{\"need_search\": true, \"search_query\": \"å…·é«”æœå°‹é—œéµå­—\"}}"
        )
        try:
            response = await loop.run_in_executor(None, lambda: self.llm.invoke([HumanMessage(content=probe_prompt)]))
            raw = self._llm_content(response).strip()

            # æª¢æŸ¥æ˜¯å¦éœ€è¦ web_search
            parsed = self._parse_json(raw)
            if parsed and parsed.get("need_search"):
                search_query = parsed.get("search_query", f"{symbol} {question}")
                ws_tool = self.tool_registry.get("web_search", caller_agent="manager")
                search_result = ""
                if ws_tool:
                    try:
                        sr = await loop.run_in_executor(
                            None, lambda: ws_tool.handler.invoke({"query": search_query, "purpose": "research"})
                        )
                        search_result = str(sr)[:1500]
                    except Exception as e:
                        print(f"[PreResearch] web_search failed: {e}")

                # æœ‰äº†æœå°‹çµæœï¼Œé‡æ–°è«‹ LLM å›ç­”
                final_prompt = (
                    f"ç”¨æˆ¶å•ï¼š{question}\n\n"
                    f"ç¶²è·¯æœå°‹çµæœï¼š\n{search_result}\n\n"
                    f"è«‹æ ¹æ“šæœå°‹çµæœç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼Œè‹¥æœ‰é€£çµè«‹ä¿ç•™ Markdown æ ¼å¼ [æ¨™é¡Œ](url)ã€‚"
                )
                response2 = await loop.run_in_executor(None, lambda: self.llm.invoke([HumanMessage(content=final_prompt)]))
                return self._llm_content(response2).strip()

            return raw
        except Exception as e:
            print(f"[PreResearch] answer_question failed: {e}")
            return "æŠ±æ­‰ï¼Œæš«æ™‚ç„¡æ³•å›ç­”é€™å€‹å•é¡Œï¼Œè«‹ç›´æ¥é–‹å§‹åˆ†æã€‚"

    async def _extract_research_symbol(self, topics: list, query: str) -> str:
        """å¾ topics æˆ– query æå–ä¸»è¦äº¤æ˜“ä»£è™Ÿï¼ˆåŠ å¯†è²¨å¹£æˆ–è‚¡ç¥¨ï¼‰ã€‚"""
        import asyncio
        loop = asyncio.get_running_loop()
        if topics:
            candidate = topics[0]
        else:
            candidate = query

        try:
            prompt = (
                f"å¾ä»¥ä¸‹æ–‡å­—ä¸­æå–é‡‘èè³‡ç”¢çš„äº¤æ˜“ä»£è™Ÿï¼ˆtickerï¼‰ï¼ŒåŒ…å«åŠ å¯†è²¨å¹£ï¼ˆå¦‚ BTCã€ETHã€SOLï¼‰"
                f"æˆ–è‚¡ç¥¨ï¼ˆå¦‚ AAPLã€INTCã€TSLAã€2330ï¼‰ã€‚"
                f"åªå›è¦† ticker æœ¬èº«ï¼ˆç´”è‹±æ–‡å¤§å¯«ç¸®å¯«æˆ–æ•¸å­—ä»£è™Ÿï¼‰ï¼Œä¸è¦å…¶ä»–æ–‡å­—ã€‚"
                f"è‹¥ç„¡æ³•è­˜åˆ¥ï¼Œç›´æ¥å›è¦†åŸæ–‡å­—ã€‚\n\næ–‡å­—ï¼š{candidate}"
            )
            response = await loop.run_in_executor(None, lambda: self.llm.invoke([HumanMessage(content=prompt)]))
            return self._llm_content(response).strip().upper().split()[0]
        except Exception:
            return candidate.strip().upper().split()[0] if candidate.strip() else "BTC"

    def _format_research_summary(self, research_data: dict, symbol: str) -> str:
        # Pure sync helper, no change needed
        parts = []
        price = research_data.get("price")
        if price:
            price_info = price.get("price_info", "") if isinstance(price, dict) else str(price)
            if price_info:
                parts.append(price_info)

        news = research_data.get("news")
        if news and isinstance(news, list):
            lines = [f"ğŸ“° **{symbol} æœ€æ–°æ–°è**\n"]
            for i, item in enumerate(news[:5], 1):
                if isinstance(item, dict):
                    title    = item.get("title", "")
                    link     = item.get("url") or item.get("link", "")
                    date_raw = item.get("published_at") or item.get("published", "")
                else:
                    title, link, date_raw = str(item), "", ""
                date_str = f"ï¼ˆ{str(date_raw)[:10]}ï¼‰" if date_raw else ""
                if link:
                    lines.append(f"{i}. [{title}]({link}){date_str}")
                else:
                    lines.append(f"{i}. {title}{date_str}")
            parts.append("\n".join(lines))

        if not parts:
            return f"*ï¼ˆ{symbol} è³‡æ–™æš«æ™‚ç„¡æ³•å–å¾—ï¼Œå°‡ç›´æ¥é€²è¡Œè¦åŠƒï¼‰*"

        return "\n\n".join(parts)

    async def _plan_node(self, state: ManagerState) -> dict:
        import asyncio
        loop = asyncio.get_running_loop()
        query = state.get("query", "")

        # Multi-market pre-confirmed plan: skip LLM planning
        if state.get("plan_confirmed") and state.get("plan"):
            return {}

        if state.get("complexity") == "simple":
            plan = [asdict(SubTask(
                step=1,
                description=query,
                agent=state.get("intent", "chat"),
                tool_hint=None,
            ))]
            return {"plan": plan, "codebook_entry_id": None}

        # Complex ä»»å‹™ï¼šLLM planning + codebook è¨˜æ†¶
        similar = self.codebook.find_similar_entries(
            query, state.get("intent", "chat"), state.get("topics") or [], limit=3
        )
        agents_info = self.agent_registry.agents_info_for_prompt()
        tools_info  = ", ".join([t.name for t in self.tool_registry.list_all_tools()])

        past_text = "ç„¡"
        if similar:
            past_text = ""
            for i, e in enumerate(similar):
                plan_summary = "; ".join(f"{t['agent']}: {t['description']}" for t in e.plan)
                past_text += f"[{i+1}] Query: {e.query}\n    Plan: {plan_summary}\n"

        prompt = PromptRegistry.render(
            "manager", "plan",
            query=query,
            agent=state.get("intent", "chat"),
            topics=", ".join(state.get("topics") or []),
            clarifications="; ".join(state.get("user_clarifications") or []) or "ç„¡",
            past_experience=past_text,
            agents_info=agents_info,
            tools_info=tools_info,
            research_summary=state.get("research_summary") or "ç„¡",
            research_clarifications="; ".join(state.get("research_clarifications") or []) or "ç„¡",
            reflection_suggestion=state.get("plan_reflection_suggestion") or "ç„¡",
        )
        try:
            response = await loop.run_in_executor(None, lambda: self.llm.invoke([HumanMessage(content=prompt)]))
            data = self._parse_json(self._llm_content(response)) or {}
            plan_raw = data.get("plan", [])
            valid_fields = {f.name for f in fields(SubTask)}
            plan = []
            for p in plan_raw:
                task_data = {k: v for k, v in p.items() if k in valid_fields}
                plan.append(asdict(SubTask(**task_data)))
        except Exception as e:
            import traceback
            print(f"[Manager] plan error: {e}\n{traceback.format_exc()}")
            plan = [asdict(SubTask(
                step=1, description=state["query"],
                agent=state.get("intent", "chat"), tool_hint=None
            ))]

        codebook_entry_id = similar[0].id if similar else None
        return {"plan": plan, "codebook_entry_id": codebook_entry_id, "current_step_index": 0}

    async def _reflect_plan_node(self, state: ManagerState) -> dict:
        """Reflection quality gate: checks whether the plan actually answers the user's query.
        Only runs for complex queries. Simple queries are approved immediately.
        """
        import asyncio
        from core.config import PLAN_REFLECTION_MAX_RETRIES

        # Simple queries â€” skip reflection, approve immediately
        if state.get("complexity") != "complex":
            return {"plan_reflection_approved": True}

        # Pre-confirmed plans (multi-market) â€” skip reflection
        if state.get("plan_confirmed"):
            return {"plan_reflection_approved": True}

        loop = asyncio.get_running_loop()
        query               = state.get("query", "")
        plan                = state.get("plan") or []
        history             = state.get("history") or "ï¼ˆç„¡ï¼‰"
        topics              = ", ".join(state.get("topics") or []) or "æœªçŸ¥"
        clarifications      = "; ".join(state.get("user_clarifications") or []) or "ç„¡"
        previous_suggestion = state.get("plan_reflection_suggestion") or "ç„¡"
        agents_info         = self.agent_registry.agents_info_for_prompt()

        plan_text = "\n".join(
            f"{p.get('step', i + 1)}. [{p.get('agent', '')}] {p.get('description', '')}"
            for i, p in enumerate(plan)
        )

        prompt = PromptRegistry.render(
            "manager", "reflect_plan",
            query=query,
            history=history,
            topics=topics,
            clarifications=clarifications,
            plan_text=plan_text,
            agents_info=agents_info,
            previous_suggestion=previous_suggestion,
        )

        approved = True
        suggestion = None
        try:
            response = await loop.run_in_executor(
                None, lambda: self.llm.invoke([HumanMessage(content=prompt)])
            )
            data       = self._parse_json(self._llm_content(response)) or {}
            if not data:
                logger.warning("[Reflect] LLM returned unparseable JSON, auto-approving")
            approved   = bool(data.get("approved", True))
            suggestion = data.get("suggestion") or None
            reason     = data.get("reason", "")
            logger.info(f"[Reflect] approved={approved} reason={reason}")
        except Exception as e:
            logger.warning(f"[Reflect] reflection error, auto-approving: {e}")

        count = state.get("plan_reflection_count", 0) or 0
        if not approved:
            count += 1

        return {
            "plan_reflection_approved":   approved,
            "plan_reflection_count":      count,
            "plan_reflection_suggestion": suggestion if not approved else None,
        }

    async def _confirm_plan_node(self, state: ManagerState) -> dict:
        """HITL Point 2ï¼šè¤‡é›œä»»å‹™è¨ˆç•«ç¢ºèªï¼ˆæ”¯æ´å”å•†æ¨¡å¼ï¼‰ã€‚"""
        plan = state.get("plan") or []
        negotiation_response = state.get("negotiation_response")

        plan_with_icons = [
            {**t, "icon": AGENT_ICONS.get(t.get("agent", ""), "ğŸ”§")}
            for t in plan
        ]

        interrupt_payload = {
            "type":    "confirm_plan",
            "message": "é‡å°æ‚¨çš„å•é¡Œï¼Œæˆ‘è¦åŠƒäº†ä»¥ä¸‹åˆ†ææ­¥é©Ÿï¼š",
            "plan":    plan_with_icons,
        }
        if negotiation_response:
            interrupt_payload["negotiation_response"] = negotiation_response

        interrupt_payload["negotiation_limit_reached"] = (state.get("negotiate_count", 0) > 3)

        answer = interrupt(interrupt_payload)

        parsed = answer
        if isinstance(answer, str):
            stripped = answer.strip()
            if stripped.startswith("{"):
                try:
                    parsed = json.loads(stripped)
                except json.JSONDecodeError:
                    parsed = stripped

        if isinstance(parsed, dict):
            action = parsed.get("action", "")
            if action == "execute_custom":
                selected = parsed.get("selected_steps", [])
                filtered = [t for t in plan if t.get("step") in selected]
                return {
                    "plan_confirmed": True,
                    "plan_negotiating": False,
                    "plan": filtered or plan,
                    "current_step_index": 0,
                }
            elif action == "cancel":
                return {"plan_confirmed": False, "plan_negotiating": False}
            elif action == "discuss_question":
                # User asked a discussion question during plan confirmation â€” cancel plan, answer in chat
                question_text = parsed.get("text", "").strip()
                return {
                    "plan_confirmed":      False,
                    "plan_negotiating":    False,
                    "is_discussion":       True,
                    "discussion_question": question_text,
                }
            elif action == "modify_request":
                request_text = parsed.get("text", "").strip()
                return {
                    "plan_confirmed":      False,
                    "plan_negotiating":    True,
                    "negotiation_request": request_text,
                    "negotiation_response": None,
                }

        # If not a dict (action), treat as raw text input
        text_input = str(parsed).strip()

        # Explicit confirmation keywords (fast path â€” no LLM needed)
        CONFIRM_KEYWORDS = ["ok", "confirm", "start", "execute", "yes", "go", "å¼€å§‹", "åŸ·è¡Œ", "ç¢ºèª", "å¥½"]
        CANCEL_KEYWORDS = ["cancel", "stop", "no", "å–æ¶ˆ", "åœæ­¢"]

        if text_input.lower() in CONFIRM_KEYWORDS:
            return {"plan_confirmed": True, "plan_negotiating": False}

        if text_input.lower() in CANCEL_KEYWORDS:
            return {"plan_confirmed": False, "plan_negotiating": False}

        # Use LLM to distinguish question vs. modification request
        plan_text_for_detection = "\n".join(
            f"æ­¥é©Ÿ {t.get('step')}: [{t.get('agent')}] {t.get('description', '')}"
            for t in plan
        )
        intent = await self._detect_confirm_intent(text_input, plan_text_for_detection)

        if intent == "confirm":
            return {"plan_confirmed": True, "plan_negotiating": False}
        if intent == "cancel":
            return {"plan_confirmed": False, "plan_negotiating": False}
        if intent == "question":
            return {
                "plan_confirmed":        False,
                "plan_negotiating":      False,
                "is_discussion":         True,
                "discussion_question":   text_input,
                "discuss_mode":          True,
                "discuss_plan_snapshot": list(plan),   # freeze current plan
            }
        # intent == "modify"
        return {
            "plan_confirmed":      False,
            "plan_negotiating":    True,
            "negotiation_request": text_input,
            "negotiation_response": None,
        }

    async def _detect_confirm_intent(self, text: str, plan_text: str) -> str:
        """
        Use LLM to classify user's intent during plan confirmation.
        Returns: 'question' | 'modify' | 'confirm' | 'cancel'
        """
        import asyncio
        loop = asyncio.get_running_loop()
        prompt = (
            f"ç•¶å‰åˆ†æè¨ˆç•«ï¼š\n{plan_text}\n\n"
            f"ä½¿ç”¨è€…è¼¸å…¥ï¼š{text}\n\n"
            f"åˆ¤æ–·ä½¿ç”¨è€…æ„åœ–ï¼Œåªå›è¦†ä»¥ä¸‹ä¸€å€‹è©ï¼š\n"
            f"- questionï¼šè©¢å•è³‡è¨Šã€æ„è¦‹æˆ–å•é¡Œï¼ˆåŒ…å«è¨ˆç•«ç›¸é—œå•é¡Œã€å¸‚å ´æ•¸æ“šã€è¨ˆç•«å„ªç¼ºé»ç­‰ï¼‰\n"
            f"- modifyï¼šæ˜ç¢ºè¦æ±‚ä¿®æ”¹è¨ˆç•«æ­¥é©Ÿï¼ˆå¦‚ç§»é™¤ã€æ–°å¢ã€æ›¿æ›æŸæ­¥é©Ÿï¼‰\n"
            f"- confirmï¼šç¢ºèªåŸ·è¡Œè¨ˆç•«\n"
            f"- cancelï¼šå–æ¶ˆè¨ˆç•«\n\n"
            f"åªå›è¦†ä¸€å€‹è©ï¼Œä¸åŠ ä»»ä½•æ¨™é»æˆ–èªªæ˜ã€‚"
        )
        try:
            response = await loop.run_in_executor(
                None, lambda: self.llm.invoke([HumanMessage(content=prompt)])
            )
            intent = self._llm_content(response).strip().lower().split()[0]
            if intent in ("question", "modify", "confirm", "cancel"):
                return intent
            return "question"  # safe default
        except Exception as e:
            logger.warning(f"[ConfirmPlan] intent detection failed: {e}, defaulting to question")
            return "question"

    async def _negotiate_plan_node(self, state: ManagerState) -> dict:
        import asyncio
        loop = asyncio.get_running_loop()
        plan    = state.get("plan") or []
        request = state.get("negotiation_request") or ""
        count   = (state.get("negotiate_count") or 0) + 1

        if count > 3:
            return {
                "negotiate_count":       count,
                "plan_negotiating":      False,
                "negotiation_response":  "å·²é”å”å•†ä¸Šé™ï¼ˆ3æ¬¡ï¼‰ï¼Œè«‹ç›´æ¥ç¢ºèªæˆ–å–æ¶ˆç›®å‰è¨ˆç•«ã€‚",
            }

        plan_text   = "\n".join(
            f"æ­¥é©Ÿ {t.get('step')}: [{t.get('agent')}] {t.get('description', '')}"
            for t in plan
        )
        agents_info = self.agent_registry.agents_info_for_prompt()
        tools_info  = ", ".join([t.name for t in self.tool_registry.list_all_tools()])

        prompt = PromptRegistry.render(
            "manager", "negotiate_plan",
            query=state.get("query", ""),
            plan_text=plan_text,
            negotiation_request=request,
            tool_results=state.get("current_tool_result", "ç„¡"),
            agents_info=agents_info,
            tools_info=tools_info,
        )
        try:
            response     = await loop.run_in_executor(None, lambda: self.llm.invoke([HumanMessage(content=prompt)]))
            data         = self._parse_json(self._llm_content(response)) or {}

            tool_call = data.get("tool_call")
            if tool_call:
                tool_depth = state.get("tool_depth", 0)
                if tool_depth < 3:
                    t_name = tool_call.get("name")
                    t_args = tool_call.get("args") or {}
                    print(f"[Manager] Negotiate Tool Call: {t_name}({t_args})")
                    
                    tool = self.tool_registry.get(t_name, caller_agent="manager")
                    if tool:
                        try:
                            if hasattr(tool.handler, "invoke"):
                                t_result = await loop.run_in_executor(None, lambda: tool.handler.invoke(t_args))
                            else:
                                t_result = await loop.run_in_executor(None, lambda: tool.handler(**t_args))
                            t_output = (
                                f"--- Auto-Tool Execution ---\n"
                                f"Tool: {t_name}\n"
                                f"Result: {str(t_result)[:2000]}"
                            )
                            state["current_tool_result"] = t_output
                            state["tool_depth"] = tool_depth + 1
                            # Recursive call - in async we return the node result, so we call it again?
                            # LangGraph nodes usually shouldn't recurse directly if they are async... 
                            # actually they can.
                            return await self._negotiate_plan_node(state)
                        except Exception as te:
                            print(f"[Manager] Tool execution failed: {te}")
                    else:
                        print(f"[Manager] Tool {t_name} not found")

            new_plan_raw = data.get("modified_plan", [])
            valid_fields = {f.name for f in fields(SubTask)}
            new_plan = []
            if new_plan_raw:
                for p in new_plan_raw:
                    # Ensure required fields exist
                    if "step" not in p:
                        continue # Skip invalid steps
                    
                    task_data = {k: v for k, v in p.items() if k in valid_fields}
                    
                    # Provide defaults for required fields if missing or empty
                    if not task_data.get("description"):
                        task_data["description"] = "åŸ·è¡Œæ­¥é©Ÿ"
                    if not task_data.get("agent"):
                        task_data["agent"] = "chat"
                        
                    new_plan.append(asdict(SubTask(**task_data)))
            else:
                new_plan = plan
            negotiation_response = data.get("explanation", "å·²æ ¹æ“šæ‚¨çš„å»ºè­°èª¿æ•´è¨ˆç•«ï¼Œè«‹ç¢ºèªã€‚")
        
        except Exception as e:
            import traceback
            print(f"[Manager] negotiate_plan error: {e}\n{traceback.format_exc()}")
            new_plan             = plan
            negotiation_response = "ç„¡æ³•è™•ç†ä¿®æ”¹è«‹æ±‚ï¼Œè«‹é‡æ–°å˜—è©¦æˆ–ç›´æ¥åŸ·è¡ŒåŸè¨ˆç•«ã€‚"

        if "tool_depth" in state:
            del state["tool_depth"]
        if "current_tool_result" in state:
            del state["current_tool_result"]

        return {
            "plan":                  new_plan,
            "negotiate_count":       count,
            "plan_negotiating":      False,
            "negotiation_response":  negotiation_response,
        }

    async def _discuss_node(self, state: ManagerState) -> dict:
        """
        è¨ˆç•«è¨è«–ç¯€é»ï¼šä½¿ç”¨è€…åœ¨ confirm_plan HITL ä¸­æå•ï¼ˆéä¿®æ”¹è¨ˆç•«ï¼‰ã€‚
        å–æ¶ˆè¨ˆç•«ç‹€æ…‹ï¼Œç”¨ LLM æ ¹æ“šç ”ç©¶è³‡æ–™ç›´æ¥å›ç­”å•é¡Œï¼Œçµæœä½œç‚º final_responseã€‚
        """
        import asyncio
        loop = asyncio.get_running_loop()
        question = state.get("discussion_question") or ""
        research_summary = state.get("research_summary") or ""
        plan = state.get("plan") or []
        query = state.get("query") or ""
        history = state.get("history") or ""

        # æä¾›è¨ˆç•«å…§å®¹å’Œç ”ç©¶è³‡æ–™ä½œç‚ºä¸Šä¸‹æ–‡
        plan_text = "\n".join(
            f"æ­¥é©Ÿ {t.get('step')}: [{t.get('agent')}] {t.get('description', '')}"
            for t in plan
        ) if plan else "ï¼ˆç„¡è¨ˆç•«ï¼‰"

        context_parts = []
        if research_summary:
            context_parts.append(f"ã€å³æ™‚å¸‚å ´è³‡æ–™ã€‘\n{research_summary}")
        if plan_text != "ï¼ˆç„¡è¨ˆç•«ï¼‰":
            context_parts.append(f"ã€å‰›æ‰è¦åŠƒçš„åˆ†ææ­¥é©Ÿã€‘\n{plan_text}")
        if query:
            context_parts.append(f"ã€åŸå§‹åˆ†æè«‹æ±‚ã€‘\n{query}")

        context = "\n\n".join(context_parts) if context_parts else "ï¼ˆæš«ç„¡é¡å¤–èƒŒæ™¯è³‡æ–™ï¼‰"

        prompt = (
            f"ä»¥ä¸‹æ˜¯å‰›æ‰åˆ†æè¨è«–çš„èƒŒæ™¯è³‡æ–™ï¼š\n\n{context}\n\n"
            f"å°è©±æ­·å²ï¼š\n{history}\n\n"
            f"ä½¿ç”¨è€…æå•ï¼š{question}\n\n"
            f"è«‹ç”¨ç¹é«”ä¸­æ–‡ç›´æ¥å›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚è‹¥å•é¡Œæ¶‰åŠè¨ˆç•«æ­¥é©Ÿæˆ–å¸‚å ´è³‡æ–™ï¼Œè«‹æ ¹æ“šä¸Šä¸‹æ–‡ä½œç­”ã€‚"
            f"è‹¥å¼•ç”¨æ–°èï¼Œå¿…é ˆä¿ç•™åŸå§‹ Markdown é€£çµæ ¼å¼ [æ¨™é¡Œ](url)ï¼Œä¸è¦çœç•¥é€£çµã€‚"
            f"ä¿æŒå›ç­”æ¸…æ™°ã€å…·é«”ã€æœ‰å¹«åŠ©ã€‚è¨ˆç•«å·²å–æ¶ˆï¼Œå¾ŒçºŒä½¿ç”¨è€…å¯å†æ¬¡æå‡ºåˆ†æè«‹æ±‚ã€‚"
        )
        try:
            response = await loop.run_in_executor(None, lambda: self.llm.invoke([HumanMessage(content=prompt)]))
            answer = self._llm_content(response).strip()
        except Exception as e:
            logger.error(f"[Discuss] LLM error: {e}")
            answer = "æŠ±æ­‰ï¼Œæš«æ™‚ç„¡æ³•å›ç­”é€™å€‹å•é¡Œã€‚è«‹é‡æ–°æå•æˆ–ç™¼èµ·æ–°çš„åˆ†æã€‚"

        logger.info(f"[Discuss] Answered question: {question[:50]}...")
        return {
            "final_response": answer,
            "plan_confirmed":  False,
            "is_discussion":   True,
        }

    async def _execute_node(self, state: ManagerState) -> dict:
        import asyncio
        loop = asyncio.get_running_loop()
        plan = state.get("plan") or []
        results = state.get("agent_results") or []
        idx = state.get("current_step_index", 0)
        language = state.get("language", "zh-TW")  # ç²å–ç”¨æˆ¶èªè¨€åå¥½

        if idx >= len(plan):
            return {}

        task_dict = plan[idx]
        task = SubTask(**{k: v for k, v in task_dict.items()
                          if k in SubTask.__dataclass_fields__})
        task.context = {
            "history": self._build_history(state),
            "language": language,  # å‚³éèªè¨€åƒæ•¸çµ¦ Agent
        }

        logger.info(f"[Manager] Executing step {idx+1}/{len(plan)}: {task.agent} - {task.description}")

        agent = self.router.route(task.agent)
        if not agent:
            result_data = {
                "success": False, 
                "message": f"Agent {task.agent} not found",
                "agent_name": task.agent,
                "step_index": idx
            }
        else:
            if self.progress_callback:
                self.progress_callback({
                    "type": "agent_start",
                    "agent": agent.name,
                    "step": task.step,
                    "description": task.description
                })

            try:
                # Execute agent in executor to prevent blocking
                res = await loop.run_in_executor(None, agent.execute, task)
                
                result_data = {
                    "success":    res.success,
                    "message":    res.message,
                    "agent_name": res.agent_name,
                    "data":       res.data,
                    "step_index": idx
                }
                
                if self.progress_callback:
                    self.progress_callback({
                        "type": "agent_finish",
                        "agent": res.agent_name,
                        "step": task.step,
                        "success": res.success
                    })

            except Exception as e:
                logger.error(f"[{agent.name}] Execution error: {e}")
                result_data = {
                    "success": False, 
                    "message": f"åŸ·è¡Œç™¼ç”ŸéŒ¯èª¤: {str(e)}", 
                    "agent_name": agent.name,
                    "step_index": idx
                }

        new_results = list(results) + [result_data]
        
        return {
            "agent_results": new_results, 
            "current_step_index": idx + 1,
            "retry_count": (state.get("retry_count") or 0)
        }

    async def _watcher_node(self, state: ManagerState) -> dict:
        """Watcher Node: Critique execution results."""
        import asyncio
        loop = asyncio.get_running_loop()
        results = state.get("agent_results") or []
        query = state.get("query", "")
        
        if state.get("complexity") != "complex":
            return {}

        logger.info("[Watcher] Reviewing results...")
        
        for res in results:
            if not res.get("success"): 
                continue
                
            # Wrap watcher critique in executor
            # Assuming self.watcher.critique is sync
            critique = await loop.run_in_executor(
                None, 
                lambda: self.watcher.critique(
                    query=query,
                    step_description=f"Agent: {res.get('agent_name')}", 
                    result=res.get("message", "")
                )
            )
            
            if critique.get("status") == "FAIL":
                logger.warning(f"[Watcher] Flagged result from {res.get('agent_name')}: {critique.get('feedback')}")
                res["watcher_feedback"] = critique.get("feedback")
                
        return {"agent_results": results}

    async def _synthesize_node(self, state: ManagerState) -> dict:
        import asyncio
        loop = asyncio.get_running_loop()
        results = state.get("agent_results") or []

        if state.get("complexity") == "simple":
            for r in results:
                if r.get("success") and r.get("message"):
                    return {"final_response": r["message"]}
            
            failure_context = "\n".join(
                f"- [{r.get('agent_name', '?')}] {r.get('message', 'æœªçŸ¥éŒ¯èª¤')}"
                for r in results if r
            )
            chat_agent = self.router.route("chat")
            if chat_agent:
                try:
                    fallback_task = SubTask(step=1, description=state.get("query", ""), agent="chat")
                    fallback_task.context = {
                        "history":        self._build_history(state),
                        "agent_failures": failure_context,
                    }
                    fallback_result = await loop.run_in_executor(None, chat_agent.execute, fallback_task)
                    if fallback_result.success and fallback_result.message:
                        return {"final_response": fallback_result.message}
                except Exception as e:
                    logger.error(f"[Synthesize] chat fallback failed: {e}")
            return {"final_response": f"âš ï¸ ç„¡æ³•å–å¾—åˆ†ææ•¸æ“šï¼š\n{failure_context}"}

        successful = [r for r in results if r.get("success")]
        if not successful:
            failed_info = "; ".join(
                f"{r.get('agent_name', '?')}: {r.get('message', '')[:100]}"
                for r in results
            )
            return {"final_response": f"âš ï¸ æ‰€æœ‰åˆ†ææ­¥é©Ÿå‡å¤±æ•—ï¼Œç„¡æ³•ç”Ÿæˆå ±å‘Šã€‚\n\nåŸå› ï¼š{failed_info}"}

        AGENT_LABELS = {
            "crypto":   "ğŸ” åŠ å¯†è²¨å¹£",
            "tw_stock": "ğŸ‡¹ğŸ‡¼ å°è‚¡",
            "us_stock": "ğŸ“ˆ ç¾è‚¡",
            "chat":     "ğŸ’¬ å°è©±",
        }
        results_text = "\n\n---\n\n".join(
            f"## {AGENT_LABELS.get(r['agent_name'], r['agent_name'])}\n\n{r['message']}"
            for r in successful
        )
        
        plan = state.get("plan") or []
        plan_summary = "\n".join(
            f"{i+1}. [{t.get('agent')}] {t.get('description')}" 
            for i, t in enumerate(plan)
        ) or "ï¼ˆç„¡è¨ˆç•«ï¼‰"

        prompt = PromptRegistry.render(
            "manager", "synthesize",
            query=state["query"],
            plan_summary=plan_summary,
            clarifications="; ".join(state.get("user_clarifications") or []) or "ç„¡",
            results=results_text,
        )
        try:
            response = await loop.run_in_executor(None, lambda: self.llm.invoke([HumanMessage(content=prompt)]))
            final = self._llm_content(response)
        except Exception:
            final = results_text or "ï¼ˆç„¡æ³•ç”Ÿæˆå›æ‡‰ï¼‰"

        return {"final_response": final}

    async def _save_node(self, state: ManagerState) -> dict:
        # Saving to codebook might involve IO or vector DB operations, better wrap it.
        import asyncio
        loop = asyncio.get_running_loop()
        
        complexity = state.get('complexity')
        has_resp = bool(state.get('final_response'))
        has_plan = bool(state.get('plan'))
        logger.info(f"[DEBUG] _save_node: complexity={complexity}, has_response={has_resp}, has_plan={has_plan}")
        
        if complexity == "complex" and has_resp and has_plan and not state.get("is_discussion"):
            def do_save():
                logger.info("[DEBUG] _save_node: Saving to codebook...")
                from .hierarchical_memory import MemoryEntry
                from datetime import datetime
                plan_clean = [{k: v for k, v in t.items() if k not in ("result", "icon")} for t in (state.get("plan") or [])]
                entry = MemoryEntry(
                    id=str(uuid4()),
                    query=state["query"],
                    intent=state.get("intent", "chat"),
                    topics=state.get("topics") or [],
                    plan=plan_clean,
                    complexity=complexity,
                    created_at=datetime.now().isoformat(),
                    ttl_days=14,
                )
                primary_topic = (state.get("topics") or ["DEFAULT"])[0].upper()
                try:
                    self.codebook._persist_entry(entry, primary_topic)
                    self.codebook._cache[entry.id] = entry
                    self.codebook._update_index(entry)
                    logger.info(f"[DEBUG] _save_node: Saved entry {entry.id}")
                    return {"codebook_entry_id": entry.id}
                except Exception as e:
                    logger.error(f"[DEBUG] _save_node: Failed to save to codebook: {e}")
                    return {}

            return await loop.run_in_executor(None, do_save)

        return {}

    # â”€â”€ è·¯ç”±å‡½æ•¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _after_pre_research(self, state: ManagerState) -> str:
        """pre_research çµæŸå¾Œï¼šè‹¥ä½¿ç”¨è€…æå•ï¼ˆdiscuss_questionï¼‰ï¼Œç›´æ¥çµæŸï¼›å¦å‰‡é€²å…¥ planã€‚"""
        return "save" if state.get("is_discussion") else "plan"

    def _after_classify(self, state: ManagerState) -> str:
        if state.get("plan_confirmed"):
            return "plan"   # Multi-market: skip pre_research, plan node will no-op
        if state.get("complexity") == "ambiguous":
            return "clarify"
        elif state.get("complexity") == "complex" and state.get("intent") == "crypto":
            return "pre_research"   # complex åŠ å¯†è²¨å¹£ä»»å‹™å…ˆåšé ç ”ç©¶
        else:
            return "plan"

    def _after_plan(self, state: ManagerState) -> str:
        if state.get("plan_confirmed"):
            return "execute"
        if state.get("complexity") == "complex":
            return "reflect_plan"   # complex â†’ quality gate before user confirmation
        return "execute"

    def _after_reflect_plan(self, state: ManagerState) -> str:
        """Route after reflection: confirm if approved or max retries hit, else re-plan."""
        from core.config import PLAN_REFLECTION_MAX_RETRIES
        approved = state.get("plan_reflection_approved", True)
        count    = state.get("plan_reflection_count", 0)

        if approved or count > PLAN_REFLECTION_MAX_RETRIES:
            return "confirm"   # proceed to user confirmation
        return "re_plan"       # loop back to plan node for a better plan

    def _after_confirm(self, state: ManagerState) -> str:
        if state.get("plan_negotiating"):
            return "negotiate"
        if state.get("is_discussion"):
            return "discuss"
        return "execute" if state.get("plan_confirmed") else "end"

    def _after_execute(self, state: ManagerState) -> str:
        idx = state.get("current_step_index", 0)
        plan = state.get("plan") or []
        if idx < len(plan):
            return "continue"
        return "done"

    # â”€â”€ CLI å…¥å£ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def process(self, query: str, session_id: str = None) -> str:
        """CLI æ¨¡å¼ï¼šè‡ªå‹•è™•ç† interruptï¼Œä½¿ç”¨ HITLManager æå•ã€‚"""
        if session_id is None:
            session_id = str(uuid4())

        config = {"configurable": {"thread_id": session_id}}
        initial = {
            "session_id":                session_id,
            "query":                     query,
            "agent_results":             [],
            "user_clarifications":       [],
            "retry_count":               0,
            "plan_reflection_count":     0,
            "plan_reflection_suggestion": None,
            "discuss_mode":              False,
            "discuss_plan_snapshot":     None,
            "replan_request":            False,
        }

        result = self.graph.invoke(initial, config)

        # CLI loopï¼šè‡ªå‹•è™•ç†æ‰€æœ‰ interrupt
        while result.get("__interrupt__"):
            iv     = result["__interrupt__"][0].value
            itype  = iv.get("type", "")

            if itype == "confirm_plan":
                # CLI æ¨¡å¼ï¼šé¡¯ç¤ºè¨ˆç•«æ­¥é©Ÿä¸¦è©¢å•æ˜¯å¦åŸ·è¡Œ
                plan = iv.get("plan", [])
                plan_text = "\n".join(
                    f"  {t.get('icon','ğŸ”§')} {t.get('description','')}" for t in plan
                )
                question = f"{iv.get('message','åŸ·è¡Œè¨ˆç•«ï¼Ÿ')}\n{plan_text}"
                options  = ["åŸ·è¡Œ", "å–æ¶ˆ"]
            else:
                question = iv.get("question", "è«‹å›ç­”ï¼š")
                options  = iv.get("options")

            answer = input(f"\n[HITL] {question} (Options: {options}): ")
            result = self.graph.invoke(Command(resume=answer), config)

        return result.get("final_response") or "ï¼ˆç„¡å›æ‡‰ï¼‰"

    def get_status(self) -> dict:
        return {
            "agents":          [m.name for m in self.agent_registry.list_all()],
            "tools":           [t.name for t in self.tool_registry.list_all_tools()],
            "codebook":        self.codebook.stats(),
            "active_sessions": 0,  # checkpointer manages this now
        }

    # â”€â”€ å·¥å…·æ–¹æ³• â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _build_history(self, state: ManagerState) -> str:
        """çµ„åˆ DB è¼‰å…¥çš„æ­·å² + æœ¬è¼ª clarificationsï¼Œæä¾›çµ¦ agent ä½œä¸Šä¸‹æ–‡ã€‚"""
        parts = []

        # DB æ­·å²ï¼ˆç”± analysis.py åœ¨è«‹æ±‚é€²å…¥æ™‚è¼‰å…¥ï¼‰
        db_history = (state.get("history") or "").strip()
        if db_history:
            parts.append(db_history)

        # æœ¬è¼ª HITL è£œå……èªªæ˜
        clarifications = state.get("user_clarifications") or []
        if clarifications:
            parts.append("\n".join(f"è£œå…… {i+1}: {c}" for i, c in enumerate(clarifications)))

        return "\n".join(parts) if parts else "é€™æ˜¯æ–°å°è©±çš„é–‹å§‹"

    @staticmethod
    def _llm_content(response) -> str:
        """å®‰å…¨åœ°å¾ LLM å›æ‡‰æå–æ–‡å­—ã€‚content å¯èƒ½æ˜¯ strã€list æˆ– dictï¼ˆéƒ¨åˆ† LangChain adapterï¼‰ã€‚"""
        content = response.content
        if isinstance(content, str):
            return content
        if isinstance(content, list):
            # Google / Anthropic multi-part format: [{type: "text", text: "..."}]
            parts = []
            for block in content:
                if isinstance(block, dict):
                    parts.append(block.get("text", ""))
                else:
                    parts.append(str(block))
            return "".join(parts)
        if isinstance(content, dict):
            return content.get("text", str(content))
        return str(content)

    def _parse_json(self, text) -> Optional[dict]:
        if not isinstance(text, str):
            text = str(text)
        clean = text.strip()
        if "```json" in clean:
            clean = clean.split("```json")[1].split("```")[0].strip()
        elif "```" in clean:
            clean = clean.split("```")[1].split("```")[0].strip()
        try:
            return json.loads(clean)
        except json.JSONDecodeError:
            try:
                match = re.search(r'\{.*\}', text.replace('\n', ''), re.DOTALL)
                if match:
                    return json.loads(match.group(0))
            except Exception:
                pass
            return None
